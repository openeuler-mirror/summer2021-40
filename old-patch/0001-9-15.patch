From 006bc57d52711f27058f53f492309af27bb7d6e6 Mon Sep 17 00:00:00 2001
From: jiax <jiaxgong@163.com>
Date: Wed, 15 Sep 2021 10:38:25 +0800
Subject: [PATCH] 9-15

9-15

9-15

9-15
---
 block/bio.c                                   | 576 +++++++++++-
 block/blk-core.c                              |  12 +-
 fs/buffer.c                                   |   5 +-
 include/asm-generic/sections.h                |   3 +
 include/dt-bindings/input/linux-event-codes.h | 853 +++++++++++++++++-
 include/linux/dmapool.h                       |   7 +
 include/linux/gfp.h                           | 128 ++-
 include/linux/mempool.h                       |   9 +
 include/linux/module.h                        |  42 +
 include/linux/percpu.h                        |   8 +
 include/linux/slab.h                          | 399 +++++++-
 include/linux/statis_memory.h                 |  64 ++
 kernel/Makefile                               |   3 +
 kernel/module.c                               |  34 +
 kernel/params.c                               |  80 ++
 kernel/statis_memory.c                        | 256 ++++++
 mm/Kconfig                                    |   6 +
 mm/dmapool.c                                  | 160 ++++
 mm/mempolicy.c                                | 108 +++
 mm/mempool.c                                  | 151 ++++
 mm/page_alloc.c                               | 222 ++++-
 mm/percpu.c                                   | 358 +++++++-
 mm/slab_common.c                              |  85 +-
 mm/slub.c                                     | 511 ++++++++++-
 24 files changed, 4060 insertions(+), 20 deletions(-)
 mode change 120000 => 100644 include/dt-bindings/input/linux-event-codes.h
 create mode 100644 include/linux/statis_memory.h
 create mode 100644 kernel/statis_memory.c

diff --git a/block/bio.c b/block/bio.c
index da05350dfba2..766a45d02490 100644
--- a/block/bio.c
+++ b/block/bio.c
@@ -248,7 +248,8 @@ void bio_uninit(struct bio *bio)
 }
 EXPORT_SYMBOL(bio_uninit);
 
-static void bio_free(struct bio *bio)
+#ifdef CONFIG_DEBUG_KMALLOC
+static void bio_free(struct bio *bio, unsigned long caller_address)
 {
 	struct bio_set *bs = bio->bi_pool;
 	void *p;
@@ -263,7 +264,29 @@ static void bio_free(struct bio *bio)
 		 */
 		p = bio;
 		p -= bs->front_pad;
+		mempool_free(p, &bs->bio_pool);
+		//printk(KERN_DEBUG "mempool_free\n");
+	} else {
+		/* Bio was allocated by bio_kmalloc() */
+		kfree(bio);
+	}
+}
+#else
+static void bio_free(struct bio *bio)
+{
+	struct bio_set *bs = bio->bi_pool;
+	void *p;
+
+	bio_uninit(bio);
+
+	if (bs) {
+		bvec_free(&bs->bvec_pool, bio->bi_io_vec, BVEC_POOL_IDX(bio));
 
+		/*
+		 * If we have front padding, adjust the bio pointer before freeing
+		 */
+		p = bio;
+		p -= bs->front_pad;
 		mempool_free(p, &bs->bio_pool);
 	} else {
 		/* Bio was allocated by bio_kmalloc() */
@@ -271,6 +294,7 @@ static void bio_free(struct bio *bio)
 	}
 }
 
+#endif
 /*
  * Users of this function have their own bio allocation. Subsequently,
  * they must remember to pair any call to bio_init() with bio_uninit()
@@ -435,6 +459,123 @@ static void punt_bios_to_rescuer(struct bio_set *bs)
  *   RETURNS:
  *   Pointer to new bio on success, NULL on failure.
  */
+#ifdef CONFIG_DEBUG_KMALLOC
+struct bio *bio_alloc_bioset_memhook(gfp_t gfp_mask, unsigned int nr_iovecs,
+			     struct bio_set *bs, unsigned long caller_address, call_function_t cf_type)
+{
+	gfp_t saved_gfp = gfp_mask;
+	unsigned front_pad;
+	unsigned inline_vecs;
+	struct bio_vec *bvl = NULL;
+	struct bio *bio;
+	void *p;
+	
+	// struct module* mod;
+	// mod = __module_text_address(caller_address);
+	// if(mod)
+	// 	if(!strcmp(mod->name, "dm_mod"))
+	// 		printk("dm_mod-bio_alloc_bioset");
+
+	if (!bs) {
+		if (nr_iovecs > UIO_MAXIOV)
+			return NULL;
+
+		p = kmalloc_memhook(sizeof(struct bio) +
+			    nr_iovecs * sizeof(struct bio_vec),
+			    gfp_mask, caller_address, cf_type);
+		front_pad = 0;
+		inline_vecs = nr_iovecs;
+	} else {
+		/* should not use nobvec bioset for nr_iovecs > 0 */
+		if (WARN_ON_ONCE(!mempool_initialized(&bs->bvec_pool) &&
+				 nr_iovecs > 0))
+			return NULL;
+		/*
+		 * generic_make_request() converts recursion to iteration; this
+		 * means if we're running beneath it, any bios we allocate and
+		 * submit will not be submitted (and thus freed) until after we
+		 * return.
+		 *
+		 * This exposes us to a potential deadlock if we allocate
+		 * multiple bios from the same bio_set() while running
+		 * underneath generic_make_request(). If we were to allocate
+		 * multiple bios (say a stacking block driver that was splitting
+		 * bios), we would deadlock if we exhausted the mempool's
+		 * reserve.
+		 *
+		 * We solve this, and guarantee forward progress, with a rescuer
+		 * workqueue per bio_set. If we go to allocate and there are
+		 * bios on current->bio_list, we first try the allocation
+		 * without __GFP_DIRECT_RECLAIM; if that fails, we punt those
+		 * bios we would be blocking to the rescuer workqueue before
+		 * we retry with the original gfp_flags.
+		 */
+
+		if (current->bio_list &&
+		    (!bio_list_empty(&current->bio_list[0]) ||
+		     !bio_list_empty(&current->bio_list[1])) &&
+		    bs->rescue_workqueue)
+			gfp_mask &= ~__GFP_DIRECT_RECLAIM;
+
+		p = mempool_alloc_memhook(&bs->bio_pool, gfp_mask, caller_address, cf_type);
+		if (!p && gfp_mask != saved_gfp) {
+			punt_bios_to_rescuer(bs);
+			gfp_mask = saved_gfp;
+			p = mempool_alloc_memhook(&bs->bio_pool, gfp_mask, caller_address, cf_type);
+		}
+
+		front_pad = bs->front_pad;
+		inline_vecs = BIO_INLINE_VECS;
+	}
+
+	if (unlikely(!p))
+		return NULL;
+
+	bio = p + front_pad;
+	bio_init(bio, NULL, 0);
+
+	if (nr_iovecs > inline_vecs) {
+		unsigned long idx = 0;
+
+		bvl = bvec_alloc(gfp_mask, nr_iovecs, &idx, &bs->bvec_pool);
+		if (!bvl && gfp_mask != saved_gfp) {
+			punt_bios_to_rescuer(bs);
+			gfp_mask = saved_gfp;
+			bvl = bvec_alloc(gfp_mask, nr_iovecs, &idx, &bs->bvec_pool);
+		}
+
+		if (unlikely(!bvl))
+			goto err_free;
+
+		bio->bi_flags |= idx << BVEC_POOL_OFFSET;
+	} else if (nr_iovecs) {
+		bvl = bio->bi_inline_vecs;
+	}
+
+	bio->bi_pool = bs;
+	bio->bi_max_vecs = nr_iovecs;
+	bio->bi_io_vec = bvl;
+
+	return bio;
+
+err_free:
+	mempool_free(p, &bs->bio_pool);
+	return NULL;
+}
+EXPORT_SYMBOL(bio_alloc_bioset_memhook);
+
+struct bio *bio_alloc_bioset(gfp_t gfp_mask, unsigned int nr_iovecs,
+			     struct bio_set *bs)
+{
+	unsigned long caller_address;
+	call_function_t cf_type = MEMPOOL_ALLOC;
+
+	caller_address = (unsigned long)_RET_IP_;
+
+	return bio_alloc_bioset_memhook(gfp_mask, nr_iovecs, bs, caller_address, cf_type);
+}
+EXPORT_SYMBOL(bio_alloc_bioset);
+#else
 struct bio *bio_alloc_bioset(gfp_t gfp_mask, unsigned int nr_iovecs,
 			     struct bio_set *bs)
 {
@@ -531,6 +672,7 @@ struct bio *bio_alloc_bioset(gfp_t gfp_mask, unsigned int nr_iovecs,
 	return NULL;
 }
 EXPORT_SYMBOL(bio_alloc_bioset);
+#endif
 
 void zero_fill_bio_iter(struct bio *bio, struct bvec_iter start)
 {
@@ -557,16 +699,32 @@ EXPORT_SYMBOL(zero_fill_bio_iter);
  **/
 void bio_put(struct bio *bio)
 {
+	unsigned long caller_address = _RET_IP_;
+	// int flag = 0;
+
+	// struct module* mod;
+	// mod = __module_text_address(caller_address);
+	// if(mod)
+	// 	if(!strcmp(mod->name, "dm_mod"))
+	// 		flag = 1;
+
 	if (!bio_flagged(bio, BIO_REFFED))
-		bio_free(bio);
+	{
+		// if(flag)
+		// 	printk("dm_mod-putbio:1 %lx %lx\n", caller_address, (unsigned long)bio);
+		bio_free(bio, caller_address);
+	}
 	else {
 		BIO_BUG_ON(!atomic_read(&bio->__bi_cnt));
 
 		/*
 		 * last put frees it
 		 */
-		if (atomic_dec_and_test(&bio->__bi_cnt))
-			bio_free(bio);
+		if (atomic_dec_and_test(&bio->__bi_cnt)){
+			// if(flag)
+			// 	printk("dm_mod-putbio:2 %lx %lx\n", caller_address, (unsigned long)bio);
+			bio_free(bio,caller_address);
+		}
 	}
 }
 EXPORT_SYMBOL(bio_put);
@@ -622,11 +780,50 @@ EXPORT_SYMBOL(__bio_clone_fast);
  *
  * 	Like __bio_clone_fast, only also allocates the returned bio
  */
+
+#ifdef CONFIG_DEBUG_KMALLOC
+struct bio *bio_clone_fast_memhook(struct bio *bio, gfp_t gfp_mask, struct bio_set *bs, 
+	unsigned long caller_address, call_function_t cf_type)
+{
+	struct bio *b;
+
+	b = bio_alloc_bioset_memhook(gfp_mask, 0, bs, caller_address, cf_type);
+	if (!b)
+		return NULL;
+
+	__bio_clone_fast(b, bio);
+
+	if (bio_integrity(bio)) {
+		int ret;
+
+		ret = bio_integrity_clone(b, bio, gfp_mask);
+
+		if (ret < 0) {
+			bio_put(b);
+			return NULL;
+		}
+	}
+
+	return b;
+}
+EXPORT_SYMBOL(bio_clone_fast_memhook);
+
+struct bio *bio_clone_fast(struct bio *bio, gfp_t gfp_mask, struct bio_set *bs)
+{
+	unsigned long caller_address = _RET_IP_;
+	call_function_t cf_type = MEMPOOL_ALLOC;
+
+	return bio_clone_fast_memhook(bio, gfp_mask, bs, caller_address, cf_type);
+}
+EXPORT_SYMBOL(bio_clone_fast);
+#else
 struct bio *bio_clone_fast(struct bio *bio, gfp_t gfp_mask, struct bio_set *bs)
 {
 	struct bio *b;
+	unsigned long caller_address = _RET_IP_;
+	call_function_t cf_type = MEMPOOL_ALLOC;
 
-	b = bio_alloc_bioset(gfp_mask, 0, bs);
+	b = bio_alloc_bioset_memhook(gfp_mask, 0, bs, caller_address, cf_type);
 	if (!b)
 		return NULL;
 
@@ -646,6 +843,7 @@ struct bio *bio_clone_fast(struct bio *bio, gfp_t gfp_mask, struct bio_set *bs)
 	return b;
 }
 EXPORT_SYMBOL(bio_clone_fast);
+#endif
 
 /**
  *	bio_add_pc_page	-	attempt to add page to bio
@@ -1045,6 +1243,24 @@ struct bio_map_data {
 	struct iovec iov[];
 };
 
+#ifdef CONFIG_DEBUG_KMALLOC
+static struct bio_map_data *bio_alloc_map_data(struct iov_iter *data,
+					       gfp_t gfp_mask, unsigned long caller_address, call_function_t cf_type)
+{
+	struct bio_map_data *bmd;
+	if (data->nr_segs > UIO_MAXIOV)
+		return NULL;
+
+	bmd = kmalloc_memhook(sizeof(struct bio_map_data) +
+		       sizeof(struct iovec) * data->nr_segs, gfp_mask, caller_address, cf_type);
+	if (!bmd)
+		return NULL;
+	memcpy(bmd->iov, data->iov, sizeof(struct iovec) * data->nr_segs);
+	bmd->iter = *data;
+	bmd->iter.iov = bmd->iov;
+	return bmd;
+}
+#else
 static struct bio_map_data *bio_alloc_map_data(struct iov_iter *data,
 					       gfp_t gfp_mask)
 {
@@ -1061,6 +1277,7 @@ static struct bio_map_data *bio_alloc_map_data(struct iov_iter *data,
 	bmd->iter.iov = bmd->iov;
 	return bmd;
 }
+#endif
 
 /**
  * bio_copy_from_iter - copy all pages from iov_iter to bio
@@ -1175,6 +1392,117 @@ int bio_uncopy_user(struct bio *bio)
  *	to/from kernel pages as necessary. Must be paired with
  *	call bio_uncopy_user() on io completion.
  */
+#ifdef CONFIG_DEBUG_KMALLOC
+struct bio *bio_copy_user_iov(struct request_queue *q,
+			      struct rq_map_data *map_data,
+			      struct iov_iter *iter,
+			      gfp_t gfp_mask)
+{
+	struct bio_map_data *bmd;
+	struct page *page;
+	struct bio *bio;
+	int i = 0, ret;
+	int nr_pages;
+	unsigned int len = iter->count;
+	unsigned int offset = map_data ? offset_in_page(map_data->offset) : 0;
+	unsigned long caller_address = _RET_IP_;
+	call_function_t cf_type = MEMPOOL_ALLOC;
+
+	bmd = bio_alloc_map_data(iter, gfp_mask, caller_address, cf_type);
+	if (!bmd)
+		return ERR_PTR(-ENOMEM);
+
+	/*
+	 * We need to do a deep copy of the iov_iter including the iovecs.
+	 * The caller provided iov might point to an on-stack or otherwise
+	 * shortlived one.
+	 */
+	bmd->is_our_pages = map_data ? 0 : 1;
+
+	nr_pages = DIV_ROUND_UP(offset + len, PAGE_SIZE);
+	if (nr_pages > BIO_MAX_PAGES)
+		nr_pages = BIO_MAX_PAGES;
+
+	ret = -ENOMEM;
+	bio = bio_alloc_bioset_memhook(gfp_mask, nr_pages, NULL, caller_address, cf_type);
+	if (!bio)
+		goto out_bmd;
+
+	ret = 0;
+
+	if (map_data) {
+		nr_pages = 1 << map_data->page_order;
+		i = map_data->offset / PAGE_SIZE;
+	}
+	while (len) {
+		unsigned int bytes = PAGE_SIZE;
+
+		bytes -= offset;
+
+		if (bytes > len)
+			bytes = len;
+
+		if (map_data) {
+			if (i == map_data->nr_entries * nr_pages) {
+				ret = -ENOMEM;
+				break;
+			}
+
+			page = map_data->pages[i / nr_pages];
+			page += (i % nr_pages);
+
+			i++;
+		} else {
+			page = alloc_page(q->bounce_gfp | gfp_mask);
+			if (!page) {
+				ret = -ENOMEM;
+				break;
+			}
+		}
+
+		if (bio_add_pc_page(q, bio, page, bytes, offset) < bytes) {
+			if (!map_data)
+				__free_page(page);
+			break;
+		}
+
+		len -= bytes;
+		offset = 0;
+	}
+
+	if (ret)
+		goto cleanup;
+
+	if (map_data)
+		map_data->offset += bio->bi_iter.bi_size;
+
+	/*
+	 * success
+	 */
+	if (((iter->type & WRITE) && (!map_data || !map_data->null_mapped)) ||
+	    (map_data && map_data->from_user)) {
+		ret = bio_copy_from_iter(bio, iter);
+		if (ret)
+			goto cleanup;
+	} else {
+		if (bmd->is_our_pages)
+			zero_fill_bio(bio);
+		iov_iter_advance(iter, bio->bi_iter.bi_size);
+	}
+
+	bio->bi_private = bmd;
+	if (map_data && map_data->null_mapped)
+		bio_set_flag(bio, BIO_NULL_MAPPED);
+	return bio;
+cleanup:
+	if (!map_data)
+		bio_free_pages(bio);
+	bio_put(bio);
+out_bmd:
+	kfree(bmd);
+	return ERR_PTR(ret);
+}
+#else
 struct bio *bio_copy_user_iov(struct request_queue *q,
 			      struct rq_map_data *map_data,
 			      struct iov_iter *iter,
@@ -1282,6 +1610,7 @@ struct bio *bio_copy_user_iov(struct request_queue *q,
 	kfree(bmd);
 	return ERR_PTR(ret);
 }
+#endif
 
 /**
  *	bio_map_user_iov - map user iovec into bio
@@ -1292,6 +1621,97 @@ struct bio *bio_copy_user_iov(struct request_queue *q,
  *	Map the user space address into a bio suitable for io to a block
  *	device. Returns an error pointer in case of error.
  */
+#ifdef CONFIG_DEBUG_KMALLOC
+struct bio *bio_map_user_iov(struct request_queue *q,
+			     struct iov_iter *iter,
+			     gfp_t gfp_mask)
+{
+	int j;
+	struct bio *bio;
+	int ret;
+	struct bio_vec *bvec;
+	unsigned long caller_address = _RET_IP_;
+	call_function_t cf_type = MEMPOOL_ALLOC;
+
+	if (!iov_iter_count(iter))
+		return ERR_PTR(-EINVAL);
+
+	bio = bio_alloc_bioset_memhook(gfp_mask, iov_iter_npages(iter, BIO_MAX_PAGES), NULL, caller_address, cf_type);
+	if (!bio)
+		return ERR_PTR(-ENOMEM);
+
+	while (iov_iter_count(iter)) {
+		struct page **pages;
+		ssize_t bytes;
+		size_t offs, added = 0;
+		int npages;
+
+		bytes = iov_iter_get_pages_alloc(iter, &pages, LONG_MAX, &offs);
+		if (unlikely(bytes <= 0)) {
+			ret = bytes ? bytes : -EFAULT;
+			goto out_unmap;
+		}
+
+		npages = DIV_ROUND_UP(offs + bytes, PAGE_SIZE);
+
+		if (unlikely(offs & queue_dma_alignment(q))) {
+			ret = -EINVAL;
+			j = 0;
+		} else {
+			for (j = 0; j < npages; j++) {
+				struct page *page = pages[j];
+				unsigned int n = PAGE_SIZE - offs;
+				unsigned short prev_bi_vcnt = bio->bi_vcnt;
+
+				if (n > bytes)
+					n = bytes;
+
+				if (!bio_add_pc_page(q, bio, page, n, offs))
+					break;
+
+				/*
+				 * check if vector was merged with previous
+				 * drop page reference if needed
+				 */
+				if (bio->bi_vcnt == prev_bi_vcnt)
+					put_page(page);
+
+				added += n;
+				bytes -= n;
+				offs = 0;
+			}
+			iov_iter_advance(iter, added);
+		}
+		/*
+		 * release the pages we didn't map into the bio, if any
+		 */
+		while (j < npages)
+			put_page(pages[j++]);
+		kvfree(pages);
+		/* couldn't stuff something into bio? */
+		if (bytes)
+			break;
+	}
+
+	bio_set_flag(bio, BIO_USER_MAPPED);
+
+	/*
+	 * subtle -- if bio_map_user_iov() ended up bouncing a bio,
+	 * it would normally disappear when its bi_end_io is run.
+	 * however, we need it for the unmap, so grab an extra
+	 * reference to it
+	 */
+	bio_get(bio);
+	return bio;
+
+ out_unmap:
+	bio_for_each_segment_all(bvec, bio, j) {
+		put_page(bvec->bv_page);
+	}
+	bio_put(bio);
+	return ERR_PTR(ret);
+}
+#else
 struct bio *bio_map_user_iov(struct request_queue *q,
 			     struct iov_iter *iter,
 			     gfp_t gfp_mask)
@@ -1379,6 +1799,7 @@ struct bio *bio_map_user_iov(struct request_queue *q,
 	bio_put(bio);
 	return ERR_PTR(ret);
 }
+#endif
 
 static void __bio_unmap_user(struct bio *bio)
 {
@@ -1428,6 +1849,50 @@ static void bio_map_kern_endio(struct bio *bio)
  *	Map the kernel address into a bio suitable for io to a block
  *	device. Returns an error pointer in case of error.
  */
+#ifdef CONFIG_DEBUG_KMALLOC
+struct bio *bio_map_kern(struct request_queue *q, void *data, unsigned int len,
+			 gfp_t gfp_mask)
+{
+	unsigned long kaddr = (unsigned long)data;
+	unsigned long end = (kaddr + len + PAGE_SIZE - 1) >> PAGE_SHIFT;
+	unsigned long start = kaddr >> PAGE_SHIFT;
+	const int nr_pages = end - start;
+	int offset, i;
+	struct bio *bio;
+	unsigned long caller_address = _RET_IP_;
+	call_function_t cf_type = MEMPOOL_ALLOC;
+
+	bio = bio_alloc_bioset_memhook(gfp_mask, nr_pages, NULL, caller_address, cf_type);
+	if (!bio)
+		return ERR_PTR(-ENOMEM);
+
+	offset = offset_in_page(kaddr);
+	for (i = 0; i < nr_pages; i++) {
+		unsigned int bytes = PAGE_SIZE - offset;
+
+		if (len <= 0)
+			break;
+
+		if (bytes > len)
+			bytes = len;
+
+		if (bio_add_pc_page(q, bio, virt_to_page(data), bytes,
+				    offset) < bytes) {
+			/* we don't support partial mappings */
+			bio_put(bio);
+			return ERR_PTR(-EINVAL);
+		}
+
+		data += bytes;
+		len -= bytes;
+		offset = 0;
+	}
+
+	bio->bi_end_io = bio_map_kern_endio;
+	return bio;
+}
+EXPORT_SYMBOL(bio_map_kern);
+#else
 struct bio *bio_map_kern(struct request_queue *q, void *data, unsigned int len,
 			 gfp_t gfp_mask)
 {
@@ -1468,6 +1933,7 @@ struct bio *bio_map_kern(struct request_queue *q, void *data, unsigned int len,
 	return bio;
 }
 EXPORT_SYMBOL(bio_map_kern);
+#endif
 
 static void bio_copy_kern_endio(struct bio *bio)
 {
@@ -1500,6 +1966,66 @@ static void bio_copy_kern_endio_read(struct bio *bio)
  *	copy the kernel address into a bio suitable for io to a block
  *	device. Returns an error pointer in case of error.
  */
+#ifdef CONFIG_DEBUG_KMALLOC
+struct bio *bio_copy_kern(struct request_queue *q, void *data, unsigned int len,
+			  gfp_t gfp_mask, int reading)
+{
+	unsigned long kaddr = (unsigned long)data;
+	unsigned long end = (kaddr + len + PAGE_SIZE - 1) >> PAGE_SHIFT;
+	unsigned long start = kaddr >> PAGE_SHIFT;
+	struct bio *bio;
+	void *p = data;
+	int nr_pages = 0;
+	unsigned long caller_address = _RET_IP_;
+	call_function_t cf_type = MEMPOOL_ALLOC;
+
+	/*
+	 * Overflow, abort
+	 */
+	if (end < start)
+		return ERR_PTR(-EINVAL);
+
+	nr_pages = end - start;
+	bio = bio_alloc_bioset_memhook(gfp_mask, nr_pages, NULL, caller_address, cf_type);
+	if (!bio)
+		return ERR_PTR(-ENOMEM);
+
+	while (len) {
+		struct page *page;
+		unsigned int bytes = PAGE_SIZE;
+
+		if (bytes > len)
+			bytes = len;
+
+		page = alloc_page(q->bounce_gfp | gfp_mask);
+		if (!page)
+			goto cleanup;
+
+		if (!reading)
+			memcpy(page_address(page), p, bytes);
+
+		if (bio_add_pc_page(q, bio, page, bytes, 0) < bytes)
+			break;
+
+		len -= bytes;
+		p += bytes;
+	}
+
+	if (reading) {
+		bio->bi_end_io = bio_copy_kern_endio_read;
+		bio->bi_private = data;
+	} else {
+		bio->bi_end_io = bio_copy_kern_endio;
+	}
+
+	return bio;
+
+cleanup:
+	bio_free_pages(bio);
+	bio_put(bio);
+	return ERR_PTR(-ENOMEM);
+}
+#else
 struct bio *bio_copy_kern(struct request_queue *q, void *data, unsigned int len,
 			  gfp_t gfp_mask, int reading)
 {
@@ -1556,6 +2082,7 @@ struct bio *bio_copy_kern(struct request_queue *q, void *data, unsigned int len,
 	bio_put(bio);
 	return ERR_PTR(-ENOMEM);
 }
+#endif
 
 /*
  * bio_set_pages_dirty() and bio_check_pages_dirty() are support functions
@@ -1812,6 +2339,44 @@ EXPORT_SYMBOL(bio_endio);
  * to @bio's bi_io_vec; it is the caller's responsibility to ensure that
  * @bio is not freed before the split.
  */
+#ifdef CONFIG_DEBUG_KMALLOC
+struct bio *bio_split_memhook(struct bio *bio, int sectors,
+		      gfp_t gfp, struct bio_set *bs, unsigned long caller_address, call_function_t cf_type)
+{
+	struct bio *split;
+
+	BUG_ON(sectors <= 0);
+	BUG_ON(sectors >= bio_sectors(bio));
+
+	split = bio_clone_fast_memhook(bio, gfp, bs, caller_address, cf_type);
+	if (!split)
+		return NULL;
+
+	split->bi_iter.bi_size = sectors << 9;
+
+	if (bio_integrity(split))
+		bio_integrity_trim(split);
+
+	bio_advance(bio, split->bi_iter.bi_size);
+	bio->bi_iter.bi_done = 0;
+
+	if (bio_flagged(bio, BIO_TRACE_COMPLETION))
+		bio_set_flag(split, BIO_TRACE_COMPLETION);
+
+	return split;
+}
+EXPORT_SYMBOL(bio_split_memhook);
+
+struct bio *bio_split(struct bio *bio, int sectors,
+		      gfp_t gfp, struct bio_set *bs)
+{
+	unsigned long caller_address = _RET_IP_;
+	call_function_t cf_type = MEMPOOL_ALLOC;
+
+	return bio_split_memhook(bio, sectors, gfp, bs, caller_address, cf_type);
+}
+EXPORT_SYMBOL(bio_split);
+#else
 struct bio *bio_split(struct bio *bio, int sectors,
 		      gfp_t gfp, struct bio_set *bs)
 {
@@ -1838,6 +2403,7 @@ struct bio *bio_split(struct bio *bio, int sectors,
 	return split;
 }
 EXPORT_SYMBOL(bio_split);
+#endif
 
 /**
  * bio_trim - trim a bio
diff --git a/block/blk-core.c b/block/blk-core.c
index 93970be37c17..8b95b84a8cea 100644
--- a/block/blk-core.c
+++ b/block/blk-core.c
@@ -2461,6 +2461,10 @@ blk_qc_t generic_make_request(struct bio *bio)
 			/* Create a fresh bio_list for all subordinate requests */
 			bio_list_on_stack[1] = bio_list_on_stack[0];
 			bio_list_init(&bio_list_on_stack[0]);
+
+// #ifdef CONFIG_DEBUG_KMALLOC
+// 			printk("call-generic_make_request:%lx\n", (unsigned long)_RET_IP_);
+// #endif
 			ret = q->make_request_fn(q, bio);
 
 			/* sort new bios into those for a lower level
@@ -2569,7 +2573,13 @@ blk_qc_t submit_bio(struct bio *bio)
 				bio_devname(bio, b), count);
 		}
 	}
-
+	// printk("submit-bio:%lx\n",(unsigned long)bio);
+	record_to_sysfs(ksize(bio), bio, (unsigned long)bio->bi_disk->queue->make_request_fn, MEMPOOL_ALLOC);
+	// mod = __module_text_address((unsigned long)bio->bi_disk->queue->make_request_fn);
+	// if(mod)
+	// 	printk(KERN_DEBUG "bio %s:%lx\n",mod->name, (unsigned long)bio->bi_disk->queue->make_request_fn);
+	// else
+	// 	printk(KERN_DEBUG "bio:%lx\n",(unsigned long)bio->bi_disk->queue->make_request_fn);
 	return generic_make_request(bio);
 }
 EXPORT_SYMBOL(submit_bio);
diff --git a/fs/buffer.c b/fs/buffer.c
index 2a213e8bb97c..cc521d99528d 100644
--- a/fs/buffer.c
+++ b/fs/buffer.c
@@ -3087,7 +3087,10 @@ static int submit_bh_wbc(int op, int op_flags, struct buffer_head *bh,
 	if (buffer_prio(bh))
 		op_flags |= REQ_PRIO;
 	bio_set_op_attrs(bio, op, op_flags);
-
+// #ifdef CONFIG_DEBUG_KMALLOC
+// 	printk(KERN_DEBUG "(my)submit_bh_wbc:%lx\n", (unsigned long)bio);
+//     // find_module_by_stack(0);
+// #endif
 	submit_bio(bio);
 	return 0;
 }
diff --git a/include/asm-generic/sections.h b/include/asm-generic/sections.h
index ea5987bb0b84..65086f94408e 100644
--- a/include/asm-generic/sections.h
+++ b/include/asm-generic/sections.h
@@ -32,6 +32,9 @@
  *	__softirqentry_text_start, __softirqentry_text_end
  *	__start_opd, __end_opd
  */
+
+// extern char __test_module_start[], __test_module_end[];
+
 extern char _text[], _stext[], _etext[];
 extern char _data[], _sdata[], _edata[];
 extern char __bss_start[], __bss_stop[];
diff --git a/include/dt-bindings/input/linux-event-codes.h b/include/dt-bindings/input/linux-event-codes.h
deleted file mode 120000
index 693bbcd2670a..000000000000
--- a/include/dt-bindings/input/linux-event-codes.h
+++ /dev/null
@@ -1 +0,0 @@
-../../uapi/linux/input-event-codes.h
\ No newline at end of file
diff --git a/include/dt-bindings/input/linux-event-codes.h b/include/dt-bindings/input/linux-event-codes.h
new file mode 100644
index 000000000000..c3e84f7c8261
--- /dev/null
+++ b/include/dt-bindings/input/linux-event-codes.h
@@ -0,0 +1,852 @@
+/* SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note */
+/*
+ * Input event codes
+ *
+ *    *** IMPORTANT ***
+ * This file is not only included from C-code but also from devicetree source
+ * files. As such this file MUST only contain comments and defines.
+ *
+ * Copyright (c) 1999-2002 Vojtech Pavlik
+ * Copyright (c) 2015 Hans de Goede <hdegoede@redhat.com>
+ *
+ * This program is free software; you can redistribute it and/or modify it
+ * under the terms of the GNU General Public License version 2 as published by
+ * the Free Software Foundation.
+ */
+#ifndef _UAPI_INPUT_EVENT_CODES_H
+#define _UAPI_INPUT_EVENT_CODES_H
+
+/*
+ * Device properties and quirks
+ */
+
+#define INPUT_PROP_POINTER		0x00	/* needs a pointer */
+#define INPUT_PROP_DIRECT		0x01	/* direct input devices */
+#define INPUT_PROP_BUTTONPAD		0x02	/* has button(s) under pad */
+#define INPUT_PROP_SEMI_MT		0x03	/* touch rectangle only */
+#define INPUT_PROP_TOPBUTTONPAD		0x04	/* softbuttons at top of pad */
+#define INPUT_PROP_POINTING_STICK	0x05	/* is a pointing stick */
+#define INPUT_PROP_ACCELEROMETER	0x06	/* has accelerometer */
+
+#define INPUT_PROP_MAX			0x1f
+#define INPUT_PROP_CNT			(INPUT_PROP_MAX + 1)
+
+/*
+ * Event types
+ */
+
+#define EV_SYN			0x00
+#define EV_KEY			0x01
+#define EV_REL			0x02
+#define EV_ABS			0x03
+#define EV_MSC			0x04
+#define EV_SW			0x05
+#define EV_LED			0x11
+#define EV_SND			0x12
+#define EV_REP			0x14
+#define EV_FF			0x15
+#define EV_PWR			0x16
+#define EV_FF_STATUS		0x17
+#define EV_MAX			0x1f
+#define EV_CNT			(EV_MAX+1)
+
+/*
+ * Synchronization events.
+ */
+
+#define SYN_REPORT		0
+#define SYN_CONFIG		1
+#define SYN_MT_REPORT		2
+#define SYN_DROPPED		3
+#define SYN_MAX			0xf
+#define SYN_CNT			(SYN_MAX+1)
+
+/*
+ * Keys and buttons
+ *
+ * Most of the keys/buttons are modeled after USB HUT 1.12
+ * (see http://www.usb.org/developers/hidpage).
+ * Abbreviations in the comments:
+ * AC - Application Control
+ * AL - Application Launch Button
+ * SC - System Control
+ */
+
+#define KEY_RESERVED		0
+#define KEY_ESC			1
+#define KEY_1			2
+#define KEY_2			3
+#define KEY_3			4
+#define KEY_4			5
+#define KEY_5			6
+#define KEY_6			7
+#define KEY_7			8
+#define KEY_8			9
+#define KEY_9			10
+#define KEY_0			11
+#define KEY_MINUS		12
+#define KEY_EQUAL		13
+#define KEY_BACKSPACE		14
+#define KEY_TAB			15
+#define KEY_Q			16
+#define KEY_W			17
+#define KEY_E			18
+#define KEY_R			19
+#define KEY_T			20
+#define KEY_Y			21
+#define KEY_U			22
+#define KEY_I			23
+#define KEY_O			24
+#define KEY_P			25
+#define KEY_LEFTBRACE		26
+#define KEY_RIGHTBRACE		27
+#define KEY_ENTER		28
+#define KEY_LEFTCTRL		29
+#define KEY_A			30
+#define KEY_S			31
+#define KEY_D			32
+#define KEY_F			33
+#define KEY_G			34
+#define KEY_H			35
+#define KEY_J			36
+#define KEY_K			37
+#define KEY_L			38
+#define KEY_SEMICOLON		39
+#define KEY_APOSTROPHE		40
+#define KEY_GRAVE		41
+#define KEY_LEFTSHIFT		42
+#define KEY_BACKSLASH		43
+#define KEY_Z			44
+#define KEY_X			45
+#define KEY_C			46
+#define KEY_V			47
+#define KEY_B			48
+#define KEY_N			49
+#define KEY_M			50
+#define KEY_COMMA		51
+#define KEY_DOT			52
+#define KEY_SLASH		53
+#define KEY_RIGHTSHIFT		54
+#define KEY_KPASTERISK		55
+#define KEY_LEFTALT		56
+#define KEY_SPACE		57
+#define KEY_CAPSLOCK		58
+#define KEY_F1			59
+#define KEY_F2			60
+#define KEY_F3			61
+#define KEY_F4			62
+#define KEY_F5			63
+#define KEY_F6			64
+#define KEY_F7			65
+#define KEY_F8			66
+#define KEY_F9			67
+#define KEY_F10			68
+#define KEY_NUMLOCK		69
+#define KEY_SCROLLLOCK		70
+#define KEY_KP7			71
+#define KEY_KP8			72
+#define KEY_KP9			73
+#define KEY_KPMINUS		74
+#define KEY_KP4			75
+#define KEY_KP5			76
+#define KEY_KP6			77
+#define KEY_KPPLUS		78
+#define KEY_KP1			79
+#define KEY_KP2			80
+#define KEY_KP3			81
+#define KEY_KP0			82
+#define KEY_KPDOT		83
+
+#define KEY_ZENKAKUHANKAKU	85
+#define KEY_102ND		86
+#define KEY_F11			87
+#define KEY_F12			88
+#define KEY_RO			89
+#define KEY_KATAKANA		90
+#define KEY_HIRAGANA		91
+#define KEY_HENKAN		92
+#define KEY_KATAKANAHIRAGANA	93
+#define KEY_MUHENKAN		94
+#define KEY_KPJPCOMMA		95
+#define KEY_KPENTER		96
+#define KEY_RIGHTCTRL		97
+#define KEY_KPSLASH		98
+#define KEY_SYSRQ		99
+#define KEY_RIGHTALT		100
+#define KEY_LINEFEED		101
+#define KEY_HOME		102
+#define KEY_UP			103
+#define KEY_PAGEUP		104
+#define KEY_LEFT		105
+#define KEY_RIGHT		106
+#define KEY_END			107
+#define KEY_DOWN		108
+#define KEY_PAGEDOWN		109
+#define KEY_INSERT		110
+#define KEY_DELETE		111
+#define KEY_MACRO		112
+#define KEY_MUTE		113
+#define KEY_VOLUMEDOWN		114
+#define KEY_VOLUMEUP		115
+#define KEY_POWER		116	/* SC System Power Down */
+#define KEY_KPEQUAL		117
+#define KEY_KPPLUSMINUS		118
+#define KEY_PAUSE		119
+#define KEY_SCALE		120	/* AL Compiz Scale (Expose) */
+
+#define KEY_KPCOMMA		121
+#define KEY_HANGEUL		122
+#define KEY_HANGUEL		KEY_HANGEUL
+#define KEY_HANJA		123
+#define KEY_YEN			124
+#define KEY_LEFTMETA		125
+#define KEY_RIGHTMETA		126
+#define KEY_COMPOSE		127
+
+#define KEY_STOP		128	/* AC Stop */
+#define KEY_AGAIN		129
+#define KEY_PROPS		130	/* AC Properties */
+#define KEY_UNDO		131	/* AC Undo */
+#define KEY_FRONT		132
+#define KEY_COPY		133	/* AC Copy */
+#define KEY_OPEN		134	/* AC Open */
+#define KEY_PASTE		135	/* AC Paste */
+#define KEY_FIND		136	/* AC Search */
+#define KEY_CUT			137	/* AC Cut */
+#define KEY_HELP		138	/* AL Integrated Help Center */
+#define KEY_MENU		139	/* Menu (show menu) */
+#define KEY_CALC		140	/* AL Calculator */
+#define KEY_SETUP		141
+#define KEY_SLEEP		142	/* SC System Sleep */
+#define KEY_WAKEUP		143	/* System Wake Up */
+#define KEY_FILE		144	/* AL Local Machine Browser */
+#define KEY_SENDFILE		145
+#define KEY_DELETEFILE		146
+#define KEY_XFER		147
+#define KEY_PROG1		148
+#define KEY_PROG2		149
+#define KEY_WWW			150	/* AL Internet Browser */
+#define KEY_MSDOS		151
+#define KEY_COFFEE		152	/* AL Terminal Lock/Screensaver */
+#define KEY_SCREENLOCK		KEY_COFFEE
+#define KEY_ROTATE_DISPLAY	153	/* Display orientation for e.g. tablets */
+#define KEY_DIRECTION		KEY_ROTATE_DISPLAY
+#define KEY_CYCLEWINDOWS	154
+#define KEY_MAIL		155
+#define KEY_BOOKMARKS		156	/* AC Bookmarks */
+#define KEY_COMPUTER		157
+#define KEY_BACK		158	/* AC Back */
+#define KEY_FORWARD		159	/* AC Forward */
+#define KEY_CLOSECD		160
+#define KEY_EJECTCD		161
+#define KEY_EJECTCLOSECD	162
+#define KEY_NEXTSONG		163
+#define KEY_PLAYPAUSE		164
+#define KEY_PREVIOUSSONG	165
+#define KEY_STOPCD		166
+#define KEY_RECORD		167
+#define KEY_REWIND		168
+#define KEY_PHONE		169	/* Media Select Telephone */
+#define KEY_ISO			170
+#define KEY_CONFIG		171	/* AL Consumer Control Configuration */
+#define KEY_HOMEPAGE		172	/* AC Home */
+#define KEY_REFRESH		173	/* AC Refresh */
+#define KEY_EXIT		174	/* AC Exit */
+#define KEY_MOVE		175
+#define KEY_EDIT		176
+#define KEY_SCROLLUP		177
+#define KEY_SCROLLDOWN		178
+#define KEY_KPLEFTPAREN		179
+#define KEY_KPRIGHTPAREN	180
+#define KEY_NEW			181	/* AC New */
+#define KEY_REDO		182	/* AC Redo/Repeat */
+
+#define KEY_F13			183
+#define KEY_F14			184
+#define KEY_F15			185
+#define KEY_F16			186
+#define KEY_F17			187
+#define KEY_F18			188
+#define KEY_F19			189
+#define KEY_F20			190
+#define KEY_F21			191
+#define KEY_F22			192
+#define KEY_F23			193
+#define KEY_F24			194
+
+#define KEY_PLAYCD		200
+#define KEY_PAUSECD		201
+#define KEY_PROG3		202
+#define KEY_PROG4		203
+#define KEY_DASHBOARD		204	/* AL Dashboard */
+#define KEY_SUSPEND		205
+#define KEY_CLOSE		206	/* AC Close */
+#define KEY_PLAY		207
+#define KEY_FASTFORWARD		208
+#define KEY_BASSBOOST		209
+#define KEY_PRINT		210	/* AC Print */
+#define KEY_HP			211
+#define KEY_CAMERA		212
+#define KEY_SOUND		213
+#define KEY_QUESTION		214
+#define KEY_EMAIL		215
+#define KEY_CHAT		216
+#define KEY_SEARCH		217
+#define KEY_CONNECT		218
+#define KEY_FINANCE		219	/* AL Checkbook/Finance */
+#define KEY_SPORT		220
+#define KEY_SHOP		221
+#define KEY_ALTERASE		222
+#define KEY_CANCEL		223	/* AC Cancel */
+#define KEY_BRIGHTNESSDOWN	224
+#define KEY_BRIGHTNESSUP	225
+#define KEY_MEDIA		226
+
+#define KEY_SWITCHVIDEOMODE	227	/* Cycle between available video
+					   outputs (Monitor/LCD/TV-out/etc) */
+#define KEY_KBDILLUMTOGGLE	228
+#define KEY_KBDILLUMDOWN	229
+#define KEY_KBDILLUMUP		230
+
+#define KEY_SEND		231	/* AC Send */
+#define KEY_REPLY		232	/* AC Reply */
+#define KEY_FORWARDMAIL		233	/* AC Forward Msg */
+#define KEY_SAVE		234	/* AC Save */
+#define KEY_DOCUMENTS		235
+
+#define KEY_BATTERY		236
+
+#define KEY_BLUETOOTH		237
+#define KEY_WLAN		238
+#define KEY_UWB			239
+
+#define KEY_UNKNOWN		240
+
+#define KEY_VIDEO_NEXT		241	/* drive next video source */
+#define KEY_VIDEO_PREV		242	/* drive previous video source */
+#define KEY_BRIGHTNESS_CYCLE	243	/* brightness up, after max is min */
+#define KEY_BRIGHTNESS_AUTO	244	/* Set Auto Brightness: manual
+					  brightness control is off,
+					  rely on ambient */
+#define KEY_BRIGHTNESS_ZERO	KEY_BRIGHTNESS_AUTO
+#define KEY_DISPLAY_OFF		245	/* display device to off state */
+
+#define KEY_WWAN		246	/* Wireless WAN (LTE, UMTS, GSM, etc.) */
+#define KEY_WIMAX		KEY_WWAN
+#define KEY_RFKILL		247	/* Key that controls all radios */
+
+#define KEY_MICMUTE		248	/* Mute / unmute the microphone */
+
+/* Code 255 is reserved for special needs of AT keyboard driver */
+
+#define BTN_MISC		0x100
+#define BTN_0			0x100
+#define BTN_1			0x101
+#define BTN_2			0x102
+#define BTN_3			0x103
+#define BTN_4			0x104
+#define BTN_5			0x105
+#define BTN_6			0x106
+#define BTN_7			0x107
+#define BTN_8			0x108
+#define BTN_9			0x109
+
+#define BTN_MOUSE		0x110
+#define BTN_LEFT		0x110
+#define BTN_RIGHT		0x111
+#define BTN_MIDDLE		0x112
+#define BTN_SIDE		0x113
+#define BTN_EXTRA		0x114
+#define BTN_FORWARD		0x115
+#define BTN_BACK		0x116
+#define BTN_TASK		0x117
+
+#define BTN_JOYSTICK		0x120
+#define BTN_TRIGGER		0x120
+#define BTN_THUMB		0x121
+#define BTN_THUMB2		0x122
+#define BTN_TOP			0x123
+#define BTN_TOP2		0x124
+#define BTN_PINKIE		0x125
+#define BTN_BASE		0x126
+#define BTN_BASE2		0x127
+#define BTN_BASE3		0x128
+#define BTN_BASE4		0x129
+#define BTN_BASE5		0x12a
+#define BTN_BASE6		0x12b
+#define BTN_DEAD		0x12f
+
+#define BTN_GAMEPAD		0x130
+#define BTN_SOUTH		0x130
+#define BTN_A			BTN_SOUTH
+#define BTN_EAST		0x131
+#define BTN_B			BTN_EAST
+#define BTN_C			0x132
+#define BTN_NORTH		0x133
+#define BTN_X			BTN_NORTH
+#define BTN_WEST		0x134
+#define BTN_Y			BTN_WEST
+#define BTN_Z			0x135
+#define BTN_TL			0x136
+#define BTN_TR			0x137
+#define BTN_TL2			0x138
+#define BTN_TR2			0x139
+#define BTN_SELECT		0x13a
+#define BTN_START		0x13b
+#define BTN_MODE		0x13c
+#define BTN_THUMBL		0x13d
+#define BTN_THUMBR		0x13e
+
+#define BTN_DIGI		0x140
+#define BTN_TOOL_PEN		0x140
+#define BTN_TOOL_RUBBER		0x141
+#define BTN_TOOL_BRUSH		0x142
+#define BTN_TOOL_PENCIL		0x143
+#define BTN_TOOL_AIRBRUSH	0x144
+#define BTN_TOOL_FINGER		0x145
+#define BTN_TOOL_MOUSE		0x146
+#define BTN_TOOL_LENS		0x147
+#define BTN_TOOL_QUINTTAP	0x148	/* Five fingers on trackpad */
+#define BTN_STYLUS3		0x149
+#define BTN_TOUCH		0x14a
+#define BTN_STYLUS		0x14b
+#define BTN_STYLUS2		0x14c
+#define BTN_TOOL_DOUBLETAP	0x14d
+#define BTN_TOOL_TRIPLETAP	0x14e
+#define BTN_TOOL_QUADTAP	0x14f	/* Four fingers on trackpad */
+
+#define BTN_WHEEL		0x150
+#define BTN_GEAR_DOWN		0x150
+#define BTN_GEAR_UP		0x151
+
+#define KEY_OK			0x160
+#define KEY_SELECT		0x161
+#define KEY_GOTO		0x162
+#define KEY_CLEAR		0x163
+#define KEY_POWER2		0x164
+#define KEY_OPTION		0x165
+#define KEY_INFO		0x166	/* AL OEM Features/Tips/Tutorial */
+#define KEY_TIME		0x167
+#define KEY_VENDOR		0x168
+#define KEY_ARCHIVE		0x169
+#define KEY_PROGRAM		0x16a	/* Media Select Program Guide */
+#define KEY_CHANNEL		0x16b
+#define KEY_FAVORITES		0x16c
+#define KEY_EPG			0x16d
+#define KEY_PVR			0x16e	/* Media Select Home */
+#define KEY_MHP			0x16f
+#define KEY_LANGUAGE		0x170
+#define KEY_TITLE		0x171
+#define KEY_SUBTITLE		0x172
+#define KEY_ANGLE		0x173
+#define KEY_ZOOM		0x174
+#define KEY_MODE		0x175
+#define KEY_KEYBOARD		0x176
+#define KEY_SCREEN		0x177
+#define KEY_PC			0x178	/* Media Select Computer */
+#define KEY_TV			0x179	/* Media Select TV */
+#define KEY_TV2			0x17a	/* Media Select Cable */
+#define KEY_VCR			0x17b	/* Media Select VCR */
+#define KEY_VCR2		0x17c	/* VCR Plus */
+#define KEY_SAT			0x17d	/* Media Select Satellite */
+#define KEY_SAT2		0x17e
+#define KEY_CD			0x17f	/* Media Select CD */
+#define KEY_TAPE		0x180	/* Media Select Tape */
+#define KEY_RADIO		0x181
+#define KEY_TUNER		0x182	/* Media Select Tuner */
+#define KEY_PLAYER		0x183
+#define KEY_TEXT		0x184
+#define KEY_DVD			0x185	/* Media Select DVD */
+#define KEY_AUX			0x186
+#define KEY_MP3			0x187
+#define KEY_AUDIO		0x188	/* AL Audio Browser */
+#define KEY_VIDEO		0x189	/* AL Movie Browser */
+#define KEY_DIRECTORY		0x18a
+#define KEY_LIST		0x18b
+#define KEY_MEMO		0x18c	/* Media Select Messages */
+#define KEY_CALENDAR		0x18d
+#define KEY_RED			0x18e
+#define KEY_GREEN		0x18f
+#define KEY_YELLOW		0x190
+#define KEY_BLUE		0x191
+#define KEY_CHANNELUP		0x192	/* Channel Increment */
+#define KEY_CHANNELDOWN		0x193	/* Channel Decrement */
+#define KEY_FIRST		0x194
+#define KEY_LAST		0x195	/* Recall Last */
+#define KEY_AB			0x196
+#define KEY_NEXT		0x197
+#define KEY_RESTART		0x198
+#define KEY_SLOW		0x199
+#define KEY_SHUFFLE		0x19a
+#define KEY_BREAK		0x19b
+#define KEY_PREVIOUS		0x19c
+#define KEY_DIGITS		0x19d
+#define KEY_TEEN		0x19e
+#define KEY_TWEN		0x19f
+#define KEY_VIDEOPHONE		0x1a0	/* Media Select Video Phone */
+#define KEY_GAMES		0x1a1	/* Media Select Games */
+#define KEY_ZOOMIN		0x1a2	/* AC Zoom In */
+#define KEY_ZOOMOUT		0x1a3	/* AC Zoom Out */
+#define KEY_ZOOMRESET		0x1a4	/* AC Zoom */
+#define KEY_WORDPROCESSOR	0x1a5	/* AL Word Processor */
+#define KEY_EDITOR		0x1a6	/* AL Text Editor */
+#define KEY_SPREADSHEET		0x1a7	/* AL Spreadsheet */
+#define KEY_GRAPHICSEDITOR	0x1a8	/* AL Graphics Editor */
+#define KEY_PRESENTATION	0x1a9	/* AL Presentation App */
+#define KEY_DATABASE		0x1aa	/* AL Database App */
+#define KEY_NEWS		0x1ab	/* AL Newsreader */
+#define KEY_VOICEMAIL		0x1ac	/* AL Voicemail */
+#define KEY_ADDRESSBOOK		0x1ad	/* AL Contacts/Address Book */
+#define KEY_MESSENGER		0x1ae	/* AL Instant Messaging */
+#define KEY_DISPLAYTOGGLE	0x1af	/* Turn display (LCD) on and off */
+#define KEY_BRIGHTNESS_TOGGLE	KEY_DISPLAYTOGGLE
+#define KEY_SPELLCHECK		0x1b0   /* AL Spell Check */
+#define KEY_LOGOFF		0x1b1   /* AL Logoff */
+
+#define KEY_DOLLAR		0x1b2
+#define KEY_EURO		0x1b3
+
+#define KEY_FRAMEBACK		0x1b4	/* Consumer - transport controls */
+#define KEY_FRAMEFORWARD	0x1b5
+#define KEY_CONTEXT_MENU	0x1b6	/* GenDesc - system context menu */
+#define KEY_MEDIA_REPEAT	0x1b7	/* Consumer - transport control */
+#define KEY_10CHANNELSUP	0x1b8	/* 10 channels up (10+) */
+#define KEY_10CHANNELSDOWN	0x1b9	/* 10 channels down (10-) */
+#define KEY_IMAGES		0x1ba	/* AL Image Browser */
+
+#define KEY_DEL_EOL		0x1c0
+#define KEY_DEL_EOS		0x1c1
+#define KEY_INS_LINE		0x1c2
+#define KEY_DEL_LINE		0x1c3
+
+#define KEY_FN			0x1d0
+#define KEY_FN_ESC		0x1d1
+#define KEY_FN_F1		0x1d2
+#define KEY_FN_F2		0x1d3
+#define KEY_FN_F3		0x1d4
+#define KEY_FN_F4		0x1d5
+#define KEY_FN_F5		0x1d6
+#define KEY_FN_F6		0x1d7
+#define KEY_FN_F7		0x1d8
+#define KEY_FN_F8		0x1d9
+#define KEY_FN_F9		0x1da
+#define KEY_FN_F10		0x1db
+#define KEY_FN_F11		0x1dc
+#define KEY_FN_F12		0x1dd
+#define KEY_FN_1		0x1de
+#define KEY_FN_2		0x1df
+#define KEY_FN_D		0x1e0
+#define KEY_FN_E		0x1e1
+#define KEY_FN_F		0x1e2
+#define KEY_FN_S		0x1e3
+#define KEY_FN_B		0x1e4
+
+#define KEY_BRL_DOT1		0x1f1
+#define KEY_BRL_DOT2		0x1f2
+#define KEY_BRL_DOT3		0x1f3
+#define KEY_BRL_DOT4		0x1f4
+#define KEY_BRL_DOT5		0x1f5
+#define KEY_BRL_DOT6		0x1f6
+#define KEY_BRL_DOT7		0x1f7
+#define KEY_BRL_DOT8		0x1f8
+#define KEY_BRL_DOT9		0x1f9
+#define KEY_BRL_DOT10		0x1fa
+
+#define KEY_NUMERIC_0		0x200	/* used by phones, remote controls, */
+#define KEY_NUMERIC_1		0x201	/* and other keypads */
+#define KEY_NUMERIC_2		0x202
+#define KEY_NUMERIC_3		0x203
+#define KEY_NUMERIC_4		0x204
+#define KEY_NUMERIC_5		0x205
+#define KEY_NUMERIC_6		0x206
+#define KEY_NUMERIC_7		0x207
+#define KEY_NUMERIC_8		0x208
+#define KEY_NUMERIC_9		0x209
+#define KEY_NUMERIC_STAR	0x20a
+#define KEY_NUMERIC_POUND	0x20b
+#define KEY_NUMERIC_A		0x20c	/* Phone key A - HUT Telephony 0xb9 */
+#define KEY_NUMERIC_B		0x20d
+#define KEY_NUMERIC_C		0x20e
+#define KEY_NUMERIC_D		0x20f
+
+#define KEY_CAMERA_FOCUS	0x210
+#define KEY_WPS_BUTTON		0x211	/* WiFi Protected Setup key */
+
+#define KEY_TOUCHPAD_TOGGLE	0x212	/* Request switch touchpad on or off */
+#define KEY_TOUCHPAD_ON		0x213
+#define KEY_TOUCHPAD_OFF	0x214
+
+#define KEY_CAMERA_ZOOMIN	0x215
+#define KEY_CAMERA_ZOOMOUT	0x216
+#define KEY_CAMERA_UP		0x217
+#define KEY_CAMERA_DOWN		0x218
+#define KEY_CAMERA_LEFT		0x219
+#define KEY_CAMERA_RIGHT	0x21a
+
+#define KEY_ATTENDANT_ON	0x21b
+#define KEY_ATTENDANT_OFF	0x21c
+#define KEY_ATTENDANT_TOGGLE	0x21d	/* Attendant call on or off */
+#define KEY_LIGHTS_TOGGLE	0x21e	/* Reading light on or off */
+
+#define BTN_DPAD_UP		0x220
+#define BTN_DPAD_DOWN		0x221
+#define BTN_DPAD_LEFT		0x222
+#define BTN_DPAD_RIGHT		0x223
+
+#define KEY_ALS_TOGGLE		0x230	/* Ambient light sensor */
+#define KEY_ROTATE_LOCK_TOGGLE	0x231	/* Display rotation lock */
+
+#define KEY_BUTTONCONFIG		0x240	/* AL Button Configuration */
+#define KEY_TASKMANAGER		0x241	/* AL Task/Project Manager */
+#define KEY_JOURNAL		0x242	/* AL Log/Journal/Timecard */
+#define KEY_CONTROLPANEL		0x243	/* AL Control Panel */
+#define KEY_APPSELECT		0x244	/* AL Select Task/Application */
+#define KEY_SCREENSAVER		0x245	/* AL Screen Saver */
+#define KEY_VOICECOMMAND		0x246	/* Listening Voice Command */
+#define KEY_ASSISTANT		0x247	/* AL Context-aware desktop assistant */
+
+#define KEY_BRIGHTNESS_MIN		0x250	/* Set Brightness to Minimum */
+#define KEY_BRIGHTNESS_MAX		0x251	/* Set Brightness to Maximum */
+
+#define KEY_KBDINPUTASSIST_PREV		0x260
+#define KEY_KBDINPUTASSIST_NEXT		0x261
+#define KEY_KBDINPUTASSIST_PREVGROUP		0x262
+#define KEY_KBDINPUTASSIST_NEXTGROUP		0x263
+#define KEY_KBDINPUTASSIST_ACCEPT		0x264
+#define KEY_KBDINPUTASSIST_CANCEL		0x265
+
+/* Diagonal movement keys */
+#define KEY_RIGHT_UP			0x266
+#define KEY_RIGHT_DOWN			0x267
+#define KEY_LEFT_UP			0x268
+#define KEY_LEFT_DOWN			0x269
+
+#define KEY_ROOT_MENU			0x26a /* Show Device's Root Menu */
+/* Show Top Menu of the Media (e.g. DVD) */
+#define KEY_MEDIA_TOP_MENU		0x26b
+#define KEY_NUMERIC_11			0x26c
+#define KEY_NUMERIC_12			0x26d
+/*
+ * Toggle Audio Description: refers to an audio service that helps blind and
+ * visually impaired consumers understand the action in a program. Note: in
+ * some countries this is referred to as "Video Description".
+ */
+#define KEY_AUDIO_DESC			0x26e
+#define KEY_3D_MODE			0x26f
+#define KEY_NEXT_FAVORITE		0x270
+#define KEY_STOP_RECORD			0x271
+#define KEY_PAUSE_RECORD		0x272
+#define KEY_VOD				0x273 /* Video on Demand */
+#define KEY_UNMUTE			0x274
+#define KEY_FASTREVERSE			0x275
+#define KEY_SLOWREVERSE			0x276
+/*
+ * Control a data application associated with the currently viewed channel,
+ * e.g. teletext or data broadcast application (MHEG, MHP, HbbTV, etc.)
+ */
+#define KEY_DATA			0x277
+#define KEY_ONSCREEN_KEYBOARD		0x278
+
+#define BTN_TRIGGER_HAPPY		0x2c0
+#define BTN_TRIGGER_HAPPY1		0x2c0
+#define BTN_TRIGGER_HAPPY2		0x2c1
+#define BTN_TRIGGER_HAPPY3		0x2c2
+#define BTN_TRIGGER_HAPPY4		0x2c3
+#define BTN_TRIGGER_HAPPY5		0x2c4
+#define BTN_TRIGGER_HAPPY6		0x2c5
+#define BTN_TRIGGER_HAPPY7		0x2c6
+#define BTN_TRIGGER_HAPPY8		0x2c7
+#define BTN_TRIGGER_HAPPY9		0x2c8
+#define BTN_TRIGGER_HAPPY10		0x2c9
+#define BTN_TRIGGER_HAPPY11		0x2ca
+#define BTN_TRIGGER_HAPPY12		0x2cb
+#define BTN_TRIGGER_HAPPY13		0x2cc
+#define BTN_TRIGGER_HAPPY14		0x2cd
+#define BTN_TRIGGER_HAPPY15		0x2ce
+#define BTN_TRIGGER_HAPPY16		0x2cf
+#define BTN_TRIGGER_HAPPY17		0x2d0
+#define BTN_TRIGGER_HAPPY18		0x2d1
+#define BTN_TRIGGER_HAPPY19		0x2d2
+#define BTN_TRIGGER_HAPPY20		0x2d3
+#define BTN_TRIGGER_HAPPY21		0x2d4
+#define BTN_TRIGGER_HAPPY22		0x2d5
+#define BTN_TRIGGER_HAPPY23		0x2d6
+#define BTN_TRIGGER_HAPPY24		0x2d7
+#define BTN_TRIGGER_HAPPY25		0x2d8
+#define BTN_TRIGGER_HAPPY26		0x2d9
+#define BTN_TRIGGER_HAPPY27		0x2da
+#define BTN_TRIGGER_HAPPY28		0x2db
+#define BTN_TRIGGER_HAPPY29		0x2dc
+#define BTN_TRIGGER_HAPPY30		0x2dd
+#define BTN_TRIGGER_HAPPY31		0x2de
+#define BTN_TRIGGER_HAPPY32		0x2df
+#define BTN_TRIGGER_HAPPY33		0x2e0
+#define BTN_TRIGGER_HAPPY34		0x2e1
+#define BTN_TRIGGER_HAPPY35		0x2e2
+#define BTN_TRIGGER_HAPPY36		0x2e3
+#define BTN_TRIGGER_HAPPY37		0x2e4
+#define BTN_TRIGGER_HAPPY38		0x2e5
+#define BTN_TRIGGER_HAPPY39		0x2e6
+#define BTN_TRIGGER_HAPPY40		0x2e7
+
+/* We avoid low common keys in module aliases so they don't get huge. */
+#define KEY_MIN_INTERESTING	KEY_MUTE
+#define KEY_MAX			0x2ff
+#define KEY_CNT			(KEY_MAX+1)
+
+/*
+ * Relative axes
+ */
+
+#define REL_X			0x00
+#define REL_Y			0x01
+#define REL_Z			0x02
+#define REL_RX			0x03
+#define REL_RY			0x04
+#define REL_RZ			0x05
+#define REL_HWHEEL		0x06
+#define REL_DIAL		0x07
+#define REL_WHEEL		0x08
+#define REL_MISC		0x09
+#define REL_MAX			0x0f
+#define REL_CNT			(REL_MAX+1)
+
+/*
+ * Absolute axes
+ */
+
+#define ABS_X			0x00
+#define ABS_Y			0x01
+#define ABS_Z			0x02
+#define ABS_RX			0x03
+#define ABS_RY			0x04
+#define ABS_RZ			0x05
+#define ABS_THROTTLE		0x06
+#define ABS_RUDDER		0x07
+#define ABS_WHEEL		0x08
+#define ABS_GAS			0x09
+#define ABS_BRAKE		0x0a
+#define ABS_HAT0X		0x10
+#define ABS_HAT0Y		0x11
+#define ABS_HAT1X		0x12
+#define ABS_HAT1Y		0x13
+#define ABS_HAT2X		0x14
+#define ABS_HAT2Y		0x15
+#define ABS_HAT3X		0x16
+#define ABS_HAT3Y		0x17
+#define ABS_PRESSURE		0x18
+#define ABS_DISTANCE		0x19
+#define ABS_TILT_X		0x1a
+#define ABS_TILT_Y		0x1b
+#define ABS_TOOL_WIDTH		0x1c
+
+#define ABS_VOLUME		0x20
+
+#define ABS_MISC		0x28
+
+/*
+ * 0x2e is reserved and should not be used in input drivers.
+ * It was used by HID as ABS_MISC+6 and userspace needs to detect if
+ * the next ABS_* event is correct or is just ABS_MISC + n.
+ * We define here ABS_RESERVED so userspace can rely on it and detect
+ * the situation described above.
+ */
+#define ABS_RESERVED		0x2e
+
+#define ABS_MT_SLOT		0x2f	/* MT slot being modified */
+#define ABS_MT_TOUCH_MAJOR	0x30	/* Major axis of touching ellipse */
+#define ABS_MT_TOUCH_MINOR	0x31	/* Minor axis (omit if circular) */
+#define ABS_MT_WIDTH_MAJOR	0x32	/* Major axis of approaching ellipse */
+#define ABS_MT_WIDTH_MINOR	0x33	/* Minor axis (omit if circular) */
+#define ABS_MT_ORIENTATION	0x34	/* Ellipse orientation */
+#define ABS_MT_POSITION_X	0x35	/* Center X touch position */
+#define ABS_MT_POSITION_Y	0x36	/* Center Y touch position */
+#define ABS_MT_TOOL_TYPE	0x37	/* Type of touching device */
+#define ABS_MT_BLOB_ID		0x38	/* Group a set of packets as a blob */
+#define ABS_MT_TRACKING_ID	0x39	/* Unique ID of initiated contact */
+#define ABS_MT_PRESSURE		0x3a	/* Pressure on contact area */
+#define ABS_MT_DISTANCE		0x3b	/* Contact hover distance */
+#define ABS_MT_TOOL_X		0x3c	/* Center X tool position */
+#define ABS_MT_TOOL_Y		0x3d	/* Center Y tool position */
+
+
+#define ABS_MAX			0x3f
+#define ABS_CNT			(ABS_MAX+1)
+
+/*
+ * Switch events
+ */
+
+#define SW_LID			0x00  /* set = lid shut */
+#define SW_TABLET_MODE		0x01  /* set = tablet mode */
+#define SW_HEADPHONE_INSERT	0x02  /* set = inserted */
+#define SW_RFKILL_ALL		0x03  /* rfkill master switch, type "any"
+					 set = radio enabled */
+#define SW_RADIO		SW_RFKILL_ALL	/* deprecated */
+#define SW_MICROPHONE_INSERT	0x04  /* set = inserted */
+#define SW_DOCK			0x05  /* set = plugged into dock */
+#define SW_LINEOUT_INSERT	0x06  /* set = inserted */
+#define SW_JACK_PHYSICAL_INSERT 0x07  /* set = mechanical switch set */
+#define SW_VIDEOOUT_INSERT	0x08  /* set = inserted */
+#define SW_CAMERA_LENS_COVER	0x09  /* set = lens covered */
+#define SW_KEYPAD_SLIDE		0x0a  /* set = keypad slide out */
+#define SW_FRONT_PROXIMITY	0x0b  /* set = front proximity sensor active */
+#define SW_ROTATE_LOCK		0x0c  /* set = rotate locked/disabled */
+#define SW_LINEIN_INSERT	0x0d  /* set = inserted */
+#define SW_MUTE_DEVICE		0x0e  /* set = device disabled */
+#define SW_PEN_INSERTED		0x0f  /* set = pen inserted */
+#define SW_MACHINE_COVER	0x10  /* set = cover closed */
+#define SW_MAX			0x10
+#define SW_CNT			(SW_MAX+1)
+
+/*
+ * Misc events
+ */
+
+#define MSC_SERIAL		0x00
+#define MSC_PULSELED		0x01
+#define MSC_GESTURE		0x02
+#define MSC_RAW			0x03
+#define MSC_SCAN		0x04
+#define MSC_TIMESTAMP		0x05
+#define MSC_MAX			0x07
+#define MSC_CNT			(MSC_MAX+1)
+
+/*
+ * LEDs
+ */
+
+#define LED_NUML		0x00
+#define LED_CAPSL		0x01
+#define LED_SCROLLL		0x02
+#define LED_COMPOSE		0x03
+#define LED_KANA		0x04
+#define LED_SLEEP		0x05
+#define LED_SUSPEND		0x06
+#define LED_MUTE		0x07
+#define LED_MISC		0x08
+#define LED_MAIL		0x09
+#define LED_CHARGING		0x0a
+#define LED_MAX			0x0f
+#define LED_CNT			(LED_MAX+1)
+
+/*
+ * Autorepeat values
+ */
+
+#define REP_DELAY		0x00
+#define REP_PERIOD		0x01
+#define REP_MAX			0x01
+#define REP_CNT			(REP_MAX+1)
+
+/*
+ * Sounds
+ */
+
+#define SND_CLICK		0x00
+#define SND_BELL		0x01
+#define SND_TONE		0x02
+#define SND_MAX			0x07
+#define SND_CNT			(SND_MAX+1)
+
+#endif
diff --git a/include/linux/dmapool.h b/include/linux/dmapool.h
index f632ecfb4238..572cab1179ac 100644
--- a/include/linux/dmapool.h
+++ b/include/linux/dmapool.h
@@ -27,6 +27,13 @@ void *dma_pool_alloc(struct dma_pool *pool, gfp_t mem_flags,
 		     dma_addr_t *handle);
 void dma_pool_free(struct dma_pool *pool, void *vaddr, dma_addr_t addr);
 
+#ifdef CONFIG_DEBUG_KMALLOC
+void *dma_pool_alloc_memhook(struct dma_pool *pool, gfp_t mem_flags,
+		     dma_addr_t *handle, unsigned long caller_address);
+
+void dma_pool_free_memhook(struct dma_pool *pool, void *vaddr, dma_addr_t addr, unsigned long caller_address);
+#endif
+
 /*
  * Managed DMA pool
  */
diff --git a/include/linux/gfp.h b/include/linux/gfp.h
index f78d1e89593f..31c69f728cb1 100644
--- a/include/linux/gfp.h
+++ b/include/linux/gfp.h
@@ -486,20 +486,66 @@ static inline void arch_free_page(struct page *page, int order) { }
 static inline void arch_alloc_page(struct page *page, int order) { }
 #endif
 
+#ifdef CONFIG_DEBUG_KMALLOC
+extern void* get_pc(void);
+extern void record_page_to_sysfs(unsigned int order, void* page,
+    	unsigned long caller_address);
+extern void free_mem_record_page(unsigned long caller_address, struct page* page, unsigned int orders);
+#endif
+
+#ifdef CONFIG_DEBUG_KMALLOC
+struct page *
+__alloc_pages_nodemask_memhook(gfp_t gfp_mask, unsigned int order, int preferred_nid,
+							nodemask_t *nodemask, unsigned long caller_address);
+#endif
 struct page *
 __alloc_pages_nodemask(gfp_t gfp_mask, unsigned int order, int preferred_nid,
 							nodemask_t *nodemask);
 
+#ifdef CONFIG_DEBUG_KMALLOC
+static inline struct page *
+__alloc_pages_memhook(gfp_t gfp_mask, unsigned int order, int preferred_nid, unsigned long caller_address)
+{
+	return __alloc_pages_nodemask_memhook(gfp_mask, order, preferred_nid, NULL, caller_address);
+}
+
+static inline struct page *
+__alloc_pages(gfp_t gfp_mask, unsigned int order, int preferred_nid)
+{
+	unsigned long caller_address = (unsigned long)get_pc();
+	
+	return __alloc_pages_memhook(gfp_mask, order, preferred_nid, caller_address);
+}
+#else
 static inline struct page *
 __alloc_pages(gfp_t gfp_mask, unsigned int order, int preferred_nid)
 {
 	return __alloc_pages_nodemask(gfp_mask, order, preferred_nid, NULL);
 }
+#endif
 
 /*
  * Allocate pages, preferring the node given as nid. The node must be valid and
  * online. For more general interface, see alloc_pages_node().
  */
+#ifdef CONFIG_DEBUG_KMALLOC
+static inline struct page *
+__alloc_pages_node_memhook(int nid, gfp_t gfp_mask, unsigned int order, unsigned long caller_address)
+{
+	VM_BUG_ON(nid < 0 || nid >= MAX_NUMNODES);
+	VM_WARN_ON((gfp_mask & __GFP_THISNODE) && !node_online(nid));
+
+	return __alloc_pages_memhook(gfp_mask, order, nid, caller_address);
+}
+
+static inline struct page *
+__alloc_pages_node(int nid, gfp_t gfp_mask, unsigned int order)
+{
+	unsigned long caller_address = (unsigned long)get_pc();
+
+	return __alloc_pages_node_memhook(nid, gfp_mask, order, caller_address);
+}
+#else
 static inline struct page *
 __alloc_pages_node(int nid, gfp_t gfp_mask, unsigned int order)
 {
@@ -508,12 +554,30 @@ __alloc_pages_node(int nid, gfp_t gfp_mask, unsigned int order)
 
 	return __alloc_pages(gfp_mask, order, nid);
 }
-
+#endif
 /*
  * Allocate pages, preferring the node given as nid. When nid == NUMA_NO_NODE,
  * prefer the current CPU's closest node. Otherwise node must be valid and
  * online.
  */
+#ifdef CONFIG_DEBUG_KMALLOC
+static inline struct page *alloc_pages_node_memhook(int nid, gfp_t gfp_mask,
+						unsigned int order, unsigned long caller_address)
+{
+	if (nid == NUMA_NO_NODE)
+		nid = numa_mem_id();
+
+	return __alloc_pages_node_memhook(nid, gfp_mask, order, caller_address);
+}
+
+static inline struct page *alloc_pages_node(int nid, gfp_t gfp_mask,
+						unsigned int order)
+{
+	unsigned long caller_address = (unsigned long)get_pc();
+	
+	return alloc_pages_node_memhook(nid, gfp_mask, order, caller_address);
+}
+#else
 static inline struct page *alloc_pages_node(int nid, gfp_t gfp_mask,
 						unsigned int order)
 {
@@ -522,28 +586,68 @@ static inline struct page *alloc_pages_node(int nid, gfp_t gfp_mask,
 
 	return __alloc_pages_node(nid, gfp_mask, order);
 }
+#endif
+
 
 #ifdef CONFIG_NUMA
 extern struct page *alloc_pages_current(gfp_t gfp_mask, unsigned order);
+#ifdef CONFIG_DEBUG_KMALLOC
+extern struct page *alloc_pages_current_memhook(gfp_t gfp_mask, unsigned order, 
+			unsigned long caller_address);
+#endif /*CONFIG_DEBUG_KMALLOC*/
+
+#ifdef CONFIG_DEBUG_KMALLOC
+static inline struct page *
+alloc_pages_memhook(gfp_t gfp_mask, unsigned int order, unsigned long caller_address)
+{
+	return alloc_pages_current_memhook(gfp_mask, order, caller_address);
+}
 
+static inline struct page *
+alloc_pages(gfp_t gfp_mask, unsigned int order)
+{
+	unsigned long caller_address = (unsigned long)get_pc();
+
+	return alloc_pages_memhook(gfp_mask, order, caller_address);
+}
+#else 
 static inline struct page *
 alloc_pages(gfp_t gfp_mask, unsigned int order)
 {
 	return alloc_pages_current(gfp_mask, order);
 }
+#endif/*CONFIG_DEBUG_KMALLOC*/
+
 extern struct page *alloc_pages_vma(gfp_t gfp_mask, int order,
 			struct vm_area_struct *vma, unsigned long addr,
 			int node, bool hugepage);
+#ifdef CONFIG_DEBUG_KMALLOC
+extern struct page *alloc_pages_vma_memhook(gfp_t gfp_mask, int order,
+			struct vm_area_struct *vma, unsigned long addr,
+			int node, bool hugepage, unsigned long caller_address);
+#endif /*CONFIG_DEBUG_KMALLOC*/
+
 #define alloc_hugepage_vma(gfp_mask, vma, addr, order)	\
 	alloc_pages_vma(gfp_mask, order, vma, addr, numa_node_id(), true)
-#else
+
+#else  /*CONFIG_NUMA*/
 #define alloc_pages(gfp_mask, order) \
 		alloc_pages_node(numa_node_id(), gfp_mask, order)
 #define alloc_pages_vma(gfp_mask, order, vma, addr, node, false)\
 	alloc_pages(gfp_mask, order)
 #define alloc_hugepage_vma(gfp_mask, vma, addr, order)	\
 	alloc_pages(gfp_mask, order)
+
+#ifdef CONFIG_DEBUG_KMALLOC
+#define alloc_pages_memhook(gfp_mask, order, caller_address) \
+	alloc_pages_node_memhook(numa_node_id(), gfp_mask, order, caller_address)
+#define alloc_pages_vma_memhook(gfp_mask, order, vma, addr, node, false, caller_address) \
+	alloc_pages_memhook(gfp_mask, order, caller_address)
+#endif  /*CONFIG_DEBUG_KMALLOC*/
+
 #endif
+
+
 #define alloc_page(gfp_mask) alloc_pages(gfp_mask, 0)
 #define alloc_page_vma(gfp_mask, vma, addr)			\
 	alloc_pages_vma(gfp_mask, 0, vma, addr, numa_node_id(), false)
@@ -552,10 +656,24 @@ extern struct page *alloc_pages_vma(gfp_t gfp_mask, int order,
 
 extern unsigned long __get_free_pages(gfp_t gfp_mask, unsigned int order);
 extern unsigned long get_zeroed_page(gfp_t gfp_mask);
+#ifdef CONFIG_DEBUG_KMALLOC
+extern unsigned long __get_free_pages_memhook(gfp_t gfp_mask, unsigned int order,
+			unsigned long caller_address);
+extern unsigned long get_zeroed_page_memhook(gfp_t gfp_mask, 
+			unsigned long caller_address);
+#endif
 
 void *alloc_pages_exact(size_t size, gfp_t gfp_mask);
 void free_pages_exact(void *virt, size_t size);
 void * __meminit alloc_pages_exact_nid(int nid, size_t size, gfp_t gfp_mask);
+#ifdef CONFIG_DEBUG_KMALLOC
+void *alloc_pages_exact_memhook(size_t size, gfp_t gfp_mask, 
+			unsigned long caller_address);
+void free_pages_exact_memhook(void *virt, size_t size, 
+			unsigned long caller_address);
+void * __meminit alloc_pages_exact_nid_memhook(int nid, size_t size, gfp_t gfp_mask,
+			unsigned long caller_address);
+#endif
 
 #define __get_free_page(gfp_mask) \
 		__get_free_pages((gfp_mask), 0)
@@ -567,6 +685,12 @@ extern void __free_pages(struct page *page, unsigned int order);
 extern void free_pages(unsigned long addr, unsigned int order);
 extern void free_unref_page(struct page *page);
 extern void free_unref_page_list(struct list_head *list);
+#ifdef CONFIG_DEBUG_KMALLOC
+extern void __free_pages_memhook(struct page *page, unsigned int order, unsigned long caller_address);
+extern void free_pages_memhook(unsigned long addr, unsigned int order, unsigned long caller_address);
+//extern void free_unref_page_memhook(struct page *page);
+//extern void free_unref_page_list_memhook(struct list_head *list);l
+#endif
 
 struct page_frag_cache;
 extern void __page_frag_cache_drain(struct page *page, unsigned int count);
diff --git a/include/linux/mempool.h b/include/linux/mempool.h
index 0c964ac107c2..be9a2ee292d3 100644
--- a/include/linux/mempool.h
+++ b/include/linux/mempool.h
@@ -8,6 +8,10 @@
 #include <linux/wait.h>
 #include <linux/compiler.h>
 
+#ifdef CONFIG_DEBUG_KMALLOC
+#include <linux/statis_memory.h>
+#endif
+
 struct kmem_cache;
 
 typedef void * (mempool_alloc_t)(gfp_t gfp_mask, void *pool_data);
@@ -108,4 +112,9 @@ static inline mempool_t *mempool_create_page_pool(int min_nr, int order)
 			      (void *)(long)order);
 }
 
+#ifdef CONFIG_DEBUG_KMALLOC
+void *mempool_alloc_memhook(mempool_t *pool, gfp_t gfp_mask, unsigned long caller_address, call_function_t cf_type);
+
+#endif
+
 #endif /* _LINUX_MEMPOOL_H */
diff --git a/include/linux/module.h b/include/linux/module.h
index e49023514820..c76696a0b8a5 100644
--- a/include/linux/module.h
+++ b/include/linux/module.h
@@ -42,6 +42,30 @@ struct modversion_info {
 struct module;
 struct exception_table_entry;
 
+#ifdef CONFIG_DEBUG_KMALLOC
+struct page_record_list {
+    struct page_record_list *next;
+    struct page *page;
+	atomic_t page_elements;
+};
+
+struct module_kobject {
+	struct kobject kobj;
+	spinlock_t record_lock;
+	unsigned long used_memory;
+	unsigned int used_pages;
+	unsigned long used_percpu;
+	unsigned long used_dmapool;
+	struct page_record_list *record_list_slab;
+	struct page_record_list *record_list_page;
+	struct page_record_list *record_list_percpu;
+	struct page_record_list *record_list_dmapool;
+	struct module *mod;
+	struct kobject *drivers_dir;
+	struct module_param_attrs *mp;
+	struct completion *kobj_completion;
+} __randomize_layout;
+#else
 struct module_kobject {
 	struct kobject kobj;
 	struct module *mod;
@@ -49,6 +73,7 @@ struct module_kobject {
 	struct module_param_attrs *mp;
 	struct completion *kobj_completion;
 } __randomize_layout;
+#endif/*CONFIG_DEBUG_KMALLOC*/
 
 struct module_attribute {
 	struct attribute attr;
@@ -72,6 +97,18 @@ extern ssize_t __modver_version_show(struct module_attribute *,
 
 extern struct module_attribute module_uevent;
 
+#ifdef CONFIG_DEBUG_KMALLOC
+
+struct module_usedmemory_attribute {
+	struct module_attribute mattr;
+	struct module* mod;
+	const char *used;
+} __attribute__ ((__aligned__(sizeof(void *))));
+
+extern ssize_t __module_usedmemory_show(struct module_attribute *,
+				     struct module_kobject *, char *);
+#endif
+
 /* These are either module local, or the kernel's dummy ones. */
 extern int init_module(void);
 extern void cleanup_module(void);
@@ -642,6 +679,11 @@ int unregister_module_notifier(struct notifier_block *nb);
 
 extern void print_modules(void);
 
+#ifdef CONFIG_DEBUG_KMALLOC
+extern void print_modules_debug(void);
+struct list_head *get_module_list(void);
+#endif
+
 static inline bool module_requested_async_probing(struct module *module)
 {
 	return module && module->async_probe_requested;
diff --git a/include/linux/percpu.h b/include/linux/percpu.h
index 70b7123f38c7..c5630010e422 100644
--- a/include/linux/percpu.h
+++ b/include/linux/percpu.h
@@ -130,6 +130,9 @@ extern int __init pcpu_page_first_chunk(size_t reserved_size,
 #endif
 
 extern void __percpu *__alloc_reserved_percpu(size_t size, size_t align);
+#ifdef CONFIG_DEBUG_KMALLOC
+extern void __percpu *__alloc_reserved_percpu_memhook(size_t size, size_t align, unsigned long caller_address);
+#endif
 extern bool __is_kernel_percpu_address(unsigned long addr, unsigned long *can_addr);
 extern bool is_kernel_percpu_address(unsigned long addr);
 
@@ -140,6 +143,11 @@ extern void __init setup_per_cpu_areas(void);
 extern void __percpu *__alloc_percpu_gfp(size_t size, size_t align, gfp_t gfp);
 extern void __percpu *__alloc_percpu(size_t size, size_t align);
 extern void free_percpu(void __percpu *__pdata);
+#ifdef CONFIG_DEBUG_KMALLOC
+extern void __percpu *__alloc_percpu_gfp_memhook(size_t size, size_t align, gfp_t gfp, unsigned long caller_address);
+extern void __percpu *__alloc_percpu_memhook(size_t size, size_t align, unsigned long caller_address);
+extern void free_percpu_memhook(void __percpu *__pdata, unsigned long caller_address);
+#endif
 extern phys_addr_t per_cpu_ptr_to_phys(void *addr);
 
 #define alloc_percpu_gfp(type, gfp)					\
diff --git a/include/linux/slab.h b/include/linux/slab.h
index 788f04a7ca76..37d03718231f 100644
--- a/include/linux/slab.h
+++ b/include/linux/slab.h
@@ -18,6 +18,11 @@
 #include <linux/workqueue.h>
 #include <linux/percpu-refcount.h>
 
+#ifdef CONFIG_DEBUG_KMALLOC
+#include <linux/module.h>
+#include <linux/statis_memory.h>
+#endif
+
 
 /*
  * Flags to pass to kmem_cache_create().
@@ -189,6 +194,9 @@ void * __must_check krealloc(const void *, size_t, gfp_t);
 void kfree(const void *);
 void kzfree(const void *);
 size_t ksize(const void *);
+#ifdef CONFIG_DEBUG_KMALLOC
+void kfree_memhook(const void *x, unsigned long caller_address);
+#endif
 
 #ifdef CONFIG_HAVE_HARDENED_USERCOPY_ALLOCATOR
 void __check_heap_object(const void *ptr, unsigned long n, struct page *page,
@@ -393,6 +401,12 @@ void *__kmalloc(size_t size, gfp_t flags) __assume_kmalloc_alignment __malloc;
 void *kmem_cache_alloc(struct kmem_cache *, gfp_t flags) __assume_slab_alignment __malloc;
 void kmem_cache_free(struct kmem_cache *, void *);
 
+#ifdef CONFIG_DEBUG_KMALLOC
+void* __kmalloc_memhook(size_t size, gfp_t flags, unsigned long caller_address, call_function_t cf_type);
+void *kmem_cache_alloc_memhook(struct kmem_cache *s, gfp_t gfpflags, unsigned long caller_address, call_function_t cf_type);
+void kmem_cache_free_memhook(struct kmem_cache *s, void *x, unsigned long caller_address);
+#endif
+
 /*
  * Bulk allocation and freeing operations. These are accelerated in an
  * allocator specific way to avoid taking locks repeatedly or building
@@ -415,6 +429,45 @@ static __always_inline void kfree_bulk(size_t size, void **p)
 #ifdef CONFIG_NUMA
 void *__kmalloc_node(size_t size, gfp_t flags, int node) __assume_kmalloc_alignment __malloc;
 void *kmem_cache_alloc_node(struct kmem_cache *, gfp_t flags, int node) __assume_slab_alignment __malloc;
+#ifdef CONFIG_DEBUG_KMALLOC
+void *__kmalloc_node_memhook(size_t size, gfp_t flags, int node, 
+	unsigned long caller_address, call_function_t cf_type) __assume_kmalloc_alignment __malloc;
+void *kmem_cache_alloc_node_memhook(struct kmem_cache *, gfp_t flags, int node, unsigned long caller_address, \
+	call_function_t cf_type) __assume_slab_alignment __malloc;
+#endif
+#else
+#ifdef CONFIG_DEBUG_KMALLOC
+static __always_inline void *__kmalloc_node_memhook(size_t size, gfp_t flags, int node, unsigned long caller_address)
+{
+	return __kmalloc_memhook(size, flags, caller_address, cf_type);
+}
+
+static __always_inline void *__kmalloc_node(size_t size, gfp_t flags, int node)
+{
+	unsigned long caller_address;
+	call_function_t cf_type = __KMALLOC_NODE;
+
+	caller_address = (unsigned long)get_pc();
+
+	return __kmalloc_node_memhook(size, flags, node, caller_address, cf_type);
+
+}
+
+static __always_inline void *kmem_cache_alloc_node_memhook(struct kmem_cache *s, gfp_t flags, int node, \
+	unsigned long caller_address, call_function_t cf_type)
+{
+	return kmem_cache_alloc_memhook(s, flags, caller_address, cf_type);
+}
+
+static __always_inline void *kmem_cache_alloc_node(struct kmem_cache *s, gfp_t flags, int node)
+{
+	unsigned long caller_address;
+	call_function_t cf_type = KMEM_CACHE_ALLOC_NODE;
+
+	caller_address = (unsigned long)get_pc();
+
+	return kmem_cache_alloc_node_memhook(s, flags, node, caller_address, cf_type);
+}
 #else
 static __always_inline void *__kmalloc_node(size_t size, gfp_t flags, int node)
 {
@@ -425,16 +478,47 @@ static __always_inline void *kmem_cache_alloc_node(struct kmem_cache *s, gfp_t f
 {
 	return kmem_cache_alloc(s, flags);
 }
+#endif /*CONFIG_DEBUG_KMALLOC*/
 #endif
 
 #ifdef CONFIG_TRACING
 extern void *kmem_cache_alloc_trace(struct kmem_cache *, gfp_t, size_t) __assume_slab_alignment __malloc;
-
+#ifdef CONFIG_DEBUG_KMALLOC
+extern void *kmem_cache_alloc_trace_memhook(struct kmem_cache *, gfp_t, 
+	size_t, unsigned long, call_function_t cf_type) __assume_slab_alignment __malloc;
+#endif /*CONFIG_DEBUG_KMALLOC*/
 #ifdef CONFIG_NUMA
 extern void *kmem_cache_alloc_node_trace(struct kmem_cache *s,
 					   gfp_t gfpflags,
 					   int node, size_t size) __assume_slab_alignment __malloc;
+#ifdef CONFIG_DEBUG_KMALLOC
+extern void *kmem_cache_alloc_node_trace_memhook(struct kmem_cache *s,
+					   gfp_t gfpflags, int node, size_t size, 
+					   unsigned long caller_address, call_function_t cf_type) __assume_slab_alignment __malloc;
+#endif /*CONFIG_DEBUG_KMALLOC*/				
 #else
+#ifdef CONFIG_DEBUG_KMALLOC
+static __always_inline void *
+kmem_cache_alloc_node_trace_memhook(struct kmem_cache *s,
+			      gfp_t gfpflags, int node, size_t size, 
+				  unsigned long caller_address, call_function_t cf_type)
+{
+	return kmem_cache_alloc_trace_memhook(s, gfpflags, size, caller_address, cf_type);
+}
+
+static __always_inline void *
+kmem_cache_alloc_node_trace(struct kmem_cache *s,
+			      gfp_t gfpflags,
+			      int node, size_t size)
+{
+	unsigned long caller_address;
+	call_function_t cf_type = KMEM_CACHE_ALLOC_NODE_TRACE;
+
+	caller_address = (unsigned long)get_pc();
+
+	return kmem_cache_alloc_node_trace_memhook(s, gfpflags, node, size, caller_address, cf_type);
+}
+#else /*CONFIG_DEBUG_KMALLOC*/
 static __always_inline void *
 kmem_cache_alloc_node_trace(struct kmem_cache *s,
 			      gfp_t gfpflags,
@@ -442,9 +526,57 @@ kmem_cache_alloc_node_trace(struct kmem_cache *s,
 {
 	return kmem_cache_alloc_trace(s, gfpflags, size);
 }
+#endif
 #endif /* CONFIG_NUMA */
 
 #else /* CONFIG_TRACING */
+#ifdef CONFIG_DEBUG_KMALLOC
+
+static __always_inline void *kmem_cache_alloc_trace_memhook(struct kmem_cache *s,
+		gfp_t flags, size_t size, unsigned long caller_address, call_function_t cf_type)
+{
+	void *ret = kmem_cache_alloc_memhook(s, flags, caller_address, cf_type);
+
+	kasan_kmalloc(s, ret, size, flags);
+	return ret;
+}
+
+static __always_inline void *kmem_cache_alloc_trace(struct kmem_cache *s,
+		gfp_t flags, size_t size)
+{
+	unsigned long caller_address;
+	call_function_t cf_type = KMEM_CACHE_ALLOC_TRACE;
+
+	caller_address = (unsigned long)get_pc();
+
+	return kmem_cache_alloc_trace_memhook(s, flags, size, caller_address, cf_type);
+}
+
+static __always_inline void *
+kmem_cache_alloc_node_trace_memhook(struct kmem_cache *s,
+			      gfp_t gfpflags, int node, size_t size, 
+				  unsigned long caller_address, call_function_t cf_type)
+{
+	void *ret = kmem_cache_alloc_node_memhook(s, gfpflags, node, caller_address, cf_type);
+
+	kasan_kmalloc(s, ret, size, gfpflags);
+	return ret;
+}
+
+static __always_inline void *
+kmem_cache_alloc_node_trace(struct kmem_cache *s,
+			      gfp_t gfpflags,
+			      int node, size_t size)
+{
+	unsigned long caller_address;
+	call_function_t cf_type = KMEM_CACHE_ALLOC_NODE_TRACE;
+
+	caller_address = (unsigned long)get_pc();
+
+	return kmem_cache_alloc_node_trace_memhook(s, gfpflags, node, size, caller_address, cf_type);
+}
+
+#else
 static __always_inline void *kmem_cache_alloc_trace(struct kmem_cache *s,
 		gfp_t flags, size_t size)
 {
@@ -464,6 +596,7 @@ kmem_cache_alloc_node_trace(struct kmem_cache *s,
 	kasan_kmalloc(s, ret, size, gfpflags);
 	return ret;
 }
+#endif
 #endif /* CONFIG_TRACING */
 
 extern void *kmalloc_order(size_t size, gfp_t flags, unsigned int order) __assume_page_alignment __malloc;
@@ -478,11 +611,39 @@ kmalloc_order_trace(size_t size, gfp_t flags, unsigned int order)
 }
 #endif
 
+#ifdef CONFIG_DEBUG_KMALLOC
+static __always_inline void *kmalloc_large_memhook(size_t size, gfp_t flags, unsigned long caller_address, call_function_t cf_type)
+{
+	void *ret;
+	unsigned int order = get_order(size);
+
+	ret = kmalloc_order_trace(size, flags, order);
+
+	record_page_to_sysfs(order, ret, caller_address);
+
+	return ret;
+}
+
+static __always_inline void *kmalloc_large(size_t size, gfp_t flags)
+{
+	unsigned long caller_address = (unsigned long)get_pc();
+	call_function_t cf_type = KMALLOC_LARGE;
+
+	return kmalloc_large_memhook(size, flags, caller_address, cf_type);
+}
+
+#else
 static __always_inline void *kmalloc_large(size_t size, gfp_t flags)
 {
 	unsigned int order = get_order(size);
 	return kmalloc_order_trace(size, flags, order);
 }
+#endif
+
+#ifdef CONFIG_DEBUG_KMALLOC
+struct page_record_list *alloc_record_pl(gfp_t gfp);
+void free_record_pl(struct page_record_list *pl);
+#endif
 
 /**
  * kmalloc - allocate memory
@@ -535,6 +696,49 @@ static __always_inline void *kmalloc_large(size_t size, gfp_t flags)
  * for general use, and so are not documented here. For a full list of
  * potential flags, always refer to linux/gfp.h.
  */
+#ifdef CONFIG_DEBUG_KMALLOC
+
+//extern char _stext[], _etext[];
+//extern char __inittext_begin[], __inittext_end[];
+static __always_inline void *kmalloc_memhook(size_t size, gfp_t flags, unsigned long caller_address, call_function_t cf_type)
+{
+
+	void *ret;
+	
+	if (__builtin_constant_p(size)) {
+#ifndef CONFIG_SLOB
+		unsigned int index;
+#endif
+		if (size > KMALLOC_MAX_CACHE_SIZE){
+			ret = kmalloc_large_memhook(size, flags, caller_address, cf_type);
+			return ret;
+		}
+#ifndef CONFIG_SLOB
+		index = kmalloc_index(size);
+		if (!index)
+			return ZERO_SIZE_PTR;
+
+		return kmem_cache_alloc_trace_memhook(
+				kmalloc_caches[kmalloc_type(flags)][index],
+				flags, size, caller_address, cf_type);
+#endif
+	}
+	return __kmalloc_memhook(size, flags, caller_address, cf_type);
+}
+static __always_inline void *kmalloc(size_t size, gfp_t flags)
+{
+	void* ret;
+	unsigned long caller_address;
+	call_function_t cf_type = KMALLOC;
+
+	caller_address = (unsigned long)get_pc();
+
+	ret = kmalloc_memhook(size, flags, caller_address, cf_type);
+
+	return ret;
+}
+
+#else
 static __always_inline void *kmalloc(size_t size, gfp_t flags)
 {
 	if (__builtin_constant_p(size)) {
@@ -557,6 +761,8 @@ static __always_inline void *kmalloc(size_t size, gfp_t flags)
 	return __kmalloc(size, flags);
 }
 
+#endif /*CONFIG_KMALLOC_DEBUG*/
+
 /*
  * Determine size used for the nth kmalloc cache.
  * return size or 0 if a kmalloc cache for that
@@ -577,6 +783,36 @@ static __always_inline unsigned int kmalloc_size(unsigned int n)
 	return 0;
 }
 
+#ifdef CONFIG_DEBUG_KMALLOC
+static __always_inline void *kmalloc_node_memhook(size_t size, gfp_t flags, int node, 
+	unsigned long caller_address, call_function_t cf_type)
+{
+#ifndef CONFIG_SLOB
+	if (__builtin_constant_p(size) &&
+		size <= KMALLOC_MAX_CACHE_SIZE) {
+		unsigned int i = kmalloc_index(size);
+
+		if (!i)
+			return ZERO_SIZE_PTR;
+
+		return kmem_cache_alloc_node_trace_memhook(
+				kmalloc_caches[kmalloc_type(flags)][i],
+						flags, node, size, caller_address, cf_type);
+	}
+#endif
+	return __kmalloc_node_memhook(size, flags, node, caller_address, cf_type);
+}
+
+static __always_inline void *kmalloc_node(size_t size, gfp_t flags, int node)
+{
+	unsigned long caller_address;
+	call_function_t cf_type = KMALLOC_NODE;
+
+	caller_address = (unsigned long)get_pc();
+
+	return kmalloc_node_memhook(size, flags, node, caller_address, cf_type);
+}
+#else
 static __always_inline void *kmalloc_node(size_t size, gfp_t flags, int node)
 {
 #ifndef CONFIG_SLOB
@@ -594,6 +830,7 @@ static __always_inline void *kmalloc_node(size_t size, gfp_t flags, int node)
 #endif
 	return __kmalloc_node(size, flags, node);
 }
+#endif/*CONFIG_DEBUG_KMALLOC*/
 
 struct memcg_cache_array {
 	struct rcu_head rcu;
@@ -665,6 +902,29 @@ int memcg_update_all_caches(int num_memcgs);
  * @size: element size.
  * @flags: the type of memory to allocate (see kmalloc).
  */
+#ifdef CONFIG_DEBUG_KMALLOC
+static inline void *kmalloc_array_memhook(size_t n, size_t size, gfp_t flags,
+	unsigned long caller_address, call_function_t cf_type)
+{
+	size_t bytes;
+
+	if (unlikely(check_mul_overflow(n, size, &bytes)))
+		return NULL;
+	if (__builtin_constant_p(n) && __builtin_constant_p(size))
+		return kmalloc_memhook(bytes, flags, caller_address, cf_type);
+	return __kmalloc_memhook(bytes, flags, caller_address, cf_type);
+}
+
+static inline void *kmalloc_array(size_t n, size_t size, gfp_t flags)
+{
+	unsigned long caller_address;
+	call_function_t cf_type = KMALLOC_ARRAY;
+
+	caller_address = (unsigned long)get_pc();
+
+	return kmalloc_array_memhook(n, size, flags, caller_address, cf_type);
+}
+#else
 static inline void *kmalloc_array(size_t n, size_t size, gfp_t flags)
 {
 	size_t bytes;
@@ -675,6 +935,7 @@ static inline void *kmalloc_array(size_t n, size_t size, gfp_t flags)
 		return kmalloc(bytes, flags);
 	return __kmalloc(bytes, flags);
 }
+#endif /*CONFIG_DEBUG_KMALLOC*/
 
 /**
  * kcalloc - allocate memory for an array. The memory is set to zero.
@@ -682,11 +943,28 @@ static inline void *kmalloc_array(size_t n, size_t size, gfp_t flags)
  * @size: element size.
  * @flags: the type of memory to allocate (see kmalloc).
  */
+#ifdef CONFIG_DEBUG_KMALLOC
+static inline void *kcalloc_memhook(size_t n, size_t size, gfp_t flags,
+	unsigned long caller_address, call_function_t cf_type)
+{
+	return kmalloc_array_memhook(n, size, flags | __GFP_ZERO, caller_address, cf_type);
+}
+
+static inline void *kcalloc(size_t n, size_t size, gfp_t flags)
+{
+	unsigned long caller_address;
+	call_function_t cf_type = KCALLOC;
+
+	caller_address = (unsigned long)get_pc();
+
+	return kcalloc_memhook(n, size, flags, caller_address, cf_type);
+}
+#else
 static inline void *kcalloc(size_t n, size_t size, gfp_t flags)
 {
 	return kmalloc_array(n, size, flags | __GFP_ZERO);
 }
-
+#endif /*CONFIG_DEBUG_KMALLOC*/
 /*
  * kmalloc_track_caller is a special version of kmalloc that records the
  * calling function of the routine calling it for slab leak tracking instead
@@ -699,6 +977,36 @@ extern void *__kmalloc_track_caller(size_t, gfp_t, unsigned long);
 #define kmalloc_track_caller(size, flags) \
 	__kmalloc_track_caller(size, flags, _RET_IP_)
 
+#ifdef CONFIG_DEBUG_KMALLOC
+extern void *__kmalloc_track_caller_memhook(size_t, gfp_t, unsigned long, unsigned long, call_function_t);
+#define kmalloc_track_caller_memhook(size, flags, caller_address, cf_type) \
+	__kmalloc_track_caller_memhook(size, flags, _RET_IP_, caller_address, cf_type)
+#endif
+
+#ifdef CONFIG_DEBUG_KMALLOC
+static inline void *kmalloc_array_node_memhook(size_t n, size_t size, gfp_t flags,
+				       int node, unsigned long caller_address, call_function_t cf_type)
+{
+	size_t bytes;
+
+	if (unlikely(check_mul_overflow(n, size, &bytes)))
+		return NULL;
+	if (__builtin_constant_p(n) && __builtin_constant_p(size))
+		return kmalloc_node_memhook(bytes, flags, node, caller_address, cf_type);
+	return __kmalloc_node_memhook(bytes, flags, node, caller_address, cf_type);
+}
+
+static inline void *kmalloc_array_node(size_t n, size_t size, gfp_t flags,
+				       int node)
+{
+	unsigned long caller_address;
+	call_function_t cf_type = KMALLOC_ARRAY_NODE;
+
+	caller_address = (unsigned long)get_pc();
+
+	return kmalloc_array_node_memhook(n, size, flags, node, caller_address, cf_type);
+}
+#else
 static inline void *kmalloc_array_node(size_t n, size_t size, gfp_t flags,
 				       int node)
 {
@@ -710,12 +1018,30 @@ static inline void *kmalloc_array_node(size_t n, size_t size, gfp_t flags,
 		return kmalloc_node(bytes, flags, node);
 	return __kmalloc_node(bytes, flags, node);
 }
+#endif /*CONFIG_DEBUG_KMALLOC*/
+
+#ifdef CONFIG_DEBUG_KMALLOC
+static inline void *kcalloc_node_memhook(size_t n, size_t size, gfp_t flags, int node,
+	unsigned long caller_address, call_function_t cf_type)
+{
+	return kmalloc_array_node_memhook(n, size, flags | __GFP_ZERO, node, caller_address, cf_type);
+}
 
+static inline void *kcalloc_node(size_t n, size_t size, gfp_t flags, int node)
+{
+	unsigned long caller_address;
+	call_function_t cf_type = KCALLOC_NODE;
+
+	caller_address = (unsigned long)get_pc();
+
+	return kcalloc_node_memhook(n, size, flags, node, caller_address, cf_type);
+}
+#else
 static inline void *kcalloc_node(size_t n, size_t size, gfp_t flags, int node)
 {
 	return kmalloc_array_node(n, size, flags | __GFP_ZERO, node);
 }
-
+#endif /*CONFIG_DEBUG_KMALLOC*/
 
 #ifdef CONFIG_NUMA
 extern void *__kmalloc_node_track_caller(size_t, gfp_t, int, unsigned long);
@@ -723,30 +1049,78 @@ extern void *__kmalloc_node_track_caller(size_t, gfp_t, int, unsigned long);
 	__kmalloc_node_track_caller(size, flags, node, \
 			_RET_IP_)
 
+#ifdef CONFIG_DEBUG_KMALLOC
+extern void *__kmalloc_node_track_caller_memhook(size_t, gfp_t, int, unsigned long, 
+	unsigned long, call_function_t);
+#define kmalloc_node_track_caller_memhook(size, flags, node, caller_address, cf_type) \
+	__kmalloc_node_track_caller(size, flags, node, \
+			_RET_IP_, caller_address, cf_type)
+#endif
+
 #else /* CONFIG_NUMA */
 
 #define kmalloc_node_track_caller(size, flags, node) \
 	kmalloc_track_caller(size, flags)
 
+#ifdef CONFIG_DEBUG_KMALLOC
+#define kmalloc_node_track_caller_memhook(size, flags, node, caller_address, cf_type) \
+	kmalloc_track_caller_memhook(size, flags, caller_address, cf_type)
+#endif
+
 #endif /* CONFIG_NUMA */
 
 /*
  * Shortcuts
  */
+#ifdef CONFIG_DEBUG_KMALLOC
+static inline void *kmem_cache_zalloc_memhook(struct kmem_cache *k, gfp_t flags, \
+	unsigned long caller_address, call_function_t cf_type)
+{
+	return kmem_cache_alloc_memhook(k, flags | __GFP_ZERO, caller_address, cf_type);
+}
+
 static inline void *kmem_cache_zalloc(struct kmem_cache *k, gfp_t flags)
 {
-	return kmem_cache_alloc(k, flags | __GFP_ZERO);
+	unsigned long caller_address;
+	call_function_t cf_type = KMEM_CACHE_ZALLOC;
+
+	caller_address = (unsigned long)get_pc();
+
+	return kmem_cache_zalloc_memhook(k, flags, caller_address, cf_type);
 }
 
+#else
+static inline void *kmem_cache_zalloc(struct kmem_cache *k, gfp_t flags)
+{
+	return kmem_cache_alloc(k, flags | __GFP_ZERO);
+}
+#endif/*CONFIG_DEBUG_KMALLOC*/
 /**
  * kzalloc - allocate memory. The memory is set to zero.
  * @size: how many bytes of memory are required.
  * @flags: the type of memory to allocate (see kmalloc).
  */
+#ifdef CONFIG_DEBUG_KMALLOC
+static inline void *kzalloc_memhook(size_t size, gfp_t flags, unsigned long caller_address, call_function_t cf_type)
+{
+	return kmalloc_memhook(size, flags | __GFP_ZERO, caller_address, cf_type);
+}
+
+static inline void *kzalloc(size_t size, gfp_t flags)
+{
+	unsigned long caller_address;
+	call_function_t cf_type = KZALLOC;
+
+	caller_address = (unsigned long)get_pc();
+
+	return kzalloc_memhook(size, flags, caller_address, cf_type);
+}
+#else
 static inline void *kzalloc(size_t size, gfp_t flags)
 {
 	return kmalloc(size, flags | __GFP_ZERO);
 }
+#endif/*CONFIG_DEBUG_KMALLOC*/
 
 /**
  * kzalloc_node - allocate zeroed memory from a particular memory node.
@@ -754,10 +1128,27 @@ static inline void *kzalloc(size_t size, gfp_t flags)
  * @flags: the type of memory to allocate (see kmalloc).
  * @node: memory node from which to allocate
  */
+#ifdef CONFIG_DEBUG_KMALLOC
+static inline void *kzalloc_node_memhook(size_t size, gfp_t flags, int node,
+	unsigned long caller_address, call_function_t cf_type)
+{
+	return kmalloc_node_memhook(size, flags | __GFP_ZERO, node, caller_address, cf_type);
+}
+static inline void *kzalloc_node(size_t size, gfp_t flags, int node)
+{
+	unsigned long caller_address;
+	call_function_t cf_type = KZALLOC_NODE;
+
+	caller_address = (unsigned long)get_pc();
+
+	return kzalloc_node_memhook(size, flags, node, caller_address, cf_type);
+}
+#else
 static inline void *kzalloc_node(size_t size, gfp_t flags, int node)
 {
 	return kmalloc_node(size, flags | __GFP_ZERO, node);
 }
+#endif /*CONFIG_DEBUG_KMALLOC*/
 
 unsigned int kmem_cache_size(struct kmem_cache *s);
 void __init kmem_cache_init_late(void);
diff --git a/include/linux/statis_memory.h b/include/linux/statis_memory.h
new file mode 100644
index 000000000000..6d6930ae2d93
--- /dev/null
+++ b/include/linux/statis_memory.h
@@ -0,0 +1,64 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/*
+ * statistical module memory 
+ *
+ */
+#ifndef _LINUX_STATIS_MEMORY_H
+#define _LINUX_STATIS_MEMORY_H
+
+#include <linux/kobject.h>
+#include <linux/module.h>
+#include <linux/kernel.h>
+
+// call type
+typedef enum {
+    FREED,
+    KMALLOC,
+    KZALLOC,
+    __KMALLOC,
+    KCALLOC,
+    KMALLOC_ARRAY,
+    __KMALLOC_TRACK_CALLER,
+    __KMALLOC_NODE_TRACK_CALLER,
+    KMALLOC_LARGE,
+    KMALLOC_LARGE_NODE,
+    KMALLOC_SLAB,
+    KMALLOC_NODE,
+    __KMALLOC_NODE,
+    KZALLOC_NODE,
+    KCALLOC_NODE,
+    KMALLOC_ARRAY_NODE,
+    KREALLOC,
+    __KREALLOC,
+    KMEM_CACHE_ALLOC,
+    KMEM_CACHE_ZALLOC,
+    KMEM_CACHE_ALLOC_TRACE,
+    KMEM_CACHE_ALLOC_BULK,
+    KMEM_CACHE_ALLOC_NODE,
+    KMEM_CACHE_ALLOC_NODE_TRACE,
+    MEMPOOL_ALLOC,
+    ALLOC_PAGE,
+    FREE_PAGE
+} call_function_t;
+
+void* get_pc(void) __attribute__ ((__noinline__)) ;
+
+void record_to_sysfs(size_t size, void* ret,
+    unsigned long caller_address, call_function_t cf_type);
+
+void free_mem_record(unsigned long caller_address, void* x, size_t size);
+
+void record_page_to_sysfs(unsigned int order, void* page,
+    unsigned long caller_address);
+
+void free_mem_record_page(unsigned long caller_address, struct page* page, unsigned int order);
+
+void record_to_sysfs_percpu(unsigned long caller_address, void __percpu *x, size_t size);
+
+void free_mem_record_percpu(unsigned long caller_address, void __percpu *x, size_t size);
+
+void record_to_sysfs_dmapool(unsigned long caller_address, void *x, size_t size);
+
+void free_mem_record_dmapool(unsigned long caller_address, void *x, size_t size);
+
+#endif
diff --git a/kernel/Makefile b/kernel/Makefile
index ccc3a9c47930..04dd43fbfedb 100644
--- a/kernel/Makefile
+++ b/kernel/Makefile
@@ -12,6 +12,9 @@ obj-y     = fork.o exec_domain.o panic.o \
 	    notifier.o ksysfs.o cred.o reboot.o \
 	    async.o range.o smpboot.o ucount.o ktask.o
 
+
+obj-$(CONFIG_DEBUG_KMALLOC) += statis_memory.o
+
 obj-$(CONFIG_MODULES) += kmod.o
 obj-$(CONFIG_MULTIUSER) += groups.o
 
diff --git a/kernel/module.c b/kernel/module.c
index 69d0e28804a8..5fd23288a9f1 100644
--- a/kernel/module.c
+++ b/kernel/module.c
@@ -70,6 +70,10 @@
 #define CREATE_TRACE_POINTS
 #include <trace/events/module.h>
 
+#ifdef CONFIG_DEBUG_KMALLOC
+#include <linux/statis_memory.h>
+#endif
+
 #ifndef ARCH_SHF_SMALL
 #define ARCH_SHF_SMALL 0
 #endif
@@ -3758,6 +3762,9 @@ static int load_module(struct load_info *info, const char __user *uargs,
 	struct module *mod;
 	long err = 0;
 	char *after_dashes;
+#ifdef CONFIG_DEBUG_KMALLOC
+	struct module_usedmemory_attribute *uattr;
+#endif
 
 	err = elf_header_check(info);
 	if (err)
@@ -3894,6 +3901,29 @@ static int load_module(struct load_info *info, const char __user *uargs,
 			goto sysfs_cleanup;
 	}
 
+#ifdef CONFIG_DEBUG_KMALLOC
+	uattr = kzalloc(sizeof(struct module_usedmemory_attribute), GFP_KERNEL);
+	uattr->mattr.attr.name = "usedmemory";
+	uattr->mattr.attr.mode = S_IRUGO;
+	uattr->mattr.show = __module_usedmemory_show;
+	uattr->mod = mod;
+	uattr->used = "NULL";
+
+	mod->mkobj.used_memory = 0;
+	mod->mkobj.used_pages = 0;
+	mod->mkobj.used_percpu = 0;
+	mod->mkobj.used_dmapool = 0;
+	mod->mkobj.record_list_slab = alloc_record_pl(GFP_KERNEL);
+	mod->mkobj.record_list_page = alloc_record_pl(GFP_KERNEL);
+	mod->mkobj.record_list_percpu = alloc_record_pl(GFP_KERNEL);
+	mod->mkobj.record_list_dmapool = alloc_record_pl(GFP_KERNEL);
+	spin_lock_init(&mod->mkobj.record_lock);
+
+	err = sysfs_create_file(&mod->mkobj.kobj, &uattr->mattr.attr);
+	if (err < 0)
+		goto coming_cleanup;
+#endif
+
 	/* Get rid of temporary copy. */
 	free_copy(info);
 
@@ -4476,6 +4506,10 @@ struct module *__module_text_address(unsigned long addr)
 	return mod;
 }
 
+#ifdef CONFIG_DEBUG_KMALLOC
+EXPORT_SYMBOL(__module_text_address);
+#endif
+
 /* Don't grab lock, we're oopsing. */
 void print_modules(void)
 {
diff --git a/kernel/params.c b/kernel/params.c
index ce89f757e6da..eaf128e3498a 100644
--- a/kernel/params.c
+++ b/kernel/params.c
@@ -740,10 +740,13 @@ void destroy_params(const struct kernel_param *params, unsigned num)
 			params[i].ops->free(params[i].arg);
 }
 
+#ifdef CONFIG_DEBUG_KMALLOC
+#include <linux/statis_memory.h>
 static struct module_kobject * __init locate_module_kobject(const char *name)
 {
 	struct module_kobject *mk;
 	struct kobject *kobj;
+	//struct module_usedmemory_attribute *uattr;
 	int err;
 
 	kobj = kset_find_obj(module_kset, name);
@@ -770,11 +773,56 @@ static struct module_kobject * __init locate_module_kobject(const char *name)
 
 		/* So that we hold reference in both cases. */
 		kobject_get(&mk->kobj);
+// #ifdef CONFIG_DEBUG_KMALLOC
+// 		uattr = kzalloc(sizeof(struct module_usedmemory_attribute), GFP_KERNEL);
+// 		uattr->mattr.attr.name = "usedmemory";
+// 		uattr->mattr.attr.mode = S_IRUGO;
+// 		uattr->mattr.show = __module_usedmemory_show;
+// 		uattr->mod = mk->mod;
+// 		uattr->used = "NULL";
+// 		mk->module_memory = kzalloc(MAX_STATIS_BUF_SIZE, GFP_KERNEL);
+// 		err = sysfs_create_file(&mk->kobj, &uattr->mattr.attr);
+// #endif
 	}
 
 	return mk;
 }
 
+#else /*CONFIG_DEBUG_KMALLOC*/
+static struct module_kobject * __init locate_module_kobject(const char *name)
+{
+	struct module_kobject *mk;
+	struct kobject *kobj;
+	int err;
+
+	kobj = kset_find_obj(module_kset, name);
+	if (kobj) {
+		mk = to_module_kobject(kobj);
+	} else {
+		mk = kzalloc(sizeof(struct module_kobject), GFP_KERNEL);
+		BUG_ON(!mk);
+
+		mk->mod = THIS_MODULE;
+		mk->kobj.kset = module_kset;
+		err = kobject_init_and_add(&mk->kobj, &module_ktype, NULL,
+					   "%s", name);
+#ifdef CONFIG_MODULES
+		if (!err)
+			err = sysfs_create_file(&mk->kobj, &module_uevent.attr);
+#endif
+		if (err) {
+			kobject_put(&mk->kobj);
+			pr_crit("Adding module '%s' to sysfs failed (%d), the system may be unstable.\n",
+				name, err);
+			return NULL;
+		}
+
+		/* So that we hold reference in both cases. */
+		kobject_get(&mk->kobj);
+	}
+}
+#endif
+
 static void __init kernel_add_sysfs_param(const char *name,
 					  const struct kernel_param *kparam,
 					  unsigned int name_skip)
@@ -830,6 +878,7 @@ static void __init param_sysfs_builtin(void)
 			name_len = dot - kp->name + 1;
 			strlcpy(modname, kp->name, name_len);
 		}
+		//printk(KERN_DEBUG "kernel_param:%s\n", kp->name);
 		kernel_add_sysfs_param(modname, kp, name_len);
 	}
 }
@@ -867,6 +916,37 @@ static void __init version_sysfs_builtin(void)
 
 /* module-related sysfs stuff */
 
+//adventural
+#ifdef CONFIG_DEBUG_KMALLOC
+#include <linux/statis_memory.h>
+
+ssize_t __module_usedmemory_show(struct module_attribute * mattr,
+				     struct module_kobject *mk, char* buf)
+{
+	//struct module_usedmemory_attribute *usedattr;
+	//usedattr = container_of(mattr, struct module_usedmemory_attribute, mattr);
+	//printk(KERN_DEBUG "kmalloc:%lu\n", kmalloc_total);
+	int record_pages = 0;
+	int all_elements = 0;
+	struct page_record_list *record_list = mk->record_list_slab;
+
+	while(record_list){
+		record_pages++;
+		all_elements += atomic_read(&record_list->page_elements);
+		record_list = record_list->next;
+	}
+	
+	printk(KERN_DEBUG "usedmemory:%lx\n", mk->used_memory);
+	if(mk->used_memory >= 1024*100)
+		return scnprintf(buf, PAGE_SIZE, "Record page:%d\nRecords:%d\nSlab:%ldkb\nPages:%u\nPercpu:%ld\ndmapool:%ld\n",\
+			record_pages, all_elements, mk->used_memory/1024, mk->used_pages, mk->used_percpu, mk->used_dmapool);
+	else
+		return scnprintf(buf, PAGE_SIZE, "Record page:%d\nRecords:%d\nSlab:%ldbytes\nPages:%u\nPercpu:%ld\ndmapool:%ld\n",\
+			record_pages, all_elements, mk->used_memory, mk->used_pages, mk->used_percpu, mk->used_dmapool);
+}
+
+#endif
+
 static ssize_t module_attr_show(struct kobject *kobj,
 				struct attribute *attr,
 				char *buf)
diff --git a/kernel/statis_memory.c b/kernel/statis_memory.c
new file mode 100644
index 000000000000..cbcd9925d589
--- /dev/null
+++ b/kernel/statis_memory.c
@@ -0,0 +1,256 @@
+#include <linux/slab.h>
+#include <linux/statis_memory.h>
+#include <linux/mm.h>
+
+#define MAX_PAGE_ELEMENTS (PAGE_SIZE / sizeof(unsigned long))
+
+int add_to_list(struct page_record_list** mod_page_list, void* check_addr)
+{
+    struct page_record_list* page_list = *mod_page_list;
+    unsigned long *addr;
+    int recorded = 0;
+    int i=0;
+
+    while(page_list){
+        // page was full, search next page
+        if(atomic_read(&page_list->page_elements) == MAX_PAGE_ELEMENTS){
+            page_list = page_list->next;
+            continue;
+        }
+
+        addr = (unsigned long*)page_address(page_list->page);
+        for(i=0; i<MAX_PAGE_ELEMENTS; i++){
+            if(addr[i] == 0){
+                addr[i] = (unsigned long)check_addr;
+                recorded = 1;
+                break;
+            }
+        }
+
+        if(recorded)
+            break;
+
+        page_list = page_list->next;
+    }
+
+    //all pages was full, alloc a new page
+    if(!recorded){
+        struct page_record_list *new_list = alloc_record_pl(GFP_KERNEL);
+        addr = (unsigned long*)page_address(new_list->page);
+
+        //record
+        addr[0] = (unsigned long)check_addr;
+
+        new_list->next = *mod_page_list;
+        *mod_page_list = new_list;
+        page_list = new_list;
+    }
+
+    atomic_inc(&page_list->page_elements);
+
+    return recorded;
+}
+
+int check_if_exist(struct page_record_list* page_list, void* x)
+{
+    unsigned long *addr;
+    int i = 0;
+    int recorded = 0;
+
+    while(page_list){
+        addr = (unsigned long*)page_address(page_list->page);
+        for(i=0; i<MAX_PAGE_ELEMENTS; i++){
+            if(addr[i] == (unsigned long)x){
+                // free was alloced before
+                recorded = 1;
+                //release memory record
+                addr[i] = 0;
+                //decount the number
+                atomic_dec_and_test(&page_list->page_elements);
+                break;
+            }
+        }
+        if(recorded)
+            break;
+        page_list = page_list->next;
+    }
+
+    return recorded;
+}
+
+void* get_pc(void)
+{
+    return (void*)__builtin_return_address(0);
+}
+EXPORT_SYMBOL(get_pc);
+
+void record_to_sysfs(size_t size, void* ret,
+    unsigned long caller_address, call_function_t cf_type)
+{
+    struct module *mod;
+    unsigned long flags;
+    
+    mod = __module_text_address(caller_address);
+
+    if(mod != NULL){
+        
+        int recorded;
+
+        recorded = add_to_list(&mod->mkobj.record_list_slab, ret);
+
+        if(recorded){
+
+            spin_lock_irqsave(&mod->mkobj.record_lock, flags);
+            mod->mkobj.used_memory += size;
+            spin_unlock_irqrestore(&mod->mkobj.record_lock, flags);
+        }
+        else
+            printk(KERN_DEBUG "record_to_sysfs failed");
+    }
+}
+EXPORT_SYMBOL(record_to_sysfs);
+
+void record_page_to_sysfs(unsigned int order, void* page,
+    unsigned long caller_address)
+{
+    unsigned long flags;
+    struct module *mod;
+    
+    mod = __module_text_address(caller_address);
+
+    if(mod != NULL){
+        int recorded = add_to_list(&mod->mkobj.record_list_page, page);
+        if(recorded){
+            spin_lock_irqsave(&mod->mkobj.record_lock, flags);
+            mod->mkobj.used_pages += (1<<order); 
+            spin_unlock_irqrestore(&mod->mkobj.record_lock, flags);
+        }
+        else
+            printk(KERN_DEBUG "record_to_sysfs failed");
+    }
+}
+EXPORT_SYMBOL(record_page_to_sysfs);
+
+void free_mem_record(unsigned long caller_address, void* x, size_t size)
+{
+    struct module *mod;
+    unsigned long flags;
+
+    mod = __module_text_address(caller_address);
+
+    if(mod != NULL)
+    {
+        int recorded = check_if_exist(mod->mkobj.record_list_slab, x);
+        if(recorded){
+            spin_lock_irqsave(&mod->mkobj.record_lock, flags);
+            mod->mkobj.used_memory -= size;
+            spin_unlock_irqrestore(&mod->mkobj.record_lock, flags);
+        }
+    }
+}
+EXPORT_SYMBOL(free_mem_record);
+
+void free_mem_record_page(unsigned long caller_address, struct page* page, unsigned int order)
+{
+    struct module *mod;
+    unsigned long flags;
+
+    mod = __module_text_address(caller_address);
+
+    if(mod != NULL)
+    {
+        int recorded = check_if_exist(mod->mkobj.record_list_page, page);
+        if(recorded){
+            spin_lock_irqsave(&mod->mkobj.record_lock, flags);
+            mod->mkobj.used_pages -= (1<<order);
+            spin_unlock_irqrestore(&mod->mkobj.record_lock, flags);
+        }
+    }
+}
+EXPORT_SYMBOL(free_mem_record_page);
+
+void record_to_sysfs_percpu(unsigned long caller_address, void __percpu *x, size_t size)
+{
+    struct module *mod;
+    unsigned long flags;
+    
+    mod = __module_text_address(caller_address);
+
+    if(mod != NULL){
+
+        int recorded;
+
+        recorded = add_to_list(&mod->mkobj.record_list_percpu, x);
+
+        if(recorded){
+            spin_lock_irqsave(&mod->mkobj.record_lock, flags);
+            mod->mkobj.used_percpu += size;
+            spin_unlock_irqrestore(&mod->mkobj.record_lock, flags);
+        }
+        else
+            printk(KERN_DEBUG "record_to_sysfs failed");
+    }
+}
+EXPORT_SYMBOL(record_to_sysfs_percpu);
+
+void free_mem_record_percpu(unsigned long caller_address, void __percpu *x, size_t size)
+{
+    struct module *mod;
+    unsigned long flags;
+
+    mod = __module_text_address(caller_address);
+
+    if(mod != NULL)
+    {
+        int recorded = check_if_exist(mod->mkobj.record_list_percpu, x);
+        if(recorded){
+            spin_lock_irqsave(&mod->mkobj.record_lock, flags);
+            mod->mkobj.used_percpu -= size;
+            spin_unlock_irqrestore(&mod->mkobj.record_lock, flags);
+        }
+    }
+}
+EXPORT_SYMBOL(free_mem_record_percpu);
+
+void record_to_sysfs_dmapool(unsigned long caller_address, void *x, size_t size)
+{
+    struct module *mod;
+    unsigned long flags;
+    
+    mod = __module_text_address(caller_address);
+
+    if(mod != NULL){
+
+        int recorded;
+
+        recorded = add_to_list(&mod->mkobj.record_list_dmapool, x);
+
+        if(recorded){
+            spin_lock_irqsave(&mod->mkobj.record_lock, flags);
+            mod->mkobj.used_dmapool += size;
+            spin_unlock_irqrestore(&mod->mkobj.record_lock, flags);
+        }
+        else
+            printk(KERN_DEBUG "record_to_sysfs failed");
+    }
+}
+EXPORT_SYMBOL(record_to_sysfs_dmapool);
+
+void free_mem_record_dmapool(unsigned long caller_address, void *x, size_t size)
+{
+    struct module *mod;
+    unsigned long flags;
+
+    mod = __module_text_address(caller_address);
+
+    if(mod != NULL)
+    {
+        int recorded = check_if_exist(mod->mkobj.record_list_dmapool, x);
+        if(recorded){
+            spin_lock_irqsave(&mod->mkobj.record_lock, flags);
+            mod->mkobj.used_dmapool -= size;
+            spin_unlock_irqrestore(&mod->mkobj.record_lock, flags);
+        }
+    }
+}
+EXPORT_SYMBOL(free_mem_record_dmapool);
\ No newline at end of file
diff --git a/mm/Kconfig b/mm/Kconfig
index 12601505c4a4..5437c2504b99 100644
--- a/mm/Kconfig
+++ b/mm/Kconfig
@@ -1,6 +1,12 @@
 
 menu "Memory Management options"
 
+config DEBUG_KMALLOC
+	bool "Kmalloc debug"
+	default y
+	help
+	 Hook kmalloc
+
 config SELECT_MEMORY_MODEL
 	def_bool y
 	depends on ARCH_SELECT_MEMORY_MODEL
diff --git a/mm/dmapool.c b/mm/dmapool.c
index 6d4b97e7e9e9..5bc0236fcb26 100644
--- a/mm/dmapool.c
+++ b/mm/dmapool.c
@@ -317,6 +317,87 @@ EXPORT_SYMBOL(dma_pool_destroy);
  * and reports its dma address through the handle.
  * If such a memory block can't be allocated, %NULL is returned.
  */
+#ifdef CONFIG_DEBUG_KMALLOC
+void *dma_pool_alloc_memhook(struct dma_pool *pool, gfp_t mem_flags,
+		     dma_addr_t *handle, unsigned long caller_address)
+{
+	unsigned long flags;
+	struct dma_page *page;
+	size_t offset;
+	void *retval;
+
+	might_sleep_if(gfpflags_allow_blocking(mem_flags));
+
+	spin_lock_irqsave(&pool->lock, flags);
+	list_for_each_entry(page, &pool->page_list, page_list) {
+		if (page->offset < pool->allocation)
+			goto ready;
+	}
+
+	/* pool_alloc_page() might sleep, so temporarily drop &pool->lock */
+	spin_unlock_irqrestore(&pool->lock, flags);
+
+	page = pool_alloc_page(pool, mem_flags & (~__GFP_ZERO));
+	if (!page)
+		return NULL;
+
+	spin_lock_irqsave(&pool->lock, flags);
+
+	list_add(&page->page_list, &pool->page_list);
+ ready:
+	page->in_use++;
+	offset = page->offset;
+	page->offset = *(int *)(page->vaddr + offset);
+	retval = offset + page->vaddr;
+	*handle = offset + page->dma;
+#ifdef	DMAPOOL_DEBUG
+	{
+		int i;
+		u8 *data = retval;
+		/* page->offset is stored in first 4 bytes */
+		for (i = sizeof(page->offset); i < pool->size; i++) {
+			if (data[i] == POOL_POISON_FREED)
+				continue;
+			if (pool->dev)
+				dev_err(pool->dev,
+					"dma_pool_alloc %s, %p (corrupted)\n",
+					pool->name, retval);
+			else
+				pr_err("dma_pool_alloc %s, %p (corrupted)\n",
+					pool->name, retval);
+
+			/*
+			 * Dump the first 4 bytes even if they are not
+			 * POOL_POISON_FREED
+			 */
+			print_hex_dump(KERN_ERR, "", DUMP_PREFIX_OFFSET, 16, 1,
+					data, pool->size, 1);
+			break;
+		}
+	}
+	if (!(mem_flags & __GFP_ZERO))
+		memset(retval, POOL_POISON_ALLOCATED, pool->size);
+#endif
+	spin_unlock_irqrestore(&pool->lock, flags);
+
+	if (mem_flags & __GFP_ZERO)
+		memset(retval, 0, pool->size);
+
+	record_to_sysfs_dmapool(caller_address, retval, pool->size);
+
+	return retval;
+}
+EXPORT_SYMBOL(dma_pool_alloc_memhook);
+
+void *dma_pool_alloc(struct dma_pool *pool, gfp_t mem_flags,
+		     dma_addr_t *handle)
+{
+	unsigned long caller_address = (unsigned long)_RET_IP_;
+
+	return dma_pool_alloc_memhook(pool, mem_flags, handle, caller_address);
+}
+EXPORT_SYMBOL(dma_pool_alloc);
+#else
 void *dma_pool_alloc(struct dma_pool *pool, gfp_t mem_flags,
 		     dma_addr_t *handle)
 {
@@ -385,6 +466,7 @@ void *dma_pool_alloc(struct dma_pool *pool, gfp_t mem_flags,
 	return retval;
 }
 EXPORT_SYMBOL(dma_pool_alloc);
+#endif
 
 static struct dma_page *pool_find_page(struct dma_pool *pool, dma_addr_t dma)
 {
@@ -408,6 +490,83 @@ static struct dma_page *pool_find_page(struct dma_pool *pool, dma_addr_t dma)
  * Caller promises neither device nor driver will again touch this block
  * unless it is first re-allocated.
  */
+#ifdef CONFIG_DEBUG_KMALLOC
+void dma_pool_free_memhook(struct dma_pool *pool, void *vaddr, dma_addr_t dma, unsigned long caller_address)
+{
+	struct dma_page *page;
+	unsigned long flags;
+	unsigned int offset;
+
+	free_mem_record_dmapool(caller_address, vaddr, pool->size);
+
+	spin_lock_irqsave(&pool->lock, flags);
+	page = pool_find_page(pool, dma);
+	if (!page) {
+		spin_unlock_irqrestore(&pool->lock, flags);
+		if (pool->dev)
+			dev_err(pool->dev,
+				"dma_pool_free %s, %p/%lx (bad dma)\n",
+				pool->name, vaddr, (unsigned long)dma);
+		else
+			pr_err("dma_pool_free %s, %p/%lx (bad dma)\n",
+			       pool->name, vaddr, (unsigned long)dma);
+		return;
+	}
+
+	offset = vaddr - page->vaddr;
+#ifdef	DMAPOOL_DEBUG
+	if ((dma - page->dma) != offset) {
+		spin_unlock_irqrestore(&pool->lock, flags);
+		if (pool->dev)
+			dev_err(pool->dev,
+				"dma_pool_free %s, %p (bad vaddr)/%pad\n",
+				pool->name, vaddr, &dma);
+		else
+			pr_err("dma_pool_free %s, %p (bad vaddr)/%pad\n",
+			       pool->name, vaddr, &dma);
+		return;
+	}
+	{
+		unsigned int chain = page->offset;
+		while (chain < pool->allocation) {
+			if (chain != offset) {
+				chain = *(int *)(page->vaddr + chain);
+				continue;
+			}
+			spin_unlock_irqrestore(&pool->lock, flags);
+			if (pool->dev)
+				dev_err(pool->dev, "dma_pool_free %s, dma %pad already free\n",
+					pool->name, &dma);
+			else
+				pr_err("dma_pool_free %s, dma %pad already free\n",
+				       pool->name, &dma);
+			return;
+		}
+	}
+	memset(vaddr, POOL_POISON_FREED, pool->size);
+#endif
+
+	page->in_use--;
+	*(int *)vaddr = page->offset;
+	page->offset = offset;
+	/*
+	 * Resist a temptation to do
+	 *    if (!is_page_busy(page)) pool_free_page(pool, page);
+	 * Better have a few empty pages hang around.
+	 */
+	spin_unlock_irqrestore(&pool->lock, flags);
+
+}
+EXPORT_SYMBOL(dma_pool_free_memhook);
+
+void dma_pool_free(struct dma_pool *pool, void *vaddr, dma_addr_t dma)
+{
+	unsigned long caller_address = (unsigned long)_RET_IP_;
+
+	return dma_pool_free_memhook(pool, vaddr, dma, caller_address);
+}
+EXPORT_SYMBOL(dma_pool_free);
+#else
 void dma_pool_free(struct dma_pool *pool, void *vaddr, dma_addr_t dma)
 {
 	struct dma_page *page;
@@ -472,6 +631,7 @@ void dma_pool_free(struct dma_pool *pool, void *vaddr, dma_addr_t dma)
 	spin_unlock_irqrestore(&pool->lock, flags);
 }
 EXPORT_SYMBOL(dma_pool_free);
+#endif
 
 /*
  * Managed DMA pool
diff --git a/mm/mempolicy.c b/mm/mempolicy.c
index 3a835f96c8fe..4186a2c7bbae 100644
--- a/mm/mempolicy.c
+++ b/mm/mempolicy.c
@@ -2093,6 +2093,24 @@ bool mempolicy_nodemask_intersects(struct task_struct *tsk,
 
 /* Allocate a page in interleaved policy.
    Own path because it needs to do special accounting. */
+#ifdef CONFIG_DEBUG_KMALLOC
+static struct page *alloc_page_interleave_memhook(gfp_t gfp, unsigned order,
+				unsigned nid, unsigned long caller_address)
+{
+	struct page *page;
+
+	page = __alloc_pages_memhook(gfp, order, nid, caller_address);
+	/* skip NUMA_INTERLEAVE_HIT counter update if numa stats is disabled */
+	if (!static_branch_likely(&vm_numa_stat_key))
+		return page;
+	if (page && page_to_nid(page) == nid) {
+		preempt_disable();
+		__inc_numa_state(page_zone(page), NUMA_INTERLEAVE_HIT);
+		preempt_enable();
+	}
+	return page;
+}
+#endif
 static struct page *alloc_page_interleave(gfp_t gfp, unsigned order,
 					unsigned nid)
 {
@@ -2133,6 +2151,61 @@ static struct page *alloc_page_interleave(gfp_t gfp, unsigned order,
  *	all allocations for pages that will be mapped into user space. Returns
  *	NULL when no page can be allocated.
  */
+#ifdef CONFIG_DEBUG_KMALLOC
+struct page *
+alloc_pages_vma_memhook(gfp_t gfp, int order, struct vm_area_struct *vma,
+		unsigned long addr, int node, bool hugepage, unsigned long caller_address)
+{
+	struct mempolicy *pol;
+	struct page *page;
+	int preferred_nid;
+	nodemask_t *nmask;
+
+	pol = get_vma_policy(vma, addr);
+
+	if (pol->mode == MPOL_INTERLEAVE) {
+		unsigned nid;
+
+		nid = interleave_nid(pol, vma, addr, PAGE_SHIFT + order);
+		mpol_cond_put(pol);
+		page = alloc_page_interleave_memhook(gfp, order, nid, caller_address);
+		goto out;
+	}
+
+	if (unlikely(IS_ENABLED(CONFIG_TRANSPARENT_HUGEPAGE) && hugepage)) {
+		int hpage_node = node;
+		if (pol->mode == MPOL_PREFERRED &&
+						!(pol->flags & MPOL_F_LOCAL))
+			hpage_node = pol->v.preferred_node;
+
+		nmask = policy_nodemask(gfp, pol);
+		if (!nmask || node_isset(hpage_node, *nmask)) {
+			mpol_cond_put(pol);
+			if (!(gfp & __GFP_DIRECT_RECLAIM))
+				gfp |= __GFP_THISNODE;
+			page = __alloc_pages_node_memhook(hpage_node, gfp, order, caller_address);
+			goto out;
+		}
+	}
+
+	nmask = policy_nodemask(gfp, pol);
+	preferred_nid = policy_node(gfp, pol, node);
+	page = __alloc_pages_nodemask_memhook(gfp, order, preferred_nid, nmask, caller_address);
+	mark_vma_cdm(nmask, page, vma);
+	mpol_cond_put(pol);
+out:
+	return page;
+}
+
+struct page *
+alloc_pages_vma(gfp_t gfp, int order, struct vm_area_struct *vma,
+		unsigned long addr, int node, bool hugepage)
+{
+	unsigned long caller_address = (unsigned long)_RET_IP_;
+
+	return alloc_pages_vma_memhook(gfp, order, vma, addr, node, hugepage, caller_address);
+}
+#else
 struct page *
 alloc_pages_vma(gfp_t gfp, int order, struct vm_area_struct *vma,
 		unsigned long addr, int node, bool hugepage)
@@ -2215,6 +2288,7 @@ alloc_pages_vma(gfp_t gfp, int order, struct vm_area_struct *vma,
 out:
 	return page;
 }
+#endif
 
 /**
  * 	alloc_pages_current - Allocate pages.
@@ -2231,6 +2305,39 @@ alloc_pages_vma(gfp_t gfp, int order, struct vm_area_struct *vma,
  *	interrupt context and apply the current process NUMA policy.
  *	Returns NULL when no page can be allocated.
  */
+#ifdef CONFIG_DEBUG_KMALLOC
+struct page *alloc_pages_current_memhook(gfp_t gfp, unsigned order, 
+	unsigned long caller_address)
+{
+	struct mempolicy *pol = &default_policy;
+	struct page *page;
+
+	if (!in_interrupt() && !(gfp & __GFP_THISNODE))
+		pol = get_task_policy(current);
+
+	/*
+	 * No reference counting needed for current->mempolicy
+	 * nor system default_policy
+	 */
+	if (pol->mode == MPOL_INTERLEAVE)
+		page = alloc_page_interleave_memhook(gfp, order, interleave_nodes(pol), caller_address);
+	else
+		page = __alloc_pages_nodemask_memhook(gfp, order,
+				policy_node(gfp, pol, numa_node_id()),
+				policy_nodemask(gfp, pol), caller_address);
+
+	return page;
+}
+EXPORT_SYMBOL(alloc_pages_current_memhook);
+
+struct page *alloc_pages_current(gfp_t gfp, unsigned order)
+{
+	unsigned long caller_address = (unsigned long)_RET_IP_;
+
+	return alloc_pages_current_memhook(gfp, order, caller_address);
+}
+EXPORT_SYMBOL(alloc_pages_current);
+#else
 struct page *alloc_pages_current(gfp_t gfp, unsigned order)
 {
 	struct mempolicy *pol = &default_policy;
@@ -2253,6 +2360,7 @@ struct page *alloc_pages_current(gfp_t gfp, unsigned order)
 	return page;
 }
 EXPORT_SYMBOL(alloc_pages_current);
+#endif
 
 int vma_dup_policy(struct vm_area_struct *src, struct vm_area_struct *dst)
 {
diff --git a/mm/mempool.c b/mm/mempool.c
index 0ef8cc8d1602..39926bf22d5e 100644
--- a/mm/mempool.c
+++ b/mm/mempool.c
@@ -364,6 +364,91 @@ EXPORT_SYMBOL(mempool_resize);
  * fail if called from an IRQ context.)
  * Note: using __GFP_ZERO is not supported.
  */
+#ifdef CONFIG_DEBUG_KMALLOC
+void *mempool_alloc_memhook(mempool_t *pool, gfp_t gfp_mask, 
+	unsigned long caller_address, call_function_t cf_type)
+{
+	void *element;
+	unsigned long flags;
+	wait_queue_entry_t wait;
+	gfp_t gfp_temp;
+
+	VM_WARN_ON_ONCE(gfp_mask & __GFP_ZERO);
+	might_sleep_if(gfp_mask & __GFP_DIRECT_RECLAIM);
+
+	gfp_mask |= __GFP_NOMEMALLOC;	/* don't allocate emergency reserves */
+	gfp_mask |= __GFP_NORETRY;	/* don't loop in __alloc_pages */
+	gfp_mask |= __GFP_NOWARN;	/* failures are OK */
+
+	gfp_temp = gfp_mask & ~(__GFP_DIRECT_RECLAIM|__GFP_IO);
+
+repeat_alloc:
+
+	element = pool->alloc(gfp_temp, pool->pool_data);
+	if (likely(element != NULL)){
+		record_to_sysfs(ksize(element), element, caller_address, cf_type);
+		return element;
+	}
+
+	spin_lock_irqsave(&pool->lock, flags);
+	if (likely(pool->curr_nr)) {
+		element = remove_element(pool);
+		spin_unlock_irqrestore(&pool->lock, flags);
+		/* paired with rmb in mempool_free(), read comment there */
+		smp_wmb();
+		/*
+		 * Update the allocation stack trace as this is more useful
+		 * for debugging.
+		 */
+		kmemleak_update_trace(element);
+		record_to_sysfs(ksize(element), element, caller_address, cf_type);
+		return element;
+	}
+
+	/*
+	 * We use gfp mask w/o direct reclaim or IO for the first round.  If
+	 * alloc failed with that and @pool was empty, retry immediately.
+	 */
+	if (gfp_temp != gfp_mask) {
+		spin_unlock_irqrestore(&pool->lock, flags);
+		gfp_temp = gfp_mask;
+		goto repeat_alloc;
+	}
+
+	/* We must not sleep if !__GFP_DIRECT_RECLAIM */
+	if (!(gfp_mask & __GFP_DIRECT_RECLAIM)) {
+		spin_unlock_irqrestore(&pool->lock, flags);
+		return NULL;
+	}
+
+	/* Let's wait for someone else to return an element to @pool */
+	init_wait(&wait);
+	prepare_to_wait(&pool->wait, &wait, TASK_UNINTERRUPTIBLE);
+
+	spin_unlock_irqrestore(&pool->lock, flags);
+
+	/*
+	 * FIXME: this should be io_schedule().  The timeout is there as a
+	 * workaround for some DM problems in 2.6.18.
+	 */
+	io_schedule_timeout(5*HZ);
+
+	finish_wait(&pool->wait, &wait);
+	goto repeat_alloc;
+}
+EXPORT_SYMBOL(mempool_alloc_memhook);
+
+void *mempool_alloc(mempool_t *pool, gfp_t gfp_mask)
+{
+	unsigned long caller_address;
+	call_function_t cf_type = MEMPOOL_ALLOC;
+
+	caller_address = (unsigned long)_RET_IP_;
+
+	return mempool_alloc_memhook(pool, gfp_mask, caller_address, cf_type);
+}
+EXPORT_SYMBOL(mempool_alloc);
+#else
 void *mempool_alloc(mempool_t *pool, gfp_t gfp_mask)
 {
 	void *element;
@@ -432,6 +517,7 @@ void *mempool_alloc(mempool_t *pool, gfp_t gfp_mask)
 	goto repeat_alloc;
 }
 EXPORT_SYMBOL(mempool_alloc);
+#endif
 
 /**
  * mempool_free - return an element to the pool.
@@ -441,6 +527,70 @@ EXPORT_SYMBOL(mempool_alloc);
  *
  * this function only sleeps if the free_fn() function sleeps.
  */
+#ifdef CONFIG_DEBUG_KMALLOC
+void mempool_free_memhook(void *element, mempool_t *pool, unsigned long caller_address)
+{
+	unsigned long flags;
+
+	if (unlikely(element == NULL))
+		return;
+
+	/*
+	 * Paired with the wmb in mempool_alloc().  The preceding read is
+	 * for @element and the following @pool->curr_nr.  This ensures
+	 * that the visible value of @pool->curr_nr is from after the
+	 * allocation of @element.  This is necessary for fringe cases
+	 * where @element was passed to this task without going through
+	 * barriers.
+	 *
+	 * For example, assume @p is %NULL at the beginning and one task
+	 * performs "p = mempool_alloc(...);" while another task is doing
+	 * "while (!p) cpu_relax(); mempool_free(p, ...);".  This function
+	 * may end up using curr_nr value which is from before allocation
+	 * of @p without the following rmb.
+	 */
+	smp_rmb();
+
+	/*
+	 * For correctness, we need a test which is guaranteed to trigger
+	 * if curr_nr + #allocated == min_nr.  Testing curr_nr < min_nr
+	 * without locking achieves that and refilling as soon as possible
+	 * is desirable.
+	 *
+	 * Because curr_nr visible here is always a value after the
+	 * allocation of @element, any task which decremented curr_nr below
+	 * min_nr is guaranteed to see curr_nr < min_nr unless curr_nr gets
+	 * incremented to min_nr afterwards.  If curr_nr gets incremented
+	 * to min_nr after the allocation of @element, the elements
+	 * allocated after that are subject to the same guarantee.
+	 *
+	 * Waiters happen iff curr_nr is 0 and the above guarantee also
+	 * ensures that there will be frees which return elements to the
+	 * pool waking up the waiters.
+	 */
+	if (unlikely(pool->curr_nr < pool->min_nr)) {
+		spin_lock_irqsave(&pool->lock, flags);
+		if (likely(pool->curr_nr < pool->min_nr)) {
+			add_element(pool, element);
+			spin_unlock_irqrestore(&pool->lock, flags);
+			wake_up(&pool->wait);
+			return;
+		}
+		spin_unlock_irqrestore(&pool->lock, flags);
+	}
+	//free_mem_record(caller_address, ksize(element)); //double free
+	pool->free(element, pool->pool_data);
+}
+EXPORT_SYMBOL(mempool_free_memhook);
+
+void mempool_free(void *element, mempool_t *pool)
+{
+	unsigned long caller_address = _RET_IP_;
+		
+	mempool_free_memhook(element, pool, caller_address);
+}
+EXPORT_SYMBOL(mempool_free);
+#else
 void mempool_free(void *element, mempool_t *pool)
 {
 	unsigned long flags;
@@ -494,6 +644,7 @@ void mempool_free(void *element, mempool_t *pool)
 	pool->free(element, pool->pool_data);
 }
 EXPORT_SYMBOL(mempool_free);
+#endif
 
 /*
  * A commonly used alloc and free fn.
diff --git a/mm/page_alloc.c b/mm/page_alloc.c
index d2012e07e529..c4fe2d4309e9 100644
--- a/mm/page_alloc.c
+++ b/mm/page_alloc.c
@@ -4492,6 +4492,80 @@ static inline void finalise_ac(gfp_t gfp_mask, struct alloc_context *ac)
 /*
  * This is the 'heart' of the zoned buddy allocator.
  */
+#ifdef CONFIG_DEBUG_KMALLOC
+struct page *
+__alloc_pages_nodemask_memhook(gfp_t gfp_mask, unsigned int order, int preferred_nid,
+							nodemask_t *nodemask, unsigned long caller_address)
+{
+	struct page *page;
+	unsigned int alloc_flags = ALLOC_WMARK_LOW;
+	gfp_t alloc_mask; /* The gfp_t that was actually used for allocation */
+	struct alloc_context ac = { };
+
+	/*
+	 * There are several places where we assume that the order value is sane
+	 * so bail out early if the request is out of bound.
+	 */
+	if (unlikely(order >= MAX_ORDER)) {
+		WARN_ON_ONCE(!(gfp_mask & __GFP_NOWARN));
+		return NULL;
+	}
+
+	gfp_mask &= gfp_allowed_mask;
+	alloc_mask = gfp_mask;
+	if (!prepare_alloc_pages(gfp_mask, order, preferred_nid, nodemask, &ac, &alloc_mask, &alloc_flags))
+		return NULL;
+
+	finalise_ac(gfp_mask, &ac);
+
+	/* First allocation attempt */
+	page = get_page_from_freelist(alloc_mask, order, alloc_flags, &ac);
+	if (likely(page))
+		goto out;
+
+	/*
+	 * Apply scoped allocation constraints. This is mainly about GFP_NOFS
+	 * resp. GFP_NOIO which has to be inherited for all allocation requests
+	 * from a particular context which has been marked by
+	 * memalloc_no{fs,io}_{save,restore}.
+	 */
+	alloc_mask = current_gfp_context(gfp_mask);
+	ac.spread_dirty_pages = false;
+
+	/*
+	 * Restore the original nodemask if it was potentially replaced with
+	 * &cpuset_current_mems_allowed to optimize the fast-path attempt.
+	 */
+	if (unlikely(ac.nodemask != nodemask))
+		ac.nodemask = nodemask;
+
+	page = __alloc_pages_slowpath(alloc_mask, order, &ac);
+
+out:
+	if (memcg_kmem_enabled() && (gfp_mask & __GFP_ACCOUNT) && page &&
+	    unlikely(__memcg_kmem_charge(page, gfp_mask, order) != 0)) {
+		__free_pages(page, order);
+		page = NULL;
+	}
+
+	trace_mm_page_alloc(page, order, alloc_mask, ac.migratetype);
+
+	record_page_to_sysfs(order, page, caller_address);
+
+	return page;
+}
+EXPORT_SYMBOL(__alloc_pages_nodemask_memhook);
+
+struct page *
+__alloc_pages_nodemask(gfp_t gfp_mask, unsigned int order, int preferred_nid,
+							nodemask_t *nodemask)
+{
+	unsigned long caller_address = (unsigned long)_RET_IP_;
+
+	return __alloc_pages_nodemask_memhook(gfp_mask, order, preferred_nid, nodemask, caller_address);
+}
+EXPORT_SYMBOL(__alloc_pages_nodemask);
+#else
 struct page *
 __alloc_pages_nodemask(gfp_t gfp_mask, unsigned int order, int preferred_nid,
 							nodemask_t *nodemask)
@@ -4552,12 +4626,34 @@ __alloc_pages_nodemask(gfp_t gfp_mask, unsigned int order, int preferred_nid,
 	return page;
 }
 EXPORT_SYMBOL(__alloc_pages_nodemask);
+#endif
 
 /*
  * Common helper functions. Never use with __GFP_HIGHMEM because the returned
  * address cannot represent highmem pages. Use alloc_pages and then kmap if
  * you need to access high mem.
  */
+#ifdef CONFIG_DEBUG_KMALLOC
+unsigned long __get_free_pages_memhook(gfp_t gfp_mask, unsigned int order,
+		unsigned long caller_address)
+{
+	struct page *page;
+
+	page = alloc_pages_memhook(gfp_mask & ~__GFP_HIGHMEM, order, caller_address);
+	if (!page)
+		return 0;
+	return (unsigned long) page_address(page);
+}
+EXPORT_SYMBOL(__get_free_pages_memhook);
+
+unsigned long __get_free_pages(gfp_t gfp_mask, unsigned int order)
+{
+	unsigned long caller_address = (unsigned long)_RET_IP_;
+
+	return __get_free_pages_memhook(gfp_mask, order, caller_address);
+}
+EXPORT_SYMBOL(__get_free_pages);
+#else
 unsigned long __get_free_pages(gfp_t gfp_mask, unsigned int order)
 {
 	struct page *page;
@@ -4568,12 +4664,29 @@ unsigned long __get_free_pages(gfp_t gfp_mask, unsigned int order)
 	return (unsigned long) page_address(page);
 }
 EXPORT_SYMBOL(__get_free_pages);
+#endif
+
+#ifdef CONFIG_DEBUG_KMALLOC
+unsigned long get_zeroed_page_memhook(gfp_t gfp_mask, unsigned long caller_address)
+{
+	return __get_free_pages_memhook(gfp_mask | __GFP_ZERO, 0, caller_address);
+}
+EXPORT_SYMBOL(get_zeroed_page_memhook);
 
+unsigned long get_zeroed_page(gfp_t gfp_mask)
+{
+	unsigned long caller_address = (unsigned long)_RET_IP_;
+
+	return get_zeroed_page_memhook(gfp_mask, caller_address);
+}
+EXPORT_SYMBOL(get_zeroed_page);
+#else
 unsigned long get_zeroed_page(gfp_t gfp_mask)
 {
 	return __get_free_pages(gfp_mask | __GFP_ZERO, 0);
 }
 EXPORT_SYMBOL(get_zeroed_page);
+#endif
 
 static inline void free_the_page(struct page *page, unsigned int order)
 {
@@ -4583,6 +4696,27 @@ static inline void free_the_page(struct page *page, unsigned int order)
 		__free_pages_ok(page, order);
 }
 
+#ifdef CONFIG_DEBUG_KMALLOC
+void __free_pages_memhook(struct page *page, unsigned int order, 
+		unsigned long caller_address)
+{
+	if (put_page_testzero(page))
+		free_the_page(page, order);
+	else if (!PageHead(page))
+		while (order-- > 0)
+			free_the_page(page + (1 << order), order);
+	free_mem_record_page(caller_address, page, order);
+}
+EXPORT_SYMBOL(__free_pages_memhook);
+
+void __free_pages(struct page *page, unsigned int order)
+{
+	unsigned long caller_address = (unsigned long)_RET_IP_;
+
+	return __free_pages_memhook(page, order, caller_address);
+}
+EXPORT_SYMBOL(__free_pages);
+#else
 void __free_pages(struct page *page, unsigned int order)
 {
 	if (put_page_testzero(page))
@@ -4592,7 +4726,27 @@ void __free_pages(struct page *page, unsigned int order)
 			free_the_page(page + (1 << order), order);
 }
 EXPORT_SYMBOL(__free_pages);
+#endif
+
+#ifdef CONFIG_DEBUG_KMALLOC
+void free_pages_memhook(unsigned long addr, unsigned int order, 
+		unsigned long caller_address)
+{
+	if (addr != 0) {
+		VM_BUG_ON(!virt_addr_valid((void *)addr));
+		__free_pages_memhook(virt_to_page((void *)addr), order, caller_address);
+	}
+}
+EXPORT_SYMBOL(free_pages_memhook);
 
+void free_pages(unsigned long addr, unsigned int order)
+{
+	unsigned long caller_address = (unsigned long)_RET_IP_;
+
+	return free_pages_memhook(addr, order, caller_address);
+}
+EXPORT_SYMBOL(free_pages);
+#else
 void free_pages(unsigned long addr, unsigned int order)
 {
 	if (addr != 0) {
@@ -4600,8 +4754,8 @@ void free_pages(unsigned long addr, unsigned int order)
 		__free_pages(virt_to_page((void *)addr), order);
 	}
 }
-
 EXPORT_SYMBOL(free_pages);
+#endif
 
 /*
  * Page Fragment:
@@ -4715,6 +4869,23 @@ void page_frag_free(void *addr)
 }
 EXPORT_SYMBOL(page_frag_free);
 
+#ifdef CONFIG_DEBUG_KMALLOC
+static void *make_alloc_exact(unsigned long addr, unsigned int order,
+		size_t size, unsigned long caller_address)
+{
+	if (addr) {
+		unsigned long alloc_end = addr + (PAGE_SIZE << order);
+		unsigned long used = addr + PAGE_ALIGN(size);
+
+		split_page(virt_to_page((void *)addr), order);
+		while (used < alloc_end) {
+			free_pages_memhook(used, 0, caller_address);
+			used += PAGE_SIZE;
+		}
+	}
+	return (void *)addr;
+}
+#else
 static void *make_alloc_exact(unsigned long addr, unsigned int order,
 		size_t size)
 {
@@ -4730,9 +4901,10 @@ static void *make_alloc_exact(unsigned long addr, unsigned int order,
 	}
 	return (void *)addr;
 }
+#endif
 
 /**
- * alloc_pages_exact - allocate an exact number physically-contiguous pages.
+ *   - allocate an exact number physically-contiguous pages.
  * @size: the number of bytes to allocate
  * @gfp_mask: GFP flags for the allocation, must not contain __GFP_COMP
  *
@@ -4744,6 +4916,27 @@ static void *make_alloc_exact(unsigned long addr, unsigned int order,
  *
  * Memory allocated by this function must be released by free_pages_exact().
  */
+#ifdef CONFIG_DEBUG_KMALLOC
+void *alloc_pages_exact_memhook(size_t size, gfp_t gfp_mask, unsigned long caller_address)
+{
+	unsigned int order = get_order(size);
+	unsigned long addr;
+
+	if (WARN_ON_ONCE(gfp_mask & __GFP_COMP))
+		gfp_mask &= ~__GFP_COMP;
+	addr = __get_free_pages_memhook(gfp_mask, order, caller_address);
+	return make_alloc_exact(addr, order, size, caller_address);
+}
+EXPORT_SYMBOL(alloc_pages_exact_memhook);
+
+void *alloc_pages_exact(size_t size, gfp_t gfp_mask)
+{
+	unsigned long caller_address = (unsigned long)_RET_IP_;
+
+	return alloc_pages_exact_memhook(size, gfp_mask, caller_address);
+}
+EXPORT_SYMBOL(alloc_pages_exact);
+#else
 void *alloc_pages_exact(size_t size, gfp_t gfp_mask)
 {
 	unsigned int order = get_order(size);
@@ -4756,6 +4949,7 @@ void *alloc_pages_exact(size_t size, gfp_t gfp_mask)
 	return make_alloc_exact(addr, order, size);
 }
 EXPORT_SYMBOL(alloc_pages_exact);
+#endif
 
 /**
  * alloc_pages_exact_nid - allocate an exact number of physically-contiguous
@@ -4767,6 +4961,29 @@ EXPORT_SYMBOL(alloc_pages_exact);
  * Like alloc_pages_exact(), but try to allocate on node nid first before falling
  * back.
  */
+#ifdef CONFIG_DEBUG_KMALLOC
+void * __meminit alloc_pages_exact_nid_memhook(int nid, size_t size, gfp_t gfp_mask, 
+		unsigned long caller_address)
+{
+	unsigned int order = get_order(size);
+	struct page *p;
+
+	if (WARN_ON_ONCE(gfp_mask & __GFP_COMP))
+		gfp_mask &= ~__GFP_COMP;
+
+	p = alloc_pages_node_memhook(nid, gfp_mask, order, caller_address);
+	if (!p)
+		return NULL;
+	return make_alloc_exact((unsigned long)page_address(p), order, size, caller_address);
+}
+
+void * __meminit alloc_pages_exact_nid(int nid, size_t size, gfp_t gfp_mask)
+{
+	unsigned long caller_address = (unsigned long)_RET_IP_;
+
+	return alloc_pages_exact_nid_memhook(nid, size, gfp_mask, caller_address);
+}
+#else
 void * __meminit alloc_pages_exact_nid(int nid, size_t size, gfp_t gfp_mask)
 {
 	unsigned int order = get_order(size);
@@ -4780,6 +4997,7 @@ void * __meminit alloc_pages_exact_nid(int nid, size_t size, gfp_t gfp_mask)
 		return NULL;
 	return make_alloc_exact((unsigned long)page_address(p), order, size);
 }
+#endif
 
 /**
  * free_pages_exact - release memory allocated via alloc_pages_exact()
diff --git a/mm/percpu.c b/mm/percpu.c
index 092f777422d6..217ec8905d37 100644
--- a/mm/percpu.c
+++ b/mm/percpu.c
@@ -87,6 +87,10 @@
 #include <asm/tlbflush.h>
 #include <asm/io.h>
 
+#ifdef CONFIG_DEBUG_KMALLOC
+#include <linux/statis_memory.h>
+#endif
+
 #define CREATE_TRACE_POINTS
 #include <trace/events/percpu.h>
 
@@ -975,6 +979,54 @@ static int pcpu_find_block_fit(struct pcpu_chunk *chunk, int alloc_bits,
  * Allocated addr offset in @chunk on success.
  * -1 if no matching area is found.
  */
+#ifdef CONFIG_DEBUG_KMALLOC
+static int pcpu_alloc_area(struct pcpu_chunk *chunk, int alloc_bits,
+			   size_t align, int start, unsigned long caller_address)
+{
+	size_t align_mask = (align) ? (align - 1) : 0;
+	int bit_off, end, oslot;
+
+	lockdep_assert_held(&pcpu_lock);
+
+	oslot = pcpu_chunk_slot(chunk);
+
+	/*
+	 * Search to find a fit.
+	 */
+	end = min_t(int, start + alloc_bits + PCPU_BITMAP_BLOCK_BITS,
+		    pcpu_chunk_map_bits(chunk));
+	bit_off = bitmap_find_next_zero_area(chunk->alloc_map, end, start,
+					     alloc_bits, align_mask);
+	if (bit_off >= end)
+		return -1;
+
+	/* update alloc map */
+	bitmap_set(chunk->alloc_map, bit_off, alloc_bits);
+
+	/* update boundary map */
+	set_bit(bit_off, chunk->bound_map);
+	bitmap_clear(chunk->bound_map, bit_off + 1, alloc_bits - 1);
+	set_bit(bit_off + alloc_bits, chunk->bound_map);
+
+	chunk->free_bytes -= alloc_bits * PCPU_MIN_ALLOC_SIZE;
+
+	record_to_sysfs_percpu(caller_address, 
+		__addr_to_pcpu_ptr(chunk->base_addr+bit_off*PCPU_MIN_ALLOC_SIZE), alloc_bits*PCPU_MIN_ALLOC_SIZE);
+
+	/* update first free bit */
+	if (bit_off == chunk->first_bit)
+		chunk->first_bit = find_next_zero_bit(
+					chunk->alloc_map,
+					pcpu_chunk_map_bits(chunk),
+					bit_off + alloc_bits);
+
+	pcpu_block_update_hint_alloc(chunk, bit_off, alloc_bits);
+
+	pcpu_chunk_relocate(chunk, oslot);
+
+	return bit_off * PCPU_MIN_ALLOC_SIZE;
+}
+#else
 static int pcpu_alloc_area(struct pcpu_chunk *chunk, int alloc_bits,
 			   size_t align, int start)
 {
@@ -1018,6 +1070,7 @@ static int pcpu_alloc_area(struct pcpu_chunk *chunk, int alloc_bits,
 
 	return bit_off * PCPU_MIN_ALLOC_SIZE;
 }
+#endif
 
 /**
  * pcpu_free_area - frees the corresponding offset
@@ -1350,6 +1403,193 @@ static struct pcpu_chunk *pcpu_chunk_addr_search(void *addr)
  * RETURNS:
  * Percpu pointer to the allocated area on success, NULL on failure.
  */
+#ifdef CONFIG_DEBUG_KMALLOC
+static void __percpu *pcpu_alloc_memhook(size_t size, size_t align, bool reserved,
+				 gfp_t gfp, unsigned long caller_address)
+{
+	/* whitelisted flags that can be passed to the backing allocators */
+	gfp_t pcpu_gfp = gfp & (GFP_KERNEL | __GFP_NORETRY | __GFP_NOWARN);
+	bool is_atomic = (gfp & GFP_KERNEL) != GFP_KERNEL;
+	bool do_warn = !(gfp & __GFP_NOWARN);
+	static int warn_limit = 10;
+	struct pcpu_chunk *chunk;
+	const char *err;
+	int slot, off, cpu, ret;
+	unsigned long flags;
+	void __percpu *ptr;
+	size_t bits, bit_align;
+
+	/*
+	 * There is now a minimum allocation size of PCPU_MIN_ALLOC_SIZE,
+	 * therefore alignment must be a minimum of that many bytes.
+	 * An allocation may have internal fragmentation from rounding up
+	 * of up to PCPU_MIN_ALLOC_SIZE - 1 bytes.
+	 */
+	if (unlikely(align < PCPU_MIN_ALLOC_SIZE))
+		align = PCPU_MIN_ALLOC_SIZE;
+
+	size = ALIGN(size, PCPU_MIN_ALLOC_SIZE);
+	bits = size >> PCPU_MIN_ALLOC_SHIFT;
+	bit_align = align >> PCPU_MIN_ALLOC_SHIFT;
+
+	if (unlikely(!size || size > PCPU_MIN_UNIT_SIZE || align > PAGE_SIZE ||
+		     !is_power_of_2(align))) {
+		WARN(do_warn, "illegal size (%zu) or align (%zu) for percpu allocation\n",
+		     size, align);
+		return NULL;
+	}
+
+	if (!is_atomic) {
+		/*
+		 * pcpu_balance_workfn() allocates memory under this mutex,
+		 * and it may wait for memory reclaim. Allow current task
+		 * to become OOM victim, in case of memory pressure.
+		 */
+		if (gfp & __GFP_NOFAIL)
+			mutex_lock(&pcpu_alloc_mutex);
+		else if (mutex_lock_killable(&pcpu_alloc_mutex))
+			return NULL;
+	}
+
+	spin_lock_irqsave(&pcpu_lock, flags);
+
+	/* serve reserved allocations from the reserved chunk if available */
+	if (reserved && pcpu_reserved_chunk) {
+		chunk = pcpu_reserved_chunk;
+
+		off = pcpu_find_block_fit(chunk, bits, bit_align, is_atomic);
+		if (off < 0) {
+			err = "alloc from reserved chunk failed";
+			goto fail_unlock;
+		}
+
+		off = pcpu_alloc_area(chunk, bits, bit_align, off, caller_address);
+		if (off >= 0)
+			goto area_found;
+
+		err = "alloc from reserved chunk failed";
+		goto fail_unlock;
+	}
+
+restart:
+	/* search through normal chunks */
+	for (slot = pcpu_size_to_slot(size); slot < pcpu_nr_slots; slot++) {
+		list_for_each_entry(chunk, &pcpu_slot[slot], list) {
+			off = pcpu_find_block_fit(chunk, bits, bit_align,
+						  is_atomic);
+			if (off < 0)
+				continue;
+
+			off = pcpu_alloc_area(chunk, bits, bit_align, off, caller_address);
+			if (off >= 0)
+				goto area_found;
+
+		}
+	}
+
+	spin_unlock_irqrestore(&pcpu_lock, flags);
+
+	/*
+	 * No space left.  Create a new chunk.  We don't want multiple
+	 * tasks to create chunks simultaneously.  Serialize and create iff
+	 * there's still no empty chunk after grabbing the mutex.
+	 */
+	if (is_atomic) {
+		err = "atomic alloc failed, no space left";
+		goto fail;
+	}
+
+	if (list_empty(&pcpu_slot[pcpu_nr_slots - 1])) {
+		chunk = pcpu_create_chunk(pcpu_gfp);
+		if (!chunk) {
+			err = "failed to allocate new chunk";
+			goto fail;
+		}
+
+		spin_lock_irqsave(&pcpu_lock, flags);
+		pcpu_chunk_relocate(chunk, -1);
+	} else {
+		spin_lock_irqsave(&pcpu_lock, flags);
+	}
+
+	goto restart;
+
+area_found:
+	pcpu_stats_area_alloc(chunk, size);
+	spin_unlock_irqrestore(&pcpu_lock, flags);
+
+	/* populate if not all pages are already there */
+	if (!is_atomic) {
+		int page_start, page_end, rs, re;
+
+		page_start = PFN_DOWN(off);
+		page_end = PFN_UP(off + size);
+
+		pcpu_for_each_unpop_region(chunk->populated, rs, re,
+					   page_start, page_end) {
+			WARN_ON(chunk->immutable);
+
+			ret = pcpu_populate_chunk(chunk, rs, re, pcpu_gfp);
+
+			spin_lock_irqsave(&pcpu_lock, flags);
+			if (ret) {
+				pcpu_free_area(chunk, off);
+				err = "failed to populate";
+				goto fail_unlock;
+			}
+			pcpu_chunk_populated(chunk, rs, re, true);
+			spin_unlock_irqrestore(&pcpu_lock, flags);
+		}
+
+		mutex_unlock(&pcpu_alloc_mutex);
+	}
+
+	if (pcpu_nr_empty_pop_pages < PCPU_EMPTY_POP_PAGES_LOW)
+		pcpu_schedule_balance_work();
+
+	/* clear the areas and return address relative to base address */
+	for_each_possible_cpu(cpu)
+		memset((void *)pcpu_chunk_addr(chunk, cpu, 0) + off, 0, size);
+
+	ptr = __addr_to_pcpu_ptr(chunk->base_addr + off);
+	kmemleak_alloc_percpu(ptr, size, gfp);
+
+	trace_percpu_alloc_percpu(reserved, is_atomic, size, align,
+			chunk->base_addr, off, ptr);
+
+	return ptr;
+
+fail_unlock:
+	spin_unlock_irqrestore(&pcpu_lock, flags);
+fail:
+	trace_percpu_alloc_percpu_fail(reserved, is_atomic, size, align);
+
+	if (!is_atomic && do_warn && warn_limit) {
+		pr_warn("allocation failed, size=%zu align=%zu atomic=%d, %s\n",
+			size, align, is_atomic, err);
+		dump_stack();
+		if (!--warn_limit)
+			pr_info("limit reached, disable warning\n");
+	}
+	if (is_atomic) {
+		/* see the flag handling in pcpu_blance_workfn() */
+		pcpu_atomic_alloc_failed = true;
+		pcpu_schedule_balance_work();
+	} else {
+		mutex_unlock(&pcpu_alloc_mutex);
+	}
+	return NULL;
+}
+
+static void __percpu *pcpu_alloc(size_t size, size_t align, bool reserved,
+				 gfp_t gfp)
+{
+	unsigned long caller_address = (unsigned long)_RET_IP_;
+
+	return pcpu_alloc_memhook(size, align, reserved, gfp, caller_address);
+}
+
+#else
 static void __percpu *pcpu_alloc(size_t size, size_t align, bool reserved,
 				 gfp_t gfp)
 {
@@ -1526,6 +1766,7 @@ static void __percpu *pcpu_alloc(size_t size, size_t align, bool reserved,
 	}
 	return NULL;
 }
+#endif
 
 /**
  * __alloc_percpu_gfp - allocate dynamic percpu area
@@ -1542,11 +1783,31 @@ static void __percpu *pcpu_alloc(size_t size, size_t align, bool reserved,
  * RETURNS:
  * Percpu pointer to the allocated area on success, NULL on failure.
  */
+#ifdef CONFIG_DEBUG_KMALLOC
+void __percpu *__alloc_percpu_gfp_memhook(size_t size, size_t align, gfp_t gfp, unsigned long caller_address)
+{
+	void __percpu *x;
+
+	x = pcpu_alloc_memhook(size, align, false, gfp, caller_address);
+
+	return x;
+}
+EXPORT_SYMBOL_GPL(__alloc_percpu_gfp_memhook);
+
+void __percpu *__alloc_percpu_gfp(size_t size, size_t align, gfp_t gfp)
+{
+	unsigned long caller_address = (unsigned long)_RET_IP_;
+
+	return __alloc_percpu_gfp_memhook(size, align, gfp, caller_address);
+}
+EXPORT_SYMBOL_GPL(__alloc_percpu_gfp);
+#else
 void __percpu *__alloc_percpu_gfp(size_t size, size_t align, gfp_t gfp)
 {
 	return pcpu_alloc(size, align, false, gfp);
 }
 EXPORT_SYMBOL_GPL(__alloc_percpu_gfp);
+#endif
 
 /**
  * __alloc_percpu - allocate dynamic percpu area
@@ -1555,11 +1816,31 @@ EXPORT_SYMBOL_GPL(__alloc_percpu_gfp);
  *
  * Equivalent to __alloc_percpu_gfp(size, align, %GFP_KERNEL).
  */
+#ifdef CONFIG_DEBUG_KMALLOC
+void __percpu *__alloc_percpu_memhook(size_t size, size_t align, unsigned long caller_address)
+{
+	void __percpu *x;
+
+	x = pcpu_alloc_memhook(size, align, false, GFP_KERNEL, caller_address);
+
+	return x;
+}
+EXPORT_SYMBOL_GPL(__alloc_percpu_memhook);
+
+void __percpu *__alloc_percpu(size_t size, size_t align)
+{
+	unsigned long caller_address = (unsigned long)_RET_IP_;
+
+	return __alloc_percpu_memhook(size, align, caller_address);
+}
+EXPORT_SYMBOL_GPL(__alloc_percpu);
+#else
 void __percpu *__alloc_percpu(size_t size, size_t align)
 {
 	return pcpu_alloc(size, align, false, GFP_KERNEL);
 }
 EXPORT_SYMBOL_GPL(__alloc_percpu);
+#endif
 
 /**
  * __alloc_reserved_percpu - allocate reserved percpu area
@@ -1577,11 +1858,28 @@ EXPORT_SYMBOL_GPL(__alloc_percpu);
  * RETURNS:
  * Percpu pointer to the allocated area on success, NULL on failure.
  */
+#ifdef CONFIG_DEBUG_KMALLOC
+void __percpu *__alloc_reserved_percpu_memhook(size_t size, size_t align, unsigned long caller_address)
+{
+	void __percpu* x;
+
+	x = pcpu_alloc_memhook(size, align, true, GFP_KERNEL, caller_address);
+
+	return x;
+}
+
+void __percpu *__alloc_reserved_percpu(size_t size, size_t align)
+{
+	unsigned long caller_address = (unsigned long)_RET_IP_;
+
+	return __alloc_reserved_percpu_memhook(size, align, caller_address);
+}
+#else
 void __percpu *__alloc_reserved_percpu(size_t size, size_t align)
 {
 	return pcpu_alloc(size, align, true, GFP_KERNEL);
 }
-
+#endif
 /**
  * pcpu_balance_workfn - manage the amount of free chunks and populated pages
  * @work: unused
@@ -1716,6 +2014,63 @@ static void pcpu_balance_workfn(struct work_struct *work)
  * CONTEXT:
  * Can be called from atomic context.
  */
+#ifdef CONFIG_DEBUG_KMALLOC
+void free_percpu_memhook(void __percpu *ptr, unsigned long caller_address)
+{
+	void *addr;
+	struct pcpu_chunk *chunk;
+	unsigned long flags;
+	int off, bit_off, bits, end;
+	bool need_balance = false;
+
+	if (!ptr)
+		return;
+
+	kmemleak_free_percpu(ptr);
+
+	addr = __pcpu_ptr_to_addr(ptr);
+
+	spin_lock_irqsave(&pcpu_lock, flags);
+
+	chunk = pcpu_chunk_addr_search(addr);
+	off = addr - chunk->base_addr;
+
+	pcpu_free_area(chunk, off);
+
+	/* if there are more than one fully free chunks, wake up grim reaper */
+	if (chunk->free_bytes == pcpu_unit_size) {
+		struct pcpu_chunk *pos;
+
+		list_for_each_entry(pos, &pcpu_slot[pcpu_nr_slots - 1], list)
+			if (pos != chunk) {
+				need_balance = true;
+				break;
+			}
+	}
+
+	trace_percpu_free_percpu(chunk->base_addr, off, ptr);
+
+	spin_unlock_irqrestore(&pcpu_lock, flags);
+
+	if (need_balance)
+		pcpu_schedule_balance_work();
+
+	bit_off = off / PCPU_MIN_ALLOC_SIZE;
+	end = find_next_bit(chunk->bound_map, pcpu_chunk_map_bits(chunk),
+			    bit_off + 1);
+	bits = end - bit_off;
+	free_mem_record_percpu(caller_address, ptr, bits*PCPU_MIN_ALLOC_SIZE);
+}
+EXPORT_SYMBOL_GPL(free_percpu_memhook);
+
+void free_percpu(void __percpu *ptr)
+{
+	unsigned long caller_address = (unsigned long)_RET_IP_;
+
+	return free_percpu_memhook(ptr, caller_address);
+}
+EXPORT_SYMBOL_GPL(free_percpu);
+#else
 void free_percpu(void __percpu *ptr)
 {
 	void *addr;
@@ -1757,6 +2112,7 @@ void free_percpu(void __percpu *ptr)
 		pcpu_schedule_balance_work();
 }
 EXPORT_SYMBOL_GPL(free_percpu);
+#endif
 
 bool __is_kernel_percpu_address(unsigned long addr, unsigned long *can_addr)
 {
diff --git a/mm/slab_common.c b/mm/slab_common.c
index d208b47e01a8..38b8ac46dc9d 100644
--- a/mm/slab_common.c
+++ b/mm/slab_common.c
@@ -1576,6 +1576,28 @@ late_initcall(memcg_slabinfo_init);
 #endif /* CONFIG_DEBUG_FS && CONFIG_MEMCG_KMEM */
 #endif /* CONFIG_SLAB || CONFIG_SLUB_DEBUG */
 
+#ifdef CONFIG_DEBUG_KMALLOC
+static __always_inline void *__do_krealloc_memhook(const void *p, size_t new_size,
+					   gfp_t flags, unsigned long caller_address, call_function_t cf_type)
+{
+	void *ret;
+	size_t ks = 0;
+
+	if (p)
+		ks = ksize(p);
+
+	if (ks >= new_size) {
+		kasan_krealloc((void *)p, new_size, flags);
+		return (void *)p;
+	}
+
+	ret = kmalloc_track_caller_memhook(new_size, flags, caller_address, cf_type);
+	if (ret && p)
+		memcpy(ret, p, ks);
+
+	return ret;
+}
+#endif
 static __always_inline void *__do_krealloc(const void *p, size_t new_size,
 					   gfp_t flags)
 {
@@ -1596,7 +1618,6 @@ static __always_inline void *__do_krealloc(const void *p, size_t new_size,
 
 	return ret;
 }
-
 /**
  * __krealloc - like krealloc() but don't free @p.
  * @p: object to reallocate memory for.
@@ -1607,6 +1628,25 @@ static __always_inline void *__do_krealloc(const void *p, size_t new_size,
  * allocated buffer. Use this if you don't want to free the buffer immediately
  * like, for example, with RCU.
  */
+#ifdef CONFIG_DEBUG_KMALLOC
+void *__krealloc(const void *p, size_t new_size, gfp_t flags)
+{
+	void* ret;
+	unsigned long caller_address;
+	call_function_t cf_type = __KREALLOC;
+
+	caller_address = (unsigned long)_RET_IP_;
+
+	if (unlikely(!new_size))
+		return ZERO_SIZE_PTR;
+
+	ret = __do_krealloc_memhook(p, new_size, flags, caller_address, cf_type);
+
+	return ret;
+
+}
+EXPORT_SYMBOL(__krealloc);
+#else
 void *__krealloc(const void *p, size_t new_size, gfp_t flags)
 {
 	if (unlikely(!new_size))
@@ -1616,6 +1656,7 @@ void *__krealloc(const void *p, size_t new_size, gfp_t flags)
 
 }
 EXPORT_SYMBOL(__krealloc);
+#endif
 
 /**
  * krealloc - reallocate memory. The contents will remain unchanged.
@@ -1628,6 +1669,29 @@ EXPORT_SYMBOL(__krealloc);
  * behaves exactly like kmalloc().  If @new_size is 0 and @p is not a
  * %NULL pointer, the object pointed to is freed.
  */
+#ifdef CONFIG_DEBUG_KMALLOC
+void *krealloc(const void *p, size_t new_size, gfp_t flags)
+{
+	void* ret;
+	unsigned long caller_address;
+	call_function_t cf_type = KREALLOC;
+
+	caller_address = (unsigned long)_RET_IP_;
+
+	if (unlikely(!new_size)) {
+		kfree(p);
+		return ZERO_SIZE_PTR;
+	}
+
+	ret = __do_krealloc_memhook(p, new_size, flags, caller_address, cf_type);
+
+	if (ret && p != ret)
+		kfree(p);
+
+	return ret;
+}
+EXPORT_SYMBOL(krealloc);
+#else
 void *krealloc(const void *p, size_t new_size, gfp_t flags)
 {
 	void *ret;
@@ -1644,6 +1708,7 @@ void *krealloc(const void *p, size_t new_size, gfp_t flags)
 	return ret;
 }
 EXPORT_SYMBOL(krealloc);
+#endif
 
 /**
  * kzfree - like kfree but zero memory
@@ -1656,6 +1721,23 @@ EXPORT_SYMBOL(krealloc);
  * deal bigger than the requested buffer size passed to kmalloc(). So be
  * careful when using this function in performance sensitive code.
  */
+#ifdef CONFIG_DEBUG_KMALLOC
+void kzfree(const void *p)
+{
+	size_t ks;
+	void *mem = (void *)p;
+	unsigned long caller_address;
+
+	caller_address = (unsigned long)_RET_IP_;
+
+	if (unlikely(ZERO_OR_NULL_PTR(mem)))
+		return;
+	ks = ksize(mem);
+	memzero_explicit(mem, ks);
+	kfree_memhook(mem, caller_address);
+}
+EXPORT_SYMBOL(kzfree);
+#else
 void kzfree(const void *p)
 {
 	size_t ks;
@@ -1668,6 +1750,7 @@ void kzfree(const void *p)
 	kfree(mem);
 }
 EXPORT_SYMBOL(kzfree);
+#endif
 
 /* Tracepoints definitions. */
 EXPORT_TRACEPOINT_SYMBOL(kmalloc);
diff --git a/mm/slub.c b/mm/slub.c
index 30683ffe1823..c665b23ece18 100644
--- a/mm/slub.c
+++ b/mm/slub.c
@@ -2739,6 +2739,31 @@ static __always_inline void *slab_alloc(struct kmem_cache *s,
 	return slab_alloc_node(s, gfpflags, NUMA_NO_NODE, addr);
 }
 
+#ifdef CONFIG_DEBUG_KMALLOC
+void *kmem_cache_alloc(struct kmem_cache *s, gfp_t gfpflags)
+{
+	unsigned long caller_address;
+	call_function_t cf_type = KMEM_CACHE_ALLOC;
+
+	caller_address = (unsigned long)_RET_IP_;
+
+	return kmem_cache_alloc_memhook(s, gfpflags, caller_address, cf_type);
+}
+EXPORT_SYMBOL(kmem_cache_alloc);
+
+void *kmem_cache_alloc_memhook(struct kmem_cache *s, gfp_t gfpflags, unsigned long caller_address, call_function_t cf_type)
+{
+	void *ret = slab_alloc(s, gfpflags, _RET_IP_);
+
+	trace_kmem_cache_alloc(_RET_IP_, ret, s->object_size,
+				s->size, gfpflags);
+
+	record_to_sysfs(s->object_size, ret, caller_address, cf_type);
+
+	return ret;
+}
+EXPORT_SYMBOL(kmem_cache_alloc_memhook);
+#else
 void *kmem_cache_alloc(struct kmem_cache *s, gfp_t gfpflags)
 {
 	void *ret = slab_alloc(s, gfpflags, _RET_IP_);
@@ -2749,8 +2774,32 @@ void *kmem_cache_alloc(struct kmem_cache *s, gfp_t gfpflags)
 	return ret;
 }
 EXPORT_SYMBOL(kmem_cache_alloc);
+#endif/*CONFIG_DEBUG_KMALLOC*/
 
 #ifdef CONFIG_TRACING
+
+#ifdef CONFIG_DEBUG_KMALLOC
+void *kmem_cache_alloc_trace_memhook(struct kmem_cache *s, gfp_t gfpflags, size_t size, \
+	unsigned long caller_address, call_function_t cf_type)
+{
+	void *ret = slab_alloc(s, gfpflags, _RET_IP_);
+	trace_kmalloc(_RET_IP_, ret, size, s->size, gfpflags);
+	kasan_kmalloc(s, ret, size, gfpflags);
+	record_to_sysfs(s->object_size, ret, caller_address, cf_type);
+	return ret;
+}
+EXPORT_SYMBOL(kmem_cache_alloc_trace_memhook);
+void *kmem_cache_alloc_trace(struct kmem_cache *s, gfp_t gfpflags, size_t size)
+{
+	unsigned long caller_address;
+	call_function_t cf_type = KMEM_CACHE_ALLOC_TRACE;
+
+	caller_address = (unsigned long)_RET_IP_;
+
+	return kmem_cache_alloc_trace_memhook(s, gfpflags, size, caller_address, cf_type);
+}
+EXPORT_SYMBOL(kmem_cache_alloc_trace);
+#else
 void *kmem_cache_alloc_trace(struct kmem_cache *s, gfp_t gfpflags, size_t size)
 {
 	void *ret = slab_alloc(s, gfpflags, _RET_IP_);
@@ -2759,9 +2808,38 @@ void *kmem_cache_alloc_trace(struct kmem_cache *s, gfp_t gfpflags, size_t size)
 	return ret;
 }
 EXPORT_SYMBOL(kmem_cache_alloc_trace);
-#endif
+#endif/*CONFIG_DEBUG_KMALLOC*/
+#endif/*CONFIG_TRACING*/
 
 #ifdef CONFIG_NUMA
+
+#ifdef CONFIG_DEBUG_KMALLOC
+void *kmem_cache_alloc_node(struct kmem_cache *s, gfp_t gfpflags, int node)
+{
+	unsigned long caller_address;
+	call_function_t cf_type = KMEM_CACHE_ALLOC_NODE;
+
+	caller_address = (unsigned long)_RET_IP_;
+
+	return kmem_cache_alloc_node_memhook(s, gfpflags, node, caller_address, cf_type);
+}
+EXPORT_SYMBOL(kmem_cache_alloc_node);
+
+void *kmem_cache_alloc_node_memhook(struct kmem_cache *s, gfp_t gfpflags, int node, 
+	unsigned long caller_address, call_function_t cf_type)
+{
+	void *ret = slab_alloc_node(s, gfpflags, node, _RET_IP_);
+
+	trace_kmem_cache_alloc_node(_RET_IP_, ret,
+				    s->object_size, s->size, gfpflags, node);
+
+	record_to_sysfs(s->object_size, ret, caller_address, cf_type);
+
+	return ret;
+}
+EXPORT_SYMBOL(kmem_cache_alloc_node_memhook);
+
+#else
 void *kmem_cache_alloc_node(struct kmem_cache *s, gfp_t gfpflags, int node)
 {
 	void *ret = slab_alloc_node(s, gfpflags, node, _RET_IP_);
@@ -2772,8 +2850,42 @@ void *kmem_cache_alloc_node(struct kmem_cache *s, gfp_t gfpflags, int node)
 	return ret;
 }
 EXPORT_SYMBOL(kmem_cache_alloc_node);
+#endif /*CONFIG_DEBUG_KMALLOC*/
 
 #ifdef CONFIG_TRACING
+
+#ifdef CONFIG_DEBUG_KMALLOC
+void *kmem_cache_alloc_node_trace(struct kmem_cache *s,
+				    gfp_t gfpflags,
+				    int node, size_t size)
+{
+	unsigned long caller_address;
+	call_function_t cf_type = KMEM_CACHE_ALLOC_NODE_TRACE;
+
+	caller_address = (unsigned long)_RET_IP_;
+
+	return kmem_cache_alloc_node_trace_memhook(s, gfpflags, node, size, caller_address, cf_type);
+}
+EXPORT_SYMBOL(kmem_cache_alloc_node_trace);
+
+void *kmem_cache_alloc_node_trace_memhook(struct kmem_cache *s,
+				    gfp_t gfpflags, int node, size_t size, 
+					unsigned long caller_address, call_function_t cf_type)
+{
+	void *ret = slab_alloc_node(s, gfpflags, node, _RET_IP_);
+
+	trace_kmalloc_node(_RET_IP_, ret,
+			   size, s->size, gfpflags, node);
+
+	kasan_kmalloc(s, ret, size, gfpflags);
+
+	record_to_sysfs(s->object_size, ret, caller_address, cf_type);
+
+	return ret;
+}
+EXPORT_SYMBOL(kmem_cache_alloc_node_trace_memhook);
+
+#else
 void *kmem_cache_alloc_node_trace(struct kmem_cache *s,
 				    gfp_t gfpflags,
 				    int node, size_t size)
@@ -2787,6 +2899,7 @@ void *kmem_cache_alloc_node_trace(struct kmem_cache *s,
 	return ret;
 }
 EXPORT_SYMBOL(kmem_cache_alloc_node_trace);
+#endif  /*CONFIG_DEBUG_KMALLOC*/
 #endif
 #endif	/* CONFIG_NUMA */
 
@@ -2988,6 +3101,27 @@ void ___cache_free(struct kmem_cache *cache, void *x, unsigned long addr)
 }
 #endif
 
+#ifdef CONFIG_DEBUG_KMALLOC
+void kmem_cache_free_memhook(struct kmem_cache *s, void *x, unsigned long caller_address)
+{
+	s = cache_from_obj(s, x);
+	if (!s)
+		return;
+	slab_free(s, virt_to_head_page(x), x, NULL, 1, _RET_IP_);
+	trace_kmem_cache_free(_RET_IP_, x);
+
+	free_mem_record(caller_address, x, s->object_size);
+}
+EXPORT_SYMBOL(kmem_cache_free_memhook);
+
+void kmem_cache_free(struct kmem_cache *s, void *x)
+{
+	unsigned long caller_address = (unsigned long)_RET_IP_;
+	
+	kmem_cache_free_memhook(s, x, caller_address);
+}
+EXPORT_SYMBOL(kmem_cache_free);
+#else
 void kmem_cache_free(struct kmem_cache *s, void *x)
 {
 	s = cache_from_obj(s, x);
@@ -2997,6 +3131,7 @@ void kmem_cache_free(struct kmem_cache *s, void *x)
 	trace_kmem_cache_free(_RET_IP_, x);
 }
 EXPORT_SYMBOL(kmem_cache_free);
+#endif
 
 struct detached_freelist {
 	struct page *page;
@@ -3090,8 +3225,12 @@ int build_detached_freelist(struct kmem_cache *s, size_t size,
 }
 
 /* Note that interrupts must be enabled when calling this function. */
+#ifdef CONFIG_DEBUG_KMALLOC
 void kmem_cache_free_bulk(struct kmem_cache *s, size_t size, void **p)
 {
+	unsigned long caller_address = (unsigned long)_RET_IP_;
+	int i=0;
+
 	if (WARN_ON(!size))
 		return;
 
@@ -3103,11 +3242,115 @@ void kmem_cache_free_bulk(struct kmem_cache *s, size_t size, void **p)
 			continue;
 
 		slab_free(df.s, df.page, df.freelist, df.tail, df.cnt,_RET_IP_);
+
+		for (i = 0; i < size; i++)
+			free_mem_record(caller_address, p[i], df.s->object_size);
+
 	} while (likely(size));
 }
 EXPORT_SYMBOL(kmem_cache_free_bulk);
+#else
+void kmem_cache_free_bulk(struct kmem_cache *s, size_t size, void **p)
+{
+	if (WARN_ON(!size))
+		return;
+
+	do {
+		struct detached_freelist df;
+
+		size = build_detached_freelist(s, size, p, &df);
+		if (!df.page)
+			continue;
+
+		slab_free(df.s, df.page, df.freelist, df.tail, df.cnt,_RET_IP_);
+	} while (likely(size));
+}
+EXPORT_SYMBOL(kmem_cache_free_bulk);
+#endif
 
 /* Note that interrupts must be enabled when calling this function. */
+#ifdef CONFIG_DEBUG_KMALLOC
+int kmem_cache_alloc_bulk_memhook(struct kmem_cache *s, gfp_t flags, size_t size,
+			  void **p, unsigned long caller_address, call_function_t cf_type)
+{
+	struct kmem_cache_cpu *c;
+	int i;
+
+	/* memcg and kmem_cache debug support */
+	s = slab_pre_alloc_hook(s, flags);
+	if (unlikely(!s))
+		return false;
+	/*
+	 * Drain objects in the per cpu slab, while disabling local
+	 * IRQs, which protects against PREEMPT and interrupts
+	 * handlers invoking normal fastpath.
+	 */
+	local_irq_disable();
+	c = this_cpu_ptr(s->cpu_slab);
+
+	for (i = 0; i < size; i++) {
+		void *object = c->freelist;
+
+		if (unlikely(!object)) {
+			/*
+			 * We may have removed an object from c->freelist using
+			 * the fastpath in the previous iteration; in that case,
+			 * c->tid has not been bumped yet.
+			 * Since ___slab_alloc() may reenable interrupts while
+			 * allocating memory, we should bump c->tid now.
+			 */
+			c->tid = next_tid(c->tid);
+
+			/*
+			 * Invoking slow path likely have side-effect
+			 * of re-populating per CPU c->freelist
+			 */
+			p[i] = ___slab_alloc(s, flags, NUMA_NO_NODE,
+					    _RET_IP_, c);
+			if (unlikely(!p[i]))
+				goto error;
+
+			c = this_cpu_ptr(s->cpu_slab);
+
+			record_to_sysfs(s->object_size, p[i], caller_address, cf_type);
+
+			continue; /* goto for-loop */
+		}
+		c->freelist = get_freepointer(s, object);
+		p[i] = object;
+		record_to_sysfs(s->object_size, p[i], caller_address, cf_type);
+	}
+	c->tid = next_tid(c->tid);
+	local_irq_enable();
+
+	/* Clear memory outside IRQ disabled fastpath loop */
+	if (unlikely(flags & __GFP_ZERO)) {
+		int j;
+
+		for (j = 0; j < i; j++)
+			memset(p[j], 0, s->object_size);
+	}
+
+	/* memcg and kmem_cache debug support */
+	slab_post_alloc_hook(s, flags, size, p);
+	return i;
+error:
+	local_irq_enable();
+	slab_post_alloc_hook(s, flags, i, p);
+	__kmem_cache_free_bulk(s, i, p);
+	return 0;
+}
+
+int kmem_cache_alloc_bulk(struct kmem_cache *s, gfp_t flags, size_t size,
+			  void **p)
+{
+	unsigned long caller_address = (unsigned long)_RET_IP_;
+	call_function_t cf_type =  KMEM_CACHE_ALLOC_BULK;
+
+	return kmem_cache_alloc_bulk_memhook(s, flags, size, p, caller_address, cf_type);
+}
+EXPORT_SYMBOL(kmem_cache_alloc_bulk);
+#else
 int kmem_cache_alloc_bulk(struct kmem_cache *s, gfp_t flags, size_t size,
 			  void **p)
 {
@@ -3175,7 +3418,7 @@ int kmem_cache_alloc_bulk(struct kmem_cache *s, gfp_t flags, size_t size,
 	return 0;
 }
 EXPORT_SYMBOL(kmem_cache_alloc_bulk);
-
+#endif
 
 /*
  * Object placement in a slab is made very easy because we always start at
@@ -3766,6 +4009,44 @@ static int __init setup_slub_min_objects(char *str)
 
 __setup("slub_min_objects=", setup_slub_min_objects);
 
+#ifdef CONFIG_DEBUG_KMALLOC
+void *__kmalloc(size_t size, gfp_t flags)
+{
+	unsigned long caller_address;
+	call_function_t cf_type = __KMALLOC;
+
+	caller_address = (unsigned long)_RET_IP_;
+
+	return __kmalloc_memhook(size, flags, caller_address, cf_type);
+}
+EXPORT_SYMBOL(__kmalloc);
+
+void *__kmalloc_memhook(size_t size, gfp_t flags, unsigned long caller_address, call_function_t cf_type)
+{
+	struct kmem_cache *s;
+	void *ret;
+
+	if (unlikely(size > KMALLOC_MAX_CACHE_SIZE))
+		return kmalloc_large_memhook(size, flags, caller_address, cf_type);
+
+	s = kmalloc_slab(size, flags);
+
+	if (unlikely(ZERO_OR_NULL_PTR(s)))
+		return s;
+
+	ret = slab_alloc(s, flags, _RET_IP_);
+
+	trace_kmalloc(_RET_IP_, ret, size, s->size, flags);
+
+	kasan_kmalloc(s, ret, size, flags);
+
+	record_to_sysfs(s->object_size, ret, caller_address, cf_type);
+
+	return ret;
+}
+EXPORT_SYMBOL(__kmalloc_memhook);
+
+#else
 void *__kmalloc(size_t size, gfp_t flags)
 {
 	struct kmem_cache *s;
@@ -3788,8 +4069,28 @@ void *__kmalloc(size_t size, gfp_t flags)
 	return ret;
 }
 EXPORT_SYMBOL(__kmalloc);
+#endif/*CONFIG_DEBUG_KMALLOC*/
 
 #ifdef CONFIG_NUMA
+#ifdef CONFIG_DEBUG_KMALLOC
+static void *kmalloc_large_node_memhook(size_t size, gfp_t flags, int node, 
+	unsigned long caller_address, call_function_t cf_type)
+{
+	struct page *page;
+	void *ptr = NULL;
+
+	flags |= __GFP_COMP;
+	page = alloc_pages_node(node, flags, get_order(size));
+	if (page)
+		ptr = page_address(page);
+
+	kmalloc_large_node_hook(ptr, size, flags);
+
+	record_page_to_sysfs(get_order(size), ptr, caller_address);
+
+	return ptr;
+}
+#else
 static void *kmalloc_large_node(size_t size, gfp_t flags, int node)
 {
 	struct page *page;
@@ -3803,7 +4104,54 @@ static void *kmalloc_large_node(size_t size, gfp_t flags, int node)
 	kmalloc_large_node_hook(ptr, size, flags);
 	return ptr;
 }
+#endif
+
+#ifdef CONFIG_DEBUG_KMALLOC
+void *__kmalloc_node(size_t size, gfp_t flags, int node)
+{
+	unsigned long caller_address;
+	call_function_t cf_type = __KMALLOC_NODE;
+
+	caller_address = (unsigned long)_RET_IP_;
+
+	return __kmalloc_node_memhook(size, flags, node, caller_address, cf_type);
+}
+EXPORT_SYMBOL(__kmalloc_node);
+
+void *__kmalloc_node_memhook(size_t size, gfp_t flags, int node, 
+	unsigned long caller_address, call_function_t cf_type)
+{
+	struct kmem_cache *s;
+	void *ret;
+
+	if (unlikely(size > KMALLOC_MAX_CACHE_SIZE)) {
+		ret = kmalloc_large_node_memhook(size, flags, node, caller_address, cf_type);
+
+		trace_kmalloc_node(_RET_IP_, ret,
+				   size, PAGE_SIZE << get_order(size),
+				   flags, node);
+
+		return ret;
+	}
+
+	s = kmalloc_slab(size, flags);
+
+	if (unlikely(ZERO_OR_NULL_PTR(s)))
+		return s;
+
+	ret = slab_alloc_node(s, flags, node, _RET_IP_);
+
+	trace_kmalloc_node(_RET_IP_, ret, size, s->size, flags, node);
+
+	kasan_kmalloc(s, ret, size, flags);
+
+	record_to_sysfs(s->object_size, ret, caller_address, cf_type);
+
+	return ret;
+}
+EXPORT_SYMBOL(__kmalloc_node_memhook);
 
+#else
 void *__kmalloc_node(size_t size, gfp_t flags, int node)
 {
 	struct kmem_cache *s;
@@ -3833,6 +4181,8 @@ void *__kmalloc_node(size_t size, gfp_t flags, int node)
 	return ret;
 }
 EXPORT_SYMBOL(__kmalloc_node);
+#endif  /*CONFIG_DEBUG_KMALLOC*/
+
 #endif	/* CONFIG_NUMA */
 
 #ifdef CONFIG_HARDENED_USERCOPY
@@ -3921,6 +4271,44 @@ size_t ksize(const void *object)
 }
 EXPORT_SYMBOL(ksize);
 
+#ifdef CONFIG_DEBUG_KMALLOC
+void kfree_memhook(const void *x, unsigned long caller_address)
+{
+	struct page *page;
+	void *object = (void *)x;
+
+	if (unlikely(ZERO_OR_NULL_PTR(x)))
+		return;
+
+	page = virt_to_head_page(x);
+	if (unlikely(!PageSlab(page))) {
+		BUG_ON(!PageCompound(page));
+		kfree_hook(object);
+		__free_pages(page, compound_order(page));
+		free_mem_record_page(caller_address, page, compound_order(page));
+		return;
+	}
+	slab_free(page->slab_cache, page, object, NULL, 1, _RET_IP_);
+
+	free_mem_record(caller_address, object, ksize(x));
+
+}
+EXPORT_SYMBOL(kfree_memhook);
+
+void kfree(const void *x)
+{
+	unsigned long caller_address;
+
+	caller_address = (unsigned long)_RET_IP_;
+
+	trace_kfree(_RET_IP_, x);
+
+	kfree_memhook(x, caller_address);
+
+}
+EXPORT_SYMBOL(kfree);
+
+#else
 void kfree(const void *x)
 {
 	struct page *page;
@@ -3941,6 +4329,39 @@ void kfree(const void *x)
 	slab_free(page->slab_cache, page, object, NULL, 1, _RET_IP_);
 }
 EXPORT_SYMBOL(kfree);
+#endif
+
+#ifdef CONFIG_DEBUG_KMALLOC
+struct page_record_list *alloc_record_pl(gfp_t gfp)
+{
+	struct page_record_list *pl;
+	struct kmem_cache *s;
+
+    s = kmalloc_slab(sizeof(*pl), gfp);
+
+    pl = slab_alloc(s, gfp, _RET_IP_);
+
+	if (!pl)
+		return NULL;
+
+	pl->page = alloc_pages(gfp | __GFP_ZERO, 0);
+	if (!pl->page) {
+		kfree(pl);
+		return NULL;
+	}
+
+	pl->next = NULL;
+	atomic_set(&pl->page_elements, 0);
+
+	return pl;
+}
+
+void free_record_pl(struct page_record_list *pl)
+{
+	__free_page(pl->page);
+	kfree(pl);
+}
+#endif
 
 #define SHRINK_PROMOTE_MAX 32
 
@@ -4303,6 +4724,44 @@ int __kmem_cache_create(struct kmem_cache *s, slab_flags_t flags)
 	return err;
 }
 
+#ifdef CONFIG_DEBUG_KMALLOC
+void *__kmalloc_track_caller_memhook(size_t size, gfp_t gfpflags, unsigned long caller, \
+	unsigned long caller_address, call_function_t cf_type)
+{
+	struct kmem_cache *s;
+	void *ret;
+
+	if (unlikely(size > KMALLOC_MAX_CACHE_SIZE))
+		return kmalloc_large_memhook(size, gfpflags, caller_address, cf_type);
+
+	s = kmalloc_slab(size, gfpflags);
+
+	if (unlikely(ZERO_OR_NULL_PTR(s)))
+		return s;
+
+	ret = slab_alloc(s, gfpflags, caller);
+
+	/* Honor the call site pointer we received. */
+	trace_kmalloc(caller, ret, size, s->size, gfpflags);
+
+	record_to_sysfs(s->object_size, ret, caller_address, cf_type);
+
+	return ret;
+}
+
+void *__kmalloc_track_caller(size_t size, gfp_t gfpflags, unsigned long caller)
+{
+	void* ret;
+	unsigned long caller_address;
+	call_function_t cf_type = __KMALLOC_TRACK_CALLER;
+
+	caller_address = (unsigned long)_RET_IP_;
+
+	ret = __kmalloc_track_caller_memhook(size, gfpflags, caller, caller_address, cf_type);
+
+	return ret;
+}
+#else
 void *__kmalloc_track_caller(size_t size, gfp_t gfpflags, unsigned long caller)
 {
 	struct kmem_cache *s;
@@ -4323,8 +4782,55 @@ void *__kmalloc_track_caller(size_t size, gfp_t gfpflags, unsigned long caller)
 
 	return ret;
 }
+#endif
 
 #ifdef CONFIG_NUMA
+#ifdef CONFIG_DEBUG_KMALLOC
+void *__kmalloc_node_track_caller_memhook(size_t size, gfp_t gfpflags, int node,
+	unsigned long caller, unsigned long caller_address, call_function_t cf_type)
+{
+	struct kmem_cache *s;
+	void *ret;
+
+	if (unlikely(size > KMALLOC_MAX_CACHE_SIZE)) {
+		ret = kmalloc_large_node_memhook(size, gfpflags, node, caller_address, cf_type);
+
+		trace_kmalloc_node(caller, ret,
+				   size, PAGE_SIZE << get_order(size),
+				   gfpflags, node);
+
+		return ret;
+	}
+
+	s = kmalloc_slab(size, gfpflags);
+
+	if (unlikely(ZERO_OR_NULL_PTR(s)))
+		return s;
+
+	ret = slab_alloc_node(s, gfpflags, node, caller);
+
+	/* Honor the call site pointer we received. */
+	trace_kmalloc_node(caller, ret, size, s->size, gfpflags, node);
+
+	record_to_sysfs(s->object_size, ret, caller_address, cf_type);
+
+	return ret;
+}
+
+void *__kmalloc_node_track_caller(size_t size, gfp_t gfpflags,
+					int node, unsigned long caller)
+{
+	void* ret;
+	unsigned long caller_address;
+	call_function_t cf_type = __KMALLOC_NODE_TRACK_CALLER;
+
+	caller_address = (unsigned long)_RET_IP_;
+
+	ret = __kmalloc_node_track_caller_memhook(size, gfpflags, node, caller, caller_address, cf_type);
+
+	return ret;
+}
+#else
 void *__kmalloc_node_track_caller(size_t size, gfp_t gfpflags,
 					int node, unsigned long caller)
 {
@@ -4354,6 +4860,7 @@ void *__kmalloc_node_track_caller(size_t size, gfp_t gfpflags,
 	return ret;
 }
 #endif
+#endif
 
 #ifdef CONFIG_SYSFS
 static int count_inuse(struct page *page)
-- 
2.27.0

